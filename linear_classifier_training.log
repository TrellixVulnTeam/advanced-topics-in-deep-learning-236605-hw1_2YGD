Classifier Training Epoch #1
------------------------------------------
Epoch: #1    | Batch: #1    | Batch Loss: 2.41658878326416              | Epoch Loss: 2.41658878326416              
Epoch: #1    | Batch: #2    | Batch Loss: 2.531118392944336             | Epoch Loss: 2.473853588104248             
Epoch: #1    | Batch: #3    | Batch Loss: 2.30753755569458              | Epoch Loss: 2.418414910634359             
Epoch: #1    | Batch: #4    | Batch Loss: 1.989551067352295             | Epoch Loss: 2.3111989498138428            
Epoch: #1    | Batch: #5    | Batch Loss: 2.10693097114563              | Epoch Loss: 2.2703453540802               
Epoch: #1    | Batch: #6    | Batch Loss: 2.007031202316284             | Epoch Loss: 2.2264596621195474            
Epoch: #1    | Batch: #7    | Batch Loss: 1.7178070545196533            | Epoch Loss: 2.153795003890991             
Epoch: #1    | Batch: #8    | Batch Loss: 1.5600528717041016            | Epoch Loss: 2.07957723736763              
Epoch: #1    | Batch: #9    | Batch Loss: 1.4890320301055908            | Epoch Loss: 2.0139611032274036            
Epoch: #1    | Batch: #10   | Batch Loss: 1.5166196823120117            | Epoch Loss: 1.9642269611358643            
Epoch: #1    | Batch: #11   | Batch Loss: 1.3973603248596191            | Epoch Loss: 1.9126936305652966            
Epoch: #1    | Batch: #12   | Batch Loss: 1.341115117073059             | Epoch Loss: 1.8650620877742767            
Epoch: #1    | Batch: #13   | Batch Loss: 1.2688524723052979            | Epoch Loss: 1.8191998096612783            
Epoch: #1    | Batch: #14   | Batch Loss: 1.2722958326339722            | Epoch Loss: 1.7801352398736137            
Epoch: #1    | Batch: #15   | Batch Loss: 1.143878698348999             | Epoch Loss: 1.737718137105306             
Epoch: #1    | Batch: #16   | Batch Loss: 1.1724998950958252            | Epoch Loss: 1.7023919969797134            
Epoch: #1    | Batch: #17   | Batch Loss: 1.1149306297302246            | Epoch Loss: 1.6678354459650375            
Epoch: #1    | Batch: #18   | Batch Loss: 1.0347318649291992            | Epoch Loss: 1.63266302479638              
Epoch: #1    | Batch: #19   | Batch Loss: 1.0382790565490723            | Epoch Loss: 1.6013796580465216            
Epoch: #1    | Batch: #20   | Batch Loss: 1.0706968307495117            | Epoch Loss: 1.574845516681671             
Epoch: #1    | Batch: #21   | Batch Loss: 1.0373212099075317            | Epoch Loss: 1.5492491211209978            
Epoch: #1    | Batch: #22   | Batch Loss: 0.9445729851722717            | Epoch Loss: 1.5217638422142377            
Epoch: #1    | Batch: #23   | Batch Loss: 0.9664103984832764            | Epoch Loss: 1.4976180403128914            
Epoch: #1    | Batch: #24   | Batch Loss: 0.9439100027084351            | Epoch Loss: 1.4745468720793724            
Epoch: #1    | Batch: #25   | Batch Loss: 0.9565865397453308            | Epoch Loss: 1.4538284587860106            
Epoch: #1    | Batch: #26   | Batch Loss: 0.9701738357543945            | Epoch Loss: 1.4352263579001794            
Epoch: #1    | Batch: #27   | Batch Loss: 0.9485190510749817            | Epoch Loss: 1.417200161351098             
Epoch: #1    | Batch: #28   | Batch Loss: 0.9131758213043213            | Epoch Loss: 1.399199292063713             
Epoch: #1    | Batch: #29   | Batch Loss: 0.900315523147583             | Epoch Loss: 1.3819964034803982            
Epoch: #1    | Batch: #30   | Batch Loss: 0.8962469696998596            | Epoch Loss: 1.3658047556877135            
Epoch: #1    | Batch: #31   | Batch Loss: 0.8783178925514221            | Epoch Loss: 1.3500793730058978            
Epoch: #1    | Batch: #32   | Batch Loss: 0.9506649374961853            | Epoch Loss: 1.3375976718962193            
Epoch: #1    | Batch: #33   | Batch Loss: 0.9541481137275696            | Epoch Loss: 1.325977988315351             
Epoch: #1    | Batch: #34   | Batch Loss: 1.0068143606185913            | Epoch Loss: 1.3165908227948582            
Epoch: #1    | Batch: #35   | Batch Loss: 0.8794252276420593            | Epoch Loss: 1.3041003772190638            
Epoch: #1    | Batch: #36   | Batch Loss: 0.8481226563453674            | Epoch Loss: 1.2914343294170167            
Epoch: #1    | Batch: #37   | Batch Loss: 0.8005532622337341            | Epoch Loss: 1.2781672735471983            
Epoch: #1    | Batch: #38   | Batch Loss: 0.8767624497413635            | Epoch Loss: 1.2676039887102026            
Epoch: #1    | Batch: #39   | Batch Loss: 0.9797138571739197            | Epoch Loss: 1.2602221904656825            
Epoch: #1    | Batch: #40   | Batch Loss: 0.7831399440765381            | Epoch Loss: 1.248295134305954             
Epoch: #1    | Batch: #41   | Batch Loss: 0.7968601584434509            | Epoch Loss: 1.2372845251385758            
Epoch: #1    | Batch: #42   | Batch Loss: 0.7688750624656677            | Epoch Loss: 1.226131918884459             
Epoch: #1    | Batch: #43   | Batch Loss: 0.7380219101905823            | Epoch Loss: 1.2147805233334386            

Classifier Validation Epoch #1
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.7227272727272728            
Batch: #2    | Top-1 Accuracy: 0.759090909090909             
Batch: #3    | Top-1 Accuracy: 0.7666666666666666            
Batch: #4    | Top-1 Accuracy: 0.7795454545454545            
Batch: #5    | Top-1 Accuracy: 0.7818181818181819            
Batch: #6    | Top-1 Accuracy: 0.7787878787878788            
Batch: #7    | Top-1 Accuracy: 0.7785714285714286            
Batch: #8    | Top-1 Accuracy: 0.775                         
Batch: #9    | Top-1 Accuracy: 0.7792929292929293            
Batch: #10   | Top-1 Accuracy: 0.7863636363636364            
Batch: #11   | Top-1 Accuracy: 0.790495867768595             
Batch: #12   | Top-1 Accuracy: 0.7928030303030303            
Batch: #13   | Top-1 Accuracy: 0.7909090909090909            
Batch: #14   | Top-1 Accuracy: 0.7899350649350649            
Batch: #15   | Top-1 Accuracy: 0.7915151515151515            
Batch: #16   | Top-1 Accuracy: 0.7923295454545455            
Batch: #17   | Top-1 Accuracy: 0.7925133689839572            

Classifier Training Epoch #2
------------------------------------------
Epoch: #2    | Batch: #1    | Batch Loss: 0.8297678828239441            | Epoch Loss: 0.8297678828239441            
Epoch: #2    | Batch: #2    | Batch Loss: 0.7396217584609985            | Epoch Loss: 0.7846948206424713            
Epoch: #2    | Batch: #3    | Batch Loss: 0.8376614451408386            | Epoch Loss: 0.8023503621419271            
Epoch: #2    | Batch: #4    | Batch Loss: 0.8634024858474731            | Epoch Loss: 0.8176133930683136            
Epoch: #2    | Batch: #5    | Batch Loss: 0.7843308448791504            | Epoch Loss: 0.810956883430481             
Epoch: #2    | Batch: #6    | Batch Loss: 0.7608745694160461            | Epoch Loss: 0.8026098310947418            
Epoch: #2    | Batch: #7    | Batch Loss: 0.685314953327179             | Epoch Loss: 0.78585341998509              
Epoch: #2    | Batch: #8    | Batch Loss: 0.7917333245277405            | Epoch Loss: 0.7865884080529213            
Epoch: #2    | Batch: #9    | Batch Loss: 0.7107439637184143            | Epoch Loss: 0.7781612475713094            
Epoch: #2    | Batch: #10   | Batch Loss: 0.6224262118339539            | Epoch Loss: 0.7625877439975739            
Epoch: #2    | Batch: #11   | Batch Loss: 0.7585030794143677            | Epoch Loss: 0.762216410853646             
Epoch: #2    | Batch: #12   | Batch Loss: 0.8068285584449768            | Epoch Loss: 0.7659340898195902            
Epoch: #2    | Batch: #13   | Batch Loss: 0.7926609516143799            | Epoch Loss: 0.7679900022653433            
Epoch: #2    | Batch: #14   | Batch Loss: 0.7429365515708923            | Epoch Loss: 0.7662004700728825            
Epoch: #2    | Batch: #15   | Batch Loss: 0.7348602414131165            | Epoch Loss: 0.7641111214955648            
Epoch: #2    | Batch: #16   | Batch Loss: 0.7224683165550232            | Epoch Loss: 0.7615084461867809            
Epoch: #2    | Batch: #17   | Batch Loss: 0.6894018054008484            | Epoch Loss: 0.7572668790817261            
Epoch: #2    | Batch: #18   | Batch Loss: 0.7328963279724121            | Epoch Loss: 0.7559129595756531            
Epoch: #2    | Batch: #19   | Batch Loss: 0.772416353225708             | Epoch Loss: 0.7567815592414454            
Epoch: #2    | Batch: #20   | Batch Loss: 0.9635471105575562            | Epoch Loss: 0.7671198368072509            
Epoch: #2    | Batch: #21   | Batch Loss: 0.8234112858772278            | Epoch Loss: 0.7698003820010594            
Epoch: #2    | Batch: #22   | Batch Loss: 0.6828345060348511            | Epoch Loss: 0.765847387638959             
Epoch: #2    | Batch: #23   | Batch Loss: 0.7192335724830627            | Epoch Loss: 0.7638207000234852            
Epoch: #2    | Batch: #24   | Batch Loss: 0.7319887280464172            | Epoch Loss: 0.7624943678577741            
Epoch: #2    | Batch: #25   | Batch Loss: 0.7763150930404663            | Epoch Loss: 0.7630471968650818            
Epoch: #2    | Batch: #26   | Batch Loss: 0.6842131018638611            | Epoch Loss: 0.7600151162881118            
Epoch: #2    | Batch: #27   | Batch Loss: 0.7834029197692871            | Epoch Loss: 0.760881331231859             
Epoch: #2    | Batch: #28   | Batch Loss: 0.7453481554985046            | Epoch Loss: 0.7603265749556678            
Epoch: #2    | Batch: #29   | Batch Loss: 0.6888630986213684            | Epoch Loss: 0.7578623171510368            
Epoch: #2    | Batch: #30   | Batch Loss: 0.7957501411437988            | Epoch Loss: 0.7591252446174621            
Epoch: #2    | Batch: #31   | Batch Loss: 0.8305683732032776            | Epoch Loss: 0.7614298616686175            
Epoch: #2    | Batch: #32   | Batch Loss: 0.7395660877227783            | Epoch Loss: 0.76074661873281              
Epoch: #2    | Batch: #33   | Batch Loss: 0.8393372297286987            | Epoch Loss: 0.763128152399352             
Epoch: #2    | Batch: #34   | Batch Loss: 0.8128507733345032            | Epoch Loss: 0.7645905824268565            
Epoch: #2    | Batch: #35   | Batch Loss: 0.6641440391540527            | Epoch Loss: 0.7617206811904907            
Epoch: #2    | Batch: #36   | Batch Loss: 0.6578280925750732            | Epoch Loss: 0.7588347759511735            
Epoch: #2    | Batch: #37   | Batch Loss: 0.8087196946144104            | Epoch Loss: 0.7601830169961259            
Epoch: #2    | Batch: #38   | Batch Loss: 0.7167906165122986            | Epoch Loss: 0.7590411117202357            
Epoch: #2    | Batch: #39   | Batch Loss: 0.7579717040061951            | Epoch Loss: 0.7590136910096194            
Epoch: #2    | Batch: #40   | Batch Loss: 0.7916600108146667            | Epoch Loss: 0.7598298490047455            
Epoch: #2    | Batch: #41   | Batch Loss: 0.7891532182693481            | Epoch Loss: 0.7605450531331505            
Epoch: #2    | Batch: #42   | Batch Loss: 0.711356520652771             | Epoch Loss: 0.7593738975979033            
Epoch: #2    | Batch: #43   | Batch Loss: 0.6852224469184875            | Epoch Loss: 0.7576494452565216            

Classifier Validation Epoch #2
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.7909090909090909            
Batch: #2    | Top-1 Accuracy: 0.8204545454545454            
Batch: #3    | Top-1 Accuracy: 0.8166666666666668            
Batch: #4    | Top-1 Accuracy: 0.8181818181818182            
Batch: #5    | Top-1 Accuracy: 0.8099999999999999            
Batch: #6    | Top-1 Accuracy: 0.8128787878787879            
Batch: #7    | Top-1 Accuracy: 0.807142857142857             
Batch: #8    | Top-1 Accuracy: 0.7999999999999999            
Batch: #9    | Top-1 Accuracy: 0.7989898989898989            
Batch: #10   | Top-1 Accuracy: 0.7963636363636363            
Batch: #11   | Top-1 Accuracy: 0.8012396694214875            
Batch: #12   | Top-1 Accuracy: 0.799621212121212             
Batch: #13   | Top-1 Accuracy: 0.798251748251748             
Batch: #14   | Top-1 Accuracy: 0.8025974025974024            
Batch: #15   | Top-1 Accuracy: 0.8024242424242424            
Batch: #16   | Top-1 Accuracy: 0.8011363636363635            
Batch: #17   | Top-1 Accuracy: 0.7989304812834224            

Classifier Training Epoch #3
------------------------------------------
Epoch: #3    | Batch: #1    | Batch Loss: 0.7589142322540283            | Epoch Loss: 0.7589142322540283            
Epoch: #3    | Batch: #2    | Batch Loss: 0.7334516644477844            | Epoch Loss: 0.7461829483509064            
Epoch: #3    | Batch: #3    | Batch Loss: 0.6348966360092163            | Epoch Loss: 0.7090875109036764            
Epoch: #3    | Batch: #4    | Batch Loss: 0.7539745569229126            | Epoch Loss: 0.7203092724084854            
Epoch: #3    | Batch: #5    | Batch Loss: 0.6248658895492554            | Epoch Loss: 0.7012205958366394            
Epoch: #3    | Batch: #6    | Batch Loss: 0.5899853706359863            | Epoch Loss: 0.6826813916365305            
Epoch: #3    | Batch: #7    | Batch Loss: 0.8724709153175354            | Epoch Loss: 0.709794180733817             
Epoch: #3    | Batch: #8    | Batch Loss: 0.8251041173934937            | Epoch Loss: 0.7242079228162766            
Epoch: #3    | Batch: #9    | Batch Loss: 0.7997166514396667            | Epoch Loss: 0.7325977815522088            
Epoch: #3    | Batch: #10   | Batch Loss: 0.7128820419311523            | Epoch Loss: 0.7306262075901031            
Epoch: #3    | Batch: #11   | Batch Loss: 0.8111894130706787            | Epoch Loss: 0.7379501353610646            
Epoch: #3    | Batch: #12   | Batch Loss: 0.6801928281784058            | Epoch Loss: 0.7331370264291763            
Epoch: #3    | Batch: #13   | Batch Loss: 0.7255417108535767            | Epoch Loss: 0.7325527713848994            
Epoch: #3    | Batch: #14   | Batch Loss: 0.8389809131622314            | Epoch Loss: 0.7401547815118518            
Epoch: #3    | Batch: #15   | Batch Loss: 0.7457109093666077            | Epoch Loss: 0.7405251900355021            
Epoch: #3    | Batch: #16   | Batch Loss: 0.7926443219184875            | Epoch Loss: 0.7437826357781887            
Epoch: #3    | Batch: #17   | Batch Loss: 0.5534189343452454            | Epoch Loss: 0.7325847709880156            
Epoch: #3    | Batch: #18   | Batch Loss: 0.8069134950637817            | Epoch Loss: 0.7367141445477804            
Epoch: #3    | Batch: #19   | Batch Loss: 0.7952017784118652            | Epoch Loss: 0.7397924410669428            
Epoch: #3    | Batch: #20   | Batch Loss: 0.7028067708015442            | Epoch Loss: 0.7379431575536728            
Epoch: #3    | Batch: #21   | Batch Loss: 0.6990673542022705            | Epoch Loss: 0.7360919288226536            
Epoch: #3    | Batch: #22   | Batch Loss: 0.6821811199188232            | Epoch Loss: 0.7336414375088431            
Epoch: #3    | Batch: #23   | Batch Loss: 0.8141847848892212            | Epoch Loss: 0.7371433221775553            
Epoch: #3    | Batch: #24   | Batch Loss: 0.5810126066207886            | Epoch Loss: 0.7306378756960233            
Epoch: #3    | Batch: #25   | Batch Loss: 0.7928528785705566            | Epoch Loss: 0.7331264758110047            
Epoch: #3    | Batch: #26   | Batch Loss: 0.7641475796699524            | Epoch Loss: 0.734319595190195             
Epoch: #3    | Batch: #27   | Batch Loss: 0.6597611308097839            | Epoch Loss: 0.731558170583513             
Epoch: #3    | Batch: #28   | Batch Loss: 0.796347439289093             | Epoch Loss: 0.7338720730372837            
Epoch: #3    | Batch: #29   | Batch Loss: 0.641537070274353             | Epoch Loss: 0.7306881074247689            
Epoch: #3    | Batch: #30   | Batch Loss: 0.6641920208930969            | Epoch Loss: 0.7284715712070465            
Epoch: #3    | Batch: #31   | Batch Loss: 0.664019763469696             | Epoch Loss: 0.7263924806348739            
Epoch: #3    | Batch: #32   | Batch Loss: 0.736914336681366             | Epoch Loss: 0.7267212886363268            
Epoch: #3    | Batch: #33   | Batch Loss: 0.7558885812759399            | Epoch Loss: 0.7276051459890424            
Epoch: #3    | Batch: #34   | Batch Loss: 0.7408247590065002            | Epoch Loss: 0.7279939581366146            
Epoch: #3    | Batch: #35   | Batch Loss: 0.7730672955513               | Epoch Loss: 0.7292817677770342            
Epoch: #3    | Batch: #36   | Batch Loss: 0.8644566535949707            | Epoch Loss: 0.7330366257164214            
Epoch: #3    | Batch: #37   | Batch Loss: 0.6562513709068298            | Epoch Loss: 0.7309613485594053            
Epoch: #3    | Batch: #38   | Batch Loss: 0.579656720161438             | Epoch Loss: 0.7269796478120905            
Epoch: #3    | Batch: #39   | Batch Loss: 0.6599745750427246            | Epoch Loss: 0.7252615690231323            
Epoch: #3    | Batch: #40   | Batch Loss: 0.6601697206497192            | Epoch Loss: 0.723634272813797             
Epoch: #3    | Batch: #41   | Batch Loss: 0.8436205983161926            | Epoch Loss: 0.7265607685577579            
Epoch: #3    | Batch: #42   | Batch Loss: 0.7407709956169128            | Epoch Loss: 0.7268991072972616            
Epoch: #3    | Batch: #43   | Batch Loss: 0.7372238039970398            | Epoch Loss: 0.7271392165228378            

Classifier Validation Epoch #3
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8272727272727273            
Batch: #2    | Top-1 Accuracy: 0.825                         
Batch: #3    | Top-1 Accuracy: 0.8227272727272728            
Batch: #4    | Top-1 Accuracy: 0.8261363636363637            
Batch: #5    | Top-1 Accuracy: 0.8263636363636364            
Batch: #6    | Top-1 Accuracy: 0.8318181818181819            
Batch: #7    | Top-1 Accuracy: 0.8246753246753248            
Batch: #8    | Top-1 Accuracy: 0.8204545454545455            
Batch: #9    | Top-1 Accuracy: 0.8191919191919192            
Batch: #10   | Top-1 Accuracy: 0.8163636363636364            
Batch: #11   | Top-1 Accuracy: 0.8152892561983471            
Batch: #12   | Top-1 Accuracy: 0.8136363636363636            
Batch: #13   | Top-1 Accuracy: 0.8153846153846154            
Batch: #14   | Top-1 Accuracy: 0.8136363636363636            
Batch: #15   | Top-1 Accuracy: 0.8127272727272727            
Batch: #16   | Top-1 Accuracy: 0.8144886363636363            
Batch: #17   | Top-1 Accuracy: 0.8136363636363636            

Classifier Training Epoch #4
------------------------------------------
Epoch: #4    | Batch: #1    | Batch Loss: 0.6112874150276184            | Epoch Loss: 0.6112874150276184            
Epoch: #4    | Batch: #2    | Batch Loss: 0.7002843618392944            | Epoch Loss: 0.6557858884334564            
Epoch: #4    | Batch: #3    | Batch Loss: 0.7596808075904846            | Epoch Loss: 0.6904175281524658            
Epoch: #4    | Batch: #4    | Batch Loss: 0.7917459011077881            | Epoch Loss: 0.7157496213912964            
Epoch: #4    | Batch: #5    | Batch Loss: 0.7454788088798523            | Epoch Loss: 0.7216954588890075            
Epoch: #4    | Batch: #6    | Batch Loss: 0.6106482744216919            | Epoch Loss: 0.7031875948111216            
Epoch: #4    | Batch: #7    | Batch Loss: 0.618918776512146             | Epoch Loss: 0.6911491921969822            
Epoch: #4    | Batch: #8    | Batch Loss: 0.634567379951477             | Epoch Loss: 0.6840764656662941            
Epoch: #4    | Batch: #9    | Batch Loss: 0.9110679626464844            | Epoch Loss: 0.7092977431085374            
Epoch: #4    | Batch: #10   | Batch Loss: 0.684529185295105             | Epoch Loss: 0.7068208873271942            
Epoch: #4    | Batch: #11   | Batch Loss: 0.7229287028312683            | Epoch Loss: 0.7082852341912009            
Epoch: #4    | Batch: #12   | Batch Loss: 0.7755640149116516            | Epoch Loss: 0.7138917992512385            
Epoch: #4    | Batch: #13   | Batch Loss: 0.6838884353637695            | Epoch Loss: 0.7115838481829717            
Epoch: #4    | Batch: #14   | Batch Loss: 0.6889348030090332            | Epoch Loss: 0.7099660592419761            
Epoch: #4    | Batch: #15   | Batch Loss: 0.6602669358253479            | Epoch Loss: 0.7066527843475342            
Epoch: #4    | Batch: #16   | Batch Loss: 0.6181942224502563            | Epoch Loss: 0.7011241242289543            
Epoch: #4    | Batch: #17   | Batch Loss: 0.6058144569396973            | Epoch Loss: 0.6955176732119392            
Epoch: #4    | Batch: #18   | Batch Loss: 0.6526426076889038            | Epoch Loss: 0.6931357251273261            
Epoch: #4    | Batch: #19   | Batch Loss: 0.6555039286613464            | Epoch Loss: 0.6911551042606956            
Epoch: #4    | Batch: #20   | Batch Loss: 0.6659234762191772            | Epoch Loss: 0.6898935228586197            
Epoch: #4    | Batch: #21   | Batch Loss: 0.46998217701911926           | Epoch Loss: 0.6794215540091196            
Epoch: #4    | Batch: #22   | Batch Loss: 0.7860803008079529            | Epoch Loss: 0.6842696788636121            
Epoch: #4    | Batch: #23   | Batch Loss: 0.6056646108627319            | Epoch Loss: 0.6808520672113999            
Epoch: #4    | Batch: #24   | Batch Loss: 0.7194607853889465            | Epoch Loss: 0.682460763802131             
Epoch: #4    | Batch: #25   | Batch Loss: 0.7053140997886658            | Epoch Loss: 0.6833748972415924            
Epoch: #4    | Batch: #26   | Batch Loss: 0.6973458528518677            | Epoch Loss: 0.6839122416881415            
Epoch: #4    | Batch: #27   | Batch Loss: 0.5682865977287292            | Epoch Loss: 0.6796298104303854            
Epoch: #4    | Batch: #28   | Batch Loss: 0.7562677264213562            | Epoch Loss: 0.6823668788586345            
Epoch: #4    | Batch: #29   | Batch Loss: 0.7036226391792297            | Epoch Loss: 0.6830998361110687            
Epoch: #4    | Batch: #30   | Batch Loss: 0.6314795613288879            | Epoch Loss: 0.681379160284996             
Epoch: #4    | Batch: #31   | Batch Loss: 0.6992074251174927            | Epoch Loss: 0.6819542656021733            
Epoch: #4    | Batch: #32   | Batch Loss: 0.7737970352172852            | Epoch Loss: 0.6848243521526456            
Epoch: #4    | Batch: #33   | Batch Loss: 0.6731926798820496            | Epoch Loss: 0.6844718772353549            
Epoch: #4    | Batch: #34   | Batch Loss: 0.6955516338348389            | Epoch Loss: 0.6847977524294573            
Epoch: #4    | Batch: #35   | Batch Loss: 0.7137857675552368            | Epoch Loss: 0.685625981433051             
Epoch: #4    | Batch: #36   | Batch Loss: 0.6948808431625366            | Epoch Loss: 0.6858830609255366            
Epoch: #4    | Batch: #37   | Batch Loss: 0.6418578624725342            | Epoch Loss: 0.6846931906970771            
Epoch: #4    | Batch: #38   | Batch Loss: 0.7062150835990906            | Epoch Loss: 0.6852595562997618            
Epoch: #4    | Batch: #39   | Batch Loss: 0.694862961769104             | Epoch Loss: 0.6855057974656423            
Epoch: #4    | Batch: #40   | Batch Loss: 0.6050401926040649            | Epoch Loss: 0.6834941573441029            
Epoch: #4    | Batch: #41   | Batch Loss: 0.6210393309593201            | Epoch Loss: 0.6819708688956935            
Epoch: #4    | Batch: #42   | Batch Loss: 0.6148331165313721            | Epoch Loss: 0.6803723509822573            
Epoch: #4    | Batch: #43   | Batch Loss: 0.800236165523529             | Epoch Loss: 0.6831598815529846            

Classifier Validation Epoch #4
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8136363636363636            
Batch: #2    | Top-1 Accuracy: 0.8022727272727272            
Batch: #3    | Top-1 Accuracy: 0.7969696969696969            
Batch: #4    | Top-1 Accuracy: 0.7920454545454545            
Batch: #5    | Top-1 Accuracy: 0.7972727272727272            
Batch: #6    | Top-1 Accuracy: 0.8030303030303031            
Batch: #7    | Top-1 Accuracy: 0.8071428571428572            
Batch: #8    | Top-1 Accuracy: 0.8136363636363636            
Batch: #9    | Top-1 Accuracy: 0.8126262626262626            
Batch: #10   | Top-1 Accuracy: 0.8118181818181818            
Batch: #11   | Top-1 Accuracy: 0.8128099173553718            
Batch: #12   | Top-1 Accuracy: 0.8151515151515151            
Batch: #13   | Top-1 Accuracy: 0.8101398601398601            
Batch: #14   | Top-1 Accuracy: 0.8084415584415584            
Batch: #15   | Top-1 Accuracy: 0.8081818181818182            
Batch: #16   | Top-1 Accuracy: 0.8079545454545455            
Batch: #17   | Top-1 Accuracy: 0.8085561497326204            

Classifier Training Epoch #5
------------------------------------------
Epoch: #5    | Batch: #1    | Batch Loss: 0.726168692111969             | Epoch Loss: 0.726168692111969             
Epoch: #5    | Batch: #2    | Batch Loss: 0.6792264580726624            | Epoch Loss: 0.7026975750923157            
Epoch: #5    | Batch: #3    | Batch Loss: 0.8198630809783936            | Epoch Loss: 0.7417527437210083            
Epoch: #5    | Batch: #4    | Batch Loss: 0.5996872186660767            | Epoch Loss: 0.7062363624572754            
Epoch: #5    | Batch: #5    | Batch Loss: 0.6169682145118713            | Epoch Loss: 0.6883827328681946            
Epoch: #5    | Batch: #6    | Batch Loss: 0.6223779916763306            | Epoch Loss: 0.6773819426695505            
Epoch: #5    | Batch: #7    | Batch Loss: 0.6212830543518066            | Epoch Loss: 0.6693678157670158            
Epoch: #5    | Batch: #8    | Batch Loss: 0.7651177048683167            | Epoch Loss: 0.6813365519046783            
Epoch: #5    | Batch: #9    | Batch Loss: 0.7679399847984314            | Epoch Loss: 0.6909591555595398            
Epoch: #5    | Batch: #10   | Batch Loss: 0.7238408327102661            | Epoch Loss: 0.6942473232746125            
Epoch: #5    | Batch: #11   | Batch Loss: 0.5984033942222595            | Epoch Loss: 0.6855342388153076            
Epoch: #5    | Batch: #12   | Batch Loss: 0.6151873469352722            | Epoch Loss: 0.6796719978253046            
Epoch: #5    | Batch: #13   | Batch Loss: 0.5987793207168579            | Epoch Loss: 0.673449484201578             
Epoch: #5    | Batch: #14   | Batch Loss: 0.7444131970405579            | Epoch Loss: 0.6785183208329337            
Epoch: #5    | Batch: #15   | Batch Loss: 0.684747576713562             | Epoch Loss: 0.6789336045583089            
Epoch: #5    | Batch: #16   | Batch Loss: 0.6671956777572632            | Epoch Loss: 0.6781999841332436            
Epoch: #5    | Batch: #17   | Batch Loss: 0.5818562507629395            | Epoch Loss: 0.6725327056996963            
Epoch: #5    | Batch: #18   | Batch Loss: 0.5380494594573975            | Epoch Loss: 0.6650614142417908            
Epoch: #5    | Batch: #19   | Batch Loss: 0.6714907288551331            | Epoch Loss: 0.6653997992214403            
Epoch: #5    | Batch: #20   | Batch Loss: 0.7013678550720215            | Epoch Loss: 0.6671982020139694            
Epoch: #5    | Batch: #21   | Batch Loss: 0.6896612048149109            | Epoch Loss: 0.6682678688140142            
Epoch: #5    | Batch: #22   | Batch Loss: 0.6252927780151367            | Epoch Loss: 0.6663144555958834            
Epoch: #5    | Batch: #23   | Batch Loss: 0.6926632523536682            | Epoch Loss: 0.6674600554549176            
Epoch: #5    | Batch: #24   | Batch Loss: 0.6686686873435974            | Epoch Loss: 0.6675104151169459            
Epoch: #5    | Batch: #25   | Batch Loss: 0.6944151520729065            | Epoch Loss: 0.6685866045951844            
Epoch: #5    | Batch: #26   | Batch Loss: 0.7397422194480896            | Epoch Loss: 0.6713233590126038            
Epoch: #5    | Batch: #27   | Batch Loss: 0.8398543000221252            | Epoch Loss: 0.6775652457166601            
Epoch: #5    | Batch: #28   | Batch Loss: 0.6716534495353699            | Epoch Loss: 0.6773541101387569            
Epoch: #5    | Batch: #29   | Batch Loss: 0.6041250824928284            | Epoch Loss: 0.6748289712544145            
Epoch: #5    | Batch: #30   | Batch Loss: 0.7516597509384155            | Epoch Loss: 0.6773899972438813            
Epoch: #5    | Batch: #31   | Batch Loss: 0.6737454533576965            | Epoch Loss: 0.6772724313120688            
Epoch: #5    | Batch: #32   | Batch Loss: 0.6307789087295532            | Epoch Loss: 0.6758195087313652            
Epoch: #5    | Batch: #33   | Batch Loss: 0.7085542678833008            | Epoch Loss: 0.6768114711299087            
Epoch: #5    | Batch: #34   | Batch Loss: 0.7118772268295288            | Epoch Loss: 0.6778428168857799            
Epoch: #5    | Batch: #35   | Batch Loss: 0.7024005651473999            | Epoch Loss: 0.6785444668361119            
Epoch: #5    | Batch: #36   | Batch Loss: 0.8138583302497864            | Epoch Loss: 0.6823031852642695            
Epoch: #5    | Batch: #37   | Batch Loss: 0.6628544330596924            | Epoch Loss: 0.6817775433127945            
Epoch: #5    | Batch: #38   | Batch Loss: 0.6361405849456787            | Epoch Loss: 0.6805765707241861            
Epoch: #5    | Batch: #39   | Batch Loss: 0.6687412858009338            | Epoch Loss: 0.6802731018800002            
Epoch: #5    | Batch: #40   | Batch Loss: 0.719846248626709             | Epoch Loss: 0.6812624305486679            
Epoch: #5    | Batch: #41   | Batch Loss: 0.7083625197410583            | Epoch Loss: 0.6819234083338481            
Epoch: #5    | Batch: #42   | Batch Loss: 0.6516351103782654            | Epoch Loss: 0.6812022583825248            
Epoch: #5    | Batch: #43   | Batch Loss: 0.6144087314605713            | Epoch Loss: 0.6796489205471304            

Classifier Validation Epoch #5
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8045454545454546            
Batch: #2    | Top-1 Accuracy: 0.8136363636363637            
Batch: #3    | Top-1 Accuracy: 0.796969696969697             
Batch: #4    | Top-1 Accuracy: 0.8011363636363636            
Batch: #5    | Top-1 Accuracy: 0.8036363636363637            
Batch: #6    | Top-1 Accuracy: 0.806818181818182             
Batch: #7    | Top-1 Accuracy: 0.8077922077922078            
Batch: #8    | Top-1 Accuracy: 0.8085227272727273            
Batch: #9    | Top-1 Accuracy: 0.8111111111111112            
Batch: #10   | Top-1 Accuracy: 0.8072727272727274            
Batch: #11   | Top-1 Accuracy: 0.8111570247933885            
Batch: #12   | Top-1 Accuracy: 0.8117424242424244            
Batch: #13   | Top-1 Accuracy: 0.8132867132867134            
Batch: #14   | Top-1 Accuracy: 0.8120129870129871            
Batch: #15   | Top-1 Accuracy: 0.8160606060606062            
Batch: #16   | Top-1 Accuracy: 0.8144886363636363            
Batch: #17   | Top-1 Accuracy: 0.8155080213903743            

Classifier Training Epoch #6
------------------------------------------
Epoch: #6    | Batch: #1    | Batch Loss: 0.7838613986968994            | Epoch Loss: 0.7838613986968994            
Epoch: #6    | Batch: #2    | Batch Loss: 0.7503389120101929            | Epoch Loss: 0.7671001553535461            
Epoch: #6    | Batch: #3    | Batch Loss: 0.4936390519142151            | Epoch Loss: 0.6759464542071024            
Epoch: #6    | Batch: #4    | Batch Loss: 0.6681825518608093            | Epoch Loss: 0.6740054786205292            
Epoch: #6    | Batch: #5    | Batch Loss: 0.6851165890693665            | Epoch Loss: 0.6762277007102966            
Epoch: #6    | Batch: #6    | Batch Loss: 0.8006492853164673            | Epoch Loss: 0.6969646314779917            
Epoch: #6    | Batch: #7    | Batch Loss: 0.6530040502548218            | Epoch Loss: 0.6906845484461103            
Epoch: #6    | Batch: #8    | Batch Loss: 0.7143502235412598            | Epoch Loss: 0.693642757833004             
Epoch: #6    | Batch: #9    | Batch Loss: 0.608465313911438             | Epoch Loss: 0.6841785973972745            
Epoch: #6    | Batch: #10   | Batch Loss: 0.8443614840507507            | Epoch Loss: 0.7001968860626221            
Epoch: #6    | Batch: #11   | Batch Loss: 0.692422091960907             | Epoch Loss: 0.6994900865988298            
Epoch: #6    | Batch: #12   | Batch Loss: 0.590634822845459             | Epoch Loss: 0.6904188146193823            
Epoch: #6    | Batch: #13   | Batch Loss: 0.6156964898109436            | Epoch Loss: 0.6846709434802716            
Epoch: #6    | Batch: #14   | Batch Loss: 0.6578729152679443            | Epoch Loss: 0.6827567986079625            
Epoch: #6    | Batch: #15   | Batch Loss: 0.6766630411148071            | Epoch Loss: 0.6823505481084188            
Epoch: #6    | Batch: #16   | Batch Loss: 0.6938486695289612            | Epoch Loss: 0.6830691806972027            
Epoch: #6    | Batch: #17   | Batch Loss: 0.5522417426109314            | Epoch Loss: 0.6753734490450691            
Epoch: #6    | Batch: #18   | Batch Loss: 0.5946817994117737            | Epoch Loss: 0.6708905796209971            
Epoch: #6    | Batch: #19   | Batch Loss: 0.7076332569122314            | Epoch Loss: 0.6728244047415884            
Epoch: #6    | Batch: #20   | Batch Loss: 0.6241827011108398            | Epoch Loss: 0.670392319560051             
Epoch: #6    | Batch: #21   | Batch Loss: 0.545763373374939             | Epoch Loss: 0.6644576078369504            
Epoch: #6    | Batch: #22   | Batch Loss: 0.6791754364967346            | Epoch Loss: 0.6651266000487588            
Epoch: #6    | Batch: #23   | Batch Loss: 0.6447672843933105            | Epoch Loss: 0.6642414124115653            
Epoch: #6    | Batch: #24   | Batch Loss: 0.5164126753807068            | Epoch Loss: 0.6580818817019463            
Epoch: #6    | Batch: #25   | Batch Loss: 0.7413800954818726            | Epoch Loss: 0.6614138102531433            
Epoch: #6    | Batch: #26   | Batch Loss: 0.557390034198761             | Epoch Loss: 0.6574128957895132            
Epoch: #6    | Batch: #27   | Batch Loss: 0.596167266368866             | Epoch Loss: 0.655144539144304             
Epoch: #6    | Batch: #28   | Batch Loss: 0.643818736076355             | Epoch Loss: 0.6547400461775916            
Epoch: #6    | Batch: #29   | Batch Loss: 0.6976000070571899            | Epoch Loss: 0.6562179758630949            
Epoch: #6    | Batch: #30   | Batch Loss: 0.7539644837379456            | Epoch Loss: 0.6594761927922567            
Epoch: #6    | Batch: #31   | Batch Loss: 0.74408358335495              | Epoch Loss: 0.6622054634555694            
Epoch: #6    | Batch: #32   | Batch Loss: 0.6026097536087036            | Epoch Loss: 0.6603430975228548            
Epoch: #6    | Batch: #33   | Batch Loss: 0.5833346843719482            | Epoch Loss: 0.6580095092455546            
Epoch: #6    | Batch: #34   | Batch Loss: 0.6504064798355103            | Epoch Loss: 0.6577858907334945            
Epoch: #6    | Batch: #35   | Batch Loss: 0.619462251663208             | Epoch Loss: 0.6566909296172005            
Epoch: #6    | Batch: #36   | Batch Loss: 0.6324015259742737            | Epoch Loss: 0.6560162239604526            
Epoch: #6    | Batch: #37   | Batch Loss: 0.6439021229743958            | Epoch Loss: 0.6556888158256943            
Epoch: #6    | Batch: #38   | Batch Loss: 0.6257321238517761            | Epoch Loss: 0.6549004818263807            
Epoch: #6    | Batch: #39   | Batch Loss: 0.542981743812561             | Epoch Loss: 0.652030770595257             
Epoch: #6    | Batch: #40   | Batch Loss: 0.5510369539260864            | Epoch Loss: 0.6495059251785278            
Epoch: #6    | Batch: #41   | Batch Loss: 0.6266884803771973            | Epoch Loss: 0.648949402134593             
Epoch: #6    | Batch: #42   | Batch Loss: 0.7613095641136169            | Epoch Loss: 0.6516246440864745            
Epoch: #6    | Batch: #43   | Batch Loss: 0.5959925651550293            | Epoch Loss: 0.650330874808999             

Classifier Validation Epoch #6
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8318181818181818            
Batch: #2    | Top-1 Accuracy: 0.8227272727272728            
Batch: #3    | Top-1 Accuracy: 0.8121212121212121            
Batch: #4    | Top-1 Accuracy: 0.8215909090909091            
Batch: #5    | Top-1 Accuracy: 0.8272727272727274            
Batch: #6    | Top-1 Accuracy: 0.8287878787878787            
Batch: #7    | Top-1 Accuracy: 0.8318181818181818            
Batch: #8    | Top-1 Accuracy: 0.8335227272727272            
Batch: #9    | Top-1 Accuracy: 0.8318181818181818            
Batch: #10   | Top-1 Accuracy: 0.8272727272727274            
Batch: #11   | Top-1 Accuracy: 0.8231404958677686            
Batch: #12   | Top-1 Accuracy: 0.8238636363636364            
Batch: #13   | Top-1 Accuracy: 0.8248251748251748            
Batch: #14   | Top-1 Accuracy: 0.8240259740259741            
Batch: #15   | Top-1 Accuracy: 0.8251515151515153            
Batch: #16   | Top-1 Accuracy: 0.825                         
Batch: #17   | Top-1 Accuracy: 0.8272727272727272            

Classifier Training Epoch #7
------------------------------------------
Epoch: #7    | Batch: #1    | Batch Loss: 0.6939010620117188            | Epoch Loss: 0.6939010620117188            
Epoch: #7    | Batch: #2    | Batch Loss: 0.5640299916267395            | Epoch Loss: 0.6289655268192291            
Epoch: #7    | Batch: #3    | Batch Loss: 0.7495800852775574            | Epoch Loss: 0.6691703796386719            
Epoch: #7    | Batch: #4    | Batch Loss: 0.5388059616088867            | Epoch Loss: 0.6365792751312256            
Epoch: #7    | Batch: #5    | Batch Loss: 0.7551833391189575            | Epoch Loss: 0.6603000879287719            
Epoch: #7    | Batch: #6    | Batch Loss: 0.6972450017929077            | Epoch Loss: 0.6664575735727946            
Epoch: #7    | Batch: #7    | Batch Loss: 0.6238207817077637            | Epoch Loss: 0.6603666033063617            
Epoch: #7    | Batch: #8    | Batch Loss: 0.6702833771705627            | Epoch Loss: 0.6616062000393867            
Epoch: #7    | Batch: #9    | Batch Loss: 0.6059645414352417            | Epoch Loss: 0.6554237935278151            
Epoch: #7    | Batch: #10   | Batch Loss: 0.656886637210846             | Epoch Loss: 0.6555700778961182            
Epoch: #7    | Batch: #11   | Batch Loss: 0.624496340751648             | Epoch Loss: 0.6527451927011664            
Epoch: #7    | Batch: #12   | Batch Loss: 0.5240821838378906            | Epoch Loss: 0.6420232752958933            
Epoch: #7    | Batch: #13   | Batch Loss: 0.7057982087135315            | Epoch Loss: 0.6469290394049424            
Epoch: #7    | Batch: #14   | Batch Loss: 0.673292338848114             | Epoch Loss: 0.6488121322223118            
Epoch: #7    | Batch: #15   | Batch Loss: 0.6688002943992615            | Epoch Loss: 0.6501446763674418            
Epoch: #7    | Batch: #16   | Batch Loss: 0.6624018549919128            | Epoch Loss: 0.6509107500314713            
Epoch: #7    | Batch: #17   | Batch Loss: 0.714572548866272             | Epoch Loss: 0.654655561727636             
Epoch: #7    | Batch: #18   | Batch Loss: 0.5713719725608826            | Epoch Loss: 0.6500286956628164            
Epoch: #7    | Batch: #19   | Batch Loss: 0.6108883023262024            | Epoch Loss: 0.6479686749608893            
Epoch: #7    | Batch: #20   | Batch Loss: 0.6348676681518555            | Epoch Loss: 0.6473136246204376            
Epoch: #7    | Batch: #21   | Batch Loss: 0.7714865207672119            | Epoch Loss: 0.6532266196750459            
Epoch: #7    | Batch: #22   | Batch Loss: 0.7880746722221375            | Epoch Loss: 0.6593560766090046            
Epoch: #7    | Batch: #23   | Batch Loss: 0.7432023286819458            | Epoch Loss: 0.6630015658295673            
Epoch: #7    | Batch: #24   | Batch Loss: 0.6100205779075623            | Epoch Loss: 0.6607940246661504            
Epoch: #7    | Batch: #25   | Batch Loss: 0.6179491281509399            | Epoch Loss: 0.659080228805542             
Epoch: #7    | Batch: #26   | Batch Loss: 0.6596856117248535            | Epoch Loss: 0.659103512763977             
Epoch: #7    | Batch: #27   | Batch Loss: 0.5565189719200134            | Epoch Loss: 0.6553040853253117            
Epoch: #7    | Batch: #28   | Batch Loss: 0.6638174653053284            | Epoch Loss: 0.6556081346103123            
Epoch: #7    | Batch: #29   | Batch Loss: 0.7112215757369995            | Epoch Loss: 0.6575258394767498            
Epoch: #7    | Batch: #30   | Batch Loss: 0.6199702024459839            | Epoch Loss: 0.6562739849090576            
Epoch: #7    | Batch: #31   | Batch Loss: 0.6379750967025757            | Epoch Loss: 0.6556836981927195            
Epoch: #7    | Batch: #32   | Batch Loss: 0.6380590796470642            | Epoch Loss: 0.6551329288631678            
Epoch: #7    | Batch: #33   | Batch Loss: 0.6084417104721069            | Epoch Loss: 0.653718043457378             
Epoch: #7    | Batch: #34   | Batch Loss: 0.619255006313324             | Epoch Loss: 0.6527044247178471            
Epoch: #7    | Batch: #35   | Batch Loss: 0.7476396560668945            | Epoch Loss: 0.6554168598992484            
Epoch: #7    | Batch: #36   | Batch Loss: 0.5737317800521851            | Epoch Loss: 0.6531478299034966            
Epoch: #7    | Batch: #37   | Batch Loss: 0.6359493732452393            | Epoch Loss: 0.6526830067505708            
Epoch: #7    | Batch: #38   | Batch Loss: 0.7203819751739502            | Epoch Loss: 0.654464558551186             
Epoch: #7    | Batch: #39   | Batch Loss: 0.6688203811645508            | Epoch Loss: 0.6548326565669134            
Epoch: #7    | Batch: #40   | Batch Loss: 0.6522006392478943            | Epoch Loss: 0.6547668561339378            
Epoch: #7    | Batch: #41   | Batch Loss: 0.5550277829170227            | Epoch Loss: 0.652334195811574             
Epoch: #7    | Batch: #42   | Batch Loss: 0.7301771640777588            | Epoch Loss: 0.6541875998179117            
Epoch: #7    | Batch: #43   | Batch Loss: 0.6332671642303467            | Epoch Loss: 0.6537010780600614            

Classifier Validation Epoch #7
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8318181818181818            
Batch: #2    | Top-1 Accuracy: 0.834090909090909             
Batch: #3    | Top-1 Accuracy: 0.8257575757575757            
Batch: #4    | Top-1 Accuracy: 0.8329545454545454            
Batch: #5    | Top-1 Accuracy: 0.8281818181818181            
Batch: #6    | Top-1 Accuracy: 0.8287878787878787            
Batch: #7    | Top-1 Accuracy: 0.8344155844155844            
Batch: #8    | Top-1 Accuracy: 0.8335227272727272            
Batch: #9    | Top-1 Accuracy: 0.8338383838383838            
Batch: #10   | Top-1 Accuracy: 0.8299999999999998            
Batch: #11   | Top-1 Accuracy: 0.8301652892561983            
Batch: #12   | Top-1 Accuracy: 0.8299242424242425            
Batch: #13   | Top-1 Accuracy: 0.8300699300699301            
Batch: #14   | Top-1 Accuracy: 0.8295454545454545            
Batch: #15   | Top-1 Accuracy: 0.8248484848484848            
Batch: #16   | Top-1 Accuracy: 0.8238636363636362            
Batch: #17   | Top-1 Accuracy: 0.8216577540106951            

Classifier Training Epoch #8
------------------------------------------
Epoch: #8    | Batch: #1    | Batch Loss: 0.670265793800354             | Epoch Loss: 0.670265793800354             
Epoch: #8    | Batch: #2    | Batch Loss: 0.5533729791641235            | Epoch Loss: 0.6118193864822388            
Epoch: #8    | Batch: #3    | Batch Loss: 0.7382944226264954            | Epoch Loss: 0.6539777318636576            
Epoch: #8    | Batch: #4    | Batch Loss: 0.5454168319702148            | Epoch Loss: 0.6268375068902969            
Epoch: #8    | Batch: #5    | Batch Loss: 0.6095542907714844            | Epoch Loss: 0.6233808636665344            
Epoch: #8    | Batch: #6    | Batch Loss: 0.6826688647270203            | Epoch Loss: 0.6332621971766154            
Epoch: #8    | Batch: #7    | Batch Loss: 0.6052775382995605            | Epoch Loss: 0.6292643887656075            
Epoch: #8    | Batch: #8    | Batch Loss: 0.6974504590034485            | Epoch Loss: 0.6377876475453377            
Epoch: #8    | Batch: #9    | Batch Loss: 0.524586021900177             | Epoch Loss: 0.6252096891403198            
Epoch: #8    | Batch: #10   | Batch Loss: 0.4905104637145996            | Epoch Loss: 0.6117397665977478            
Epoch: #8    | Batch: #11   | Batch Loss: 0.6263842582702637            | Epoch Loss: 0.613071084022522             
Epoch: #8    | Batch: #12   | Batch Loss: 0.555621325969696             | Epoch Loss: 0.6082836041847864            
Epoch: #8    | Batch: #13   | Batch Loss: 0.5745143294334412            | Epoch Loss: 0.6056859676654522            
Epoch: #8    | Batch: #14   | Batch Loss: 0.5010322332382202            | Epoch Loss: 0.59821070092065              
Epoch: #8    | Batch: #15   | Batch Loss: 0.6436060070991516            | Epoch Loss: 0.6012370546658834            
Epoch: #8    | Batch: #16   | Batch Loss: 0.6430656909942627            | Epoch Loss: 0.6038513444364071            
Epoch: #8    | Batch: #17   | Batch Loss: 0.6908144354820251            | Epoch Loss: 0.608966820380267             
Epoch: #8    | Batch: #18   | Batch Loss: 0.6593183875083923            | Epoch Loss: 0.6117641296651628            
Epoch: #8    | Batch: #19   | Batch Loss: 0.7215389013290405            | Epoch Loss: 0.6175417492264196            
Epoch: #8    | Batch: #20   | Batch Loss: 0.6873089671134949            | Epoch Loss: 0.6210301101207734            
Epoch: #8    | Batch: #21   | Batch Loss: 0.6979889869689941            | Epoch Loss: 0.6246948185421172            
Epoch: #8    | Batch: #22   | Batch Loss: 0.7199608683586121            | Epoch Loss: 0.629025093533776             
Epoch: #8    | Batch: #23   | Batch Loss: 0.4929211139678955            | Epoch Loss: 0.6231075292048247            
Epoch: #8    | Batch: #24   | Batch Loss: 0.5769790410995483            | Epoch Loss: 0.6211855088671049            
Epoch: #8    | Batch: #25   | Batch Loss: 0.5494399070739746            | Epoch Loss: 0.6183156847953797            
Epoch: #8    | Batch: #26   | Batch Loss: 0.5398455858230591            | Epoch Loss: 0.615297604065675             
Epoch: #8    | Batch: #27   | Batch Loss: 0.6592427492141724            | Epoch Loss: 0.6169252020341379            
Epoch: #8    | Batch: #28   | Batch Loss: 0.7134017944335938            | Epoch Loss: 0.6203707946198327            
Epoch: #8    | Batch: #29   | Batch Loss: 0.6408058404922485            | Epoch Loss: 0.621075451374054             
Epoch: #8    | Batch: #30   | Batch Loss: 0.7214246988296509            | Epoch Loss: 0.6244204262892405            
Epoch: #8    | Batch: #31   | Batch Loss: 0.6443884968757629            | Epoch Loss: 0.6250645575984832            
Epoch: #8    | Batch: #32   | Batch Loss: 0.7115617394447327            | Epoch Loss: 0.6277675945311785            
Epoch: #8    | Batch: #33   | Batch Loss: 0.5785425901412964            | Epoch Loss: 0.6262759277314851            
Epoch: #8    | Batch: #34   | Batch Loss: 0.6382347941398621            | Epoch Loss: 0.6266276590964374            
Epoch: #8    | Batch: #35   | Batch Loss: 0.8311386108398438            | Epoch Loss: 0.632470829146249             
Epoch: #8    | Batch: #36   | Batch Loss: 0.7224794626235962            | Epoch Loss: 0.6349710689650642            
Epoch: #8    | Batch: #37   | Batch Loss: 0.5559464693069458            | Epoch Loss: 0.6328352689743042            
Epoch: #8    | Batch: #38   | Batch Loss: 0.7274369597434998            | Epoch Loss: 0.6353247871524409            
Epoch: #8    | Batch: #39   | Batch Loss: 0.5433187484741211            | Epoch Loss: 0.6329656579555609            
Epoch: #8    | Batch: #40   | Batch Loss: 0.5411520004272461            | Epoch Loss: 0.6306703165173531            
Epoch: #8    | Batch: #41   | Batch Loss: 0.5660067200660706            | Epoch Loss: 0.6290931556282974            
Epoch: #8    | Batch: #42   | Batch Loss: 0.6378054022789001            | Epoch Loss: 0.6293005900723594            
Epoch: #8    | Batch: #43   | Batch Loss: 0.5832170248031616            | Epoch Loss: 0.6282288792521454            

Classifier Validation Epoch #8
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8272727272727273            
Batch: #2    | Top-1 Accuracy: 0.8295454545454546            
Batch: #3    | Top-1 Accuracy: 0.8181818181818182            
Batch: #4    | Top-1 Accuracy: 0.8136363636363637            
Batch: #5    | Top-1 Accuracy: 0.8172727272727274            
Batch: #6    | Top-1 Accuracy: 0.8204545454545454            
Batch: #7    | Top-1 Accuracy: 0.8227272727272726            
Batch: #8    | Top-1 Accuracy: 0.8267045454545454            
Batch: #9    | Top-1 Accuracy: 0.8252525252525252            
Batch: #10   | Top-1 Accuracy: 0.8218181818181817            
Batch: #11   | Top-1 Accuracy: 0.8227272727272726            
Batch: #12   | Top-1 Accuracy: 0.8249999999999998            
Batch: #13   | Top-1 Accuracy: 0.8258741258741258            
Batch: #14   | Top-1 Accuracy: 0.8233766233766232            
Batch: #15   | Top-1 Accuracy: 0.8254545454545453            
Batch: #16   | Top-1 Accuracy: 0.8275568181818181            
Batch: #17   | Top-1 Accuracy: 0.827807486631016             

Classifier Training Epoch #9
------------------------------------------
Epoch: #9    | Batch: #1    | Batch Loss: 0.6406813263893127            | Epoch Loss: 0.6406813263893127            
Epoch: #9    | Batch: #2    | Batch Loss: 0.5602396726608276            | Epoch Loss: 0.6004604995250702            
Epoch: #9    | Batch: #3    | Batch Loss: 0.7467455863952637            | Epoch Loss: 0.649222195148468             
Epoch: #9    | Batch: #4    | Batch Loss: 0.543318510055542             | Epoch Loss: 0.6227462738752365            
Epoch: #9    | Batch: #5    | Batch Loss: 0.7089789509773254            | Epoch Loss: 0.6399928092956543            
Epoch: #9    | Batch: #6    | Batch Loss: 0.6597047448158264            | Epoch Loss: 0.6432781318823496            
Epoch: #9    | Batch: #7    | Batch Loss: 0.8005911111831665            | Epoch Loss: 0.6657514146396092            
Epoch: #9    | Batch: #8    | Batch Loss: 0.5856139659881592            | Epoch Loss: 0.655734233558178             
Epoch: #9    | Batch: #9    | Batch Loss: 0.7174647450447083            | Epoch Loss: 0.6625931792789035            
Epoch: #9    | Batch: #10   | Batch Loss: 0.573415219783783             | Epoch Loss: 0.6536753833293915            
Epoch: #9    | Batch: #11   | Batch Loss: 0.6020559668540955            | Epoch Loss: 0.6489827091043646            
Epoch: #9    | Batch: #12   | Batch Loss: 0.5796244740486145            | Epoch Loss: 0.6432028561830521            
Epoch: #9    | Batch: #13   | Batch Loss: 0.6543986201286316            | Epoch Loss: 0.6440640687942505            
Epoch: #9    | Batch: #14   | Batch Loss: 0.7118005156517029            | Epoch Loss: 0.6489023864269257            
Epoch: #9    | Batch: #15   | Batch Loss: 0.47767144441604614           | Epoch Loss: 0.637486990292867             
Epoch: #9    | Batch: #16   | Batch Loss: 0.6686444282531738            | Epoch Loss: 0.6394343301653862            
Epoch: #9    | Batch: #17   | Batch Loss: 0.6103114485740662            | Epoch Loss: 0.6377212194835439            
Epoch: #9    | Batch: #18   | Batch Loss: 0.5194749236106873            | Epoch Loss: 0.6311519808239408            
Epoch: #9    | Batch: #19   | Batch Loss: 0.49615412950515747           | Epoch Loss: 0.624046830754531             
Epoch: #9    | Batch: #20   | Batch Loss: 0.7241911292076111            | Epoch Loss: 0.629054045677185             
Epoch: #9    | Batch: #21   | Batch Loss: 0.643838107585907             | Epoch Loss: 0.6297580486252194            
Epoch: #9    | Batch: #22   | Batch Loss: 0.6519044637680054            | Epoch Loss: 0.6307647038589824            
Epoch: #9    | Batch: #23   | Batch Loss: 0.5323876142501831            | Epoch Loss: 0.6264874390933824            
Epoch: #9    | Batch: #24   | Batch Loss: 0.5708480477333069            | Epoch Loss: 0.624169131120046             
Epoch: #9    | Batch: #25   | Batch Loss: 0.6004377603530884            | Epoch Loss: 0.6232198762893677            
Epoch: #9    | Batch: #26   | Batch Loss: 0.619210422039032             | Epoch Loss: 0.6230656665105087            
Epoch: #9    | Batch: #27   | Batch Loss: 0.5840228199958801            | Epoch Loss: 0.6216196351581149            
Epoch: #9    | Batch: #28   | Batch Loss: 0.6035411953926086            | Epoch Loss: 0.6209739765950612            
Epoch: #9    | Batch: #29   | Batch Loss: 0.7008992433547974            | Epoch Loss: 0.6237300202764314            
Epoch: #9    | Batch: #30   | Batch Loss: 0.5173112750053406            | Epoch Loss: 0.620182728767395             
Epoch: #9    | Batch: #31   | Batch Loss: 0.5086488723754883            | Epoch Loss: 0.6165848624321723            
Epoch: #9    | Batch: #32   | Batch Loss: 0.6462873220443726            | Epoch Loss: 0.6175130642950535            
Epoch: #9    | Batch: #33   | Batch Loss: 0.5857511162757874            | Epoch Loss: 0.6165505810217424            
Epoch: #9    | Batch: #34   | Batch Loss: 0.6734628081321716            | Epoch Loss: 0.6182244700544021            
Epoch: #9    | Batch: #35   | Batch Loss: 0.5586422085762024            | Epoch Loss: 0.6165221197264535            
Epoch: #9    | Batch: #36   | Batch Loss: 0.5798158049583435            | Epoch Loss: 0.6155024998717837            
Epoch: #9    | Batch: #37   | Batch Loss: 0.7122294306755066            | Epoch Loss: 0.6181167412448574            
Epoch: #9    | Batch: #38   | Batch Loss: 0.5961946249008179            | Epoch Loss: 0.6175398434463301            
Epoch: #9    | Batch: #39   | Batch Loss: 0.7203958630561829            | Epoch Loss: 0.6201771772824801            
Epoch: #9    | Batch: #40   | Batch Loss: 0.593595564365387             | Epoch Loss: 0.6195126369595527            
Epoch: #9    | Batch: #41   | Batch Loss: 0.7331791520118713            | Epoch Loss: 0.622284990985219             
Epoch: #9    | Batch: #42   | Batch Loss: 0.6366474628448486            | Epoch Loss: 0.6226269546009245            
Epoch: #9    | Batch: #43   | Batch Loss: 0.5514042377471924            | Epoch Loss: 0.6209706123485121            

Classifier Validation Epoch #9
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8454545454545455            
Batch: #2    | Top-1 Accuracy: 0.8363636363636364            
Batch: #3    | Top-1 Accuracy: 0.8242424242424242            
Batch: #4    | Top-1 Accuracy: 0.8284090909090909            
Batch: #5    | Top-1 Accuracy: 0.8363636363636363            
Batch: #6    | Top-1 Accuracy: 0.828030303030303             
Batch: #7    | Top-1 Accuracy: 0.8298701298701298            
Batch: #8    | Top-1 Accuracy: 0.8289772727272728            
Batch: #9    | Top-1 Accuracy: 0.8292929292929294            
Batch: #10   | Top-1 Accuracy: 0.8272727272727274            
Batch: #11   | Top-1 Accuracy: 0.8289256198347108            
Batch: #12   | Top-1 Accuracy: 0.8272727272727273            
Batch: #13   | Top-1 Accuracy: 0.8276223776223777            
Batch: #14   | Top-1 Accuracy: 0.8308441558441559            
Batch: #15   | Top-1 Accuracy: 0.8303030303030303            
Batch: #16   | Top-1 Accuracy: 0.8298295454545455            
Batch: #17   | Top-1 Accuracy: 0.8294117647058823            

Classifier Training Epoch #10
------------------------------------------
Epoch: #10   | Batch: #1    | Batch Loss: 0.6607027649879456            | Epoch Loss: 0.6607027649879456            
Epoch: #10   | Batch: #2    | Batch Loss: 0.7122828364372253            | Epoch Loss: 0.6864928007125854            
Epoch: #10   | Batch: #3    | Batch Loss: 0.7675977945327759            | Epoch Loss: 0.7135277986526489            
Epoch: #10   | Batch: #4    | Batch Loss: 0.6321539282798767            | Epoch Loss: 0.6931843310594559            
Epoch: #10   | Batch: #5    | Batch Loss: 0.5864244103431702            | Epoch Loss: 0.6718323469161988            
Epoch: #10   | Batch: #6    | Batch Loss: 0.5705057978630066            | Epoch Loss: 0.6549445887406667            
Epoch: #10   | Batch: #7    | Batch Loss: 0.643984317779541             | Epoch Loss: 0.6533788357462201            
Epoch: #10   | Batch: #8    | Batch Loss: 0.6030373573303223            | Epoch Loss: 0.6470861509442329            
Epoch: #10   | Batch: #9    | Batch Loss: 0.581013560295105             | Epoch Loss: 0.6397447519832187            
Epoch: #10   | Batch: #10   | Batch Loss: 0.7399727702140808            | Epoch Loss: 0.6497675538063049            
Epoch: #10   | Batch: #11   | Batch Loss: 0.5877652168273926            | Epoch Loss: 0.6441309777173129            
Epoch: #10   | Batch: #12   | Batch Loss: 0.6100748777389526            | Epoch Loss: 0.6412929693857828            
Epoch: #10   | Batch: #13   | Batch Loss: 0.788053035736084             | Epoch Loss: 0.652582205258883             
Epoch: #10   | Batch: #14   | Batch Loss: 0.5738019943237305            | Epoch Loss: 0.6469550473349435            
Epoch: #10   | Batch: #15   | Batch Loss: 0.6656530499458313            | Epoch Loss: 0.648201580842336             
Epoch: #10   | Batch: #16   | Batch Loss: 0.6609774231910706            | Epoch Loss: 0.6490000709891319            
Epoch: #10   | Batch: #17   | Batch Loss: 0.49075400829315186           | Epoch Loss: 0.639691479065839             
Epoch: #10   | Batch: #18   | Batch Loss: 0.7461622953414917            | Epoch Loss: 0.6456065244144864            
Epoch: #10   | Batch: #19   | Batch Loss: 0.613996148109436             | Epoch Loss: 0.6439428203984311            
Epoch: #10   | Batch: #20   | Batch Loss: 0.6237477660179138            | Epoch Loss: 0.6429330676794052            
Epoch: #10   | Batch: #21   | Batch Loss: 0.6251623034477234            | Epoch Loss: 0.6420868408112299            
Epoch: #10   | Batch: #22   | Batch Loss: 0.811148464679718             | Epoch Loss: 0.6497714600779794            
Epoch: #10   | Batch: #23   | Batch Loss: 0.6875879168510437            | Epoch Loss: 0.6514156538507213            
Epoch: #10   | Batch: #24   | Batch Loss: 0.7279133796691895            | Epoch Loss: 0.6546030590931574            
Epoch: #10   | Batch: #25   | Batch Loss: 0.5629661083221436            | Epoch Loss: 0.6509375810623169            
Epoch: #10   | Batch: #26   | Batch Loss: 0.6283173561096191            | Epoch Loss: 0.65006757241029              
Epoch: #10   | Batch: #27   | Batch Loss: 0.5661770701408386            | Epoch Loss: 0.6469605167706808            
Epoch: #10   | Batch: #28   | Batch Loss: 0.6366385221481323            | Epoch Loss: 0.6465918741055897            
Epoch: #10   | Batch: #29   | Batch Loss: 0.6006786227226257            | Epoch Loss: 0.6450086585406599            
Epoch: #10   | Batch: #30   | Batch Loss: 0.5887643694877625            | Epoch Loss: 0.6431338489055634            
Epoch: #10   | Batch: #31   | Batch Loss: 0.707907497882843             | Epoch Loss: 0.6452233214532176            
Epoch: #10   | Batch: #32   | Batch Loss: 0.6102085709571838            | Epoch Loss: 0.6441291105002165            
Epoch: #10   | Batch: #33   | Batch Loss: 0.6046991348266602            | Epoch Loss: 0.642934262752533             
Epoch: #10   | Batch: #34   | Batch Loss: 0.7153615355491638            | Epoch Loss: 0.6450644766583162            
Epoch: #10   | Batch: #35   | Batch Loss: 0.645760715007782             | Epoch Loss: 0.6450843691825867            
Epoch: #10   | Batch: #36   | Batch Loss: 0.5823617577552795            | Epoch Loss: 0.643342074420717             
Epoch: #10   | Batch: #37   | Batch Loss: 0.625800609588623             | Epoch Loss: 0.6428679807766063            
Epoch: #10   | Batch: #38   | Batch Loss: 0.507230281829834             | Epoch Loss: 0.6392985676464281            
Epoch: #10   | Batch: #39   | Batch Loss: 0.6039391160011292            | Epoch Loss: 0.6383919150401385            
Epoch: #10   | Batch: #40   | Batch Loss: 0.5487198829650879            | Epoch Loss: 0.6361501142382622            
Epoch: #10   | Batch: #41   | Batch Loss: 0.5454352498054504            | Epoch Loss: 0.6339375565691692            
Epoch: #10   | Batch: #42   | Batch Loss: 0.6488528251647949            | Epoch Loss: 0.6342926820119222            
Epoch: #10   | Batch: #43   | Batch Loss: 0.7504549026489258            | Epoch Loss: 0.6369941290034804            

Classifier Validation Epoch #10
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.7272727272727273            
Batch: #2    | Top-1 Accuracy: 0.8045454545454546            
Batch: #3    | Top-1 Accuracy: 0.8045454545454546            
Batch: #4    | Top-1 Accuracy: 0.8068181818181818            
Batch: #5    | Top-1 Accuracy: 0.8054545454545454            
Batch: #6    | Top-1 Accuracy: 0.8098484848484849            
Batch: #7    | Top-1 Accuracy: 0.814935064935065             
Batch: #8    | Top-1 Accuracy: 0.8164772727272728            
Batch: #9    | Top-1 Accuracy: 0.8181818181818182            
Batch: #10   | Top-1 Accuracy: 0.8213636363636365            
Batch: #11   | Top-1 Accuracy: 0.8239669421487604            
Batch: #12   | Top-1 Accuracy: 0.8238636363636364            
Batch: #13   | Top-1 Accuracy: 0.8213286713286714            
Batch: #14   | Top-1 Accuracy: 0.8227272727272729            
Batch: #15   | Top-1 Accuracy: 0.8227272727272728            
Batch: #16   | Top-1 Accuracy: 0.821875                      
Batch: #17   | Top-1 Accuracy: 0.8216577540106952            

Classifier Training Epoch #11
------------------------------------------
Epoch: #11   | Batch: #1    | Batch Loss: 0.4877620339393616            | Epoch Loss: 0.4877620339393616            
Epoch: #11   | Batch: #2    | Batch Loss: 0.564674437046051             | Epoch Loss: 0.5262182354927063            
Epoch: #11   | Batch: #3    | Batch Loss: 0.5664882063865662            | Epoch Loss: 0.5396415591239929            
Epoch: #11   | Batch: #4    | Batch Loss: 0.6659638285636902            | Epoch Loss: 0.5712221264839172            
Epoch: #11   | Batch: #5    | Batch Loss: 0.6461272239685059            | Epoch Loss: 0.586203145980835             
Epoch: #11   | Batch: #6    | Batch Loss: 0.6232970356941223            | Epoch Loss: 0.5923854609330496            
Epoch: #11   | Batch: #7    | Batch Loss: 0.5598406195640564            | Epoch Loss: 0.5877361978803363            
Epoch: #11   | Batch: #8    | Batch Loss: 0.6102773547172546            | Epoch Loss: 0.590553842484951             
Epoch: #11   | Batch: #9    | Batch Loss: 0.6455649137496948            | Epoch Loss: 0.5966661837365892            
Epoch: #11   | Batch: #10   | Batch Loss: 0.5771812200546265            | Epoch Loss: 0.5947176873683929            
Epoch: #11   | Batch: #11   | Batch Loss: 0.540683388710022             | Epoch Loss: 0.5898054783994501            
Epoch: #11   | Batch: #12   | Batch Loss: 0.6412816047668457            | Epoch Loss: 0.5940951555967331            
Epoch: #11   | Batch: #13   | Batch Loss: 0.4510613977909088            | Epoch Loss: 0.5830925588424389            
Epoch: #11   | Batch: #14   | Batch Loss: 0.6852995157241821            | Epoch Loss: 0.5903930557625634            
Epoch: #11   | Batch: #15   | Batch Loss: 0.593192994594574             | Epoch Loss: 0.5905797183513641            
Epoch: #11   | Batch: #16   | Batch Loss: 0.6010317206382751            | Epoch Loss: 0.5912329684942961            
Epoch: #11   | Batch: #17   | Batch Loss: 0.5992465019226074            | Epoch Loss: 0.5917043528136086            
Epoch: #11   | Batch: #18   | Batch Loss: 0.6962840557098389            | Epoch Loss: 0.5975143363078436            
Epoch: #11   | Batch: #19   | Batch Loss: 0.5671584010124207            | Epoch Loss: 0.5959166555028212            
Epoch: #11   | Batch: #20   | Batch Loss: 0.7308467626571655            | Epoch Loss: 0.6026631608605385            
Epoch: #11   | Batch: #21   | Batch Loss: 0.5275048017501831            | Epoch Loss: 0.599084191379093             
Epoch: #11   | Batch: #22   | Batch Loss: 0.684594988822937             | Epoch Loss: 0.6029710458083586            
Epoch: #11   | Batch: #23   | Batch Loss: 0.6840447783470154            | Epoch Loss: 0.6064959907013437            
Epoch: #11   | Batch: #24   | Batch Loss: 0.6352194547653198            | Epoch Loss: 0.6076928017040094            
Epoch: #11   | Batch: #25   | Batch Loss: 0.4597657024860382            | Epoch Loss: 0.6017757177352905            
Epoch: #11   | Batch: #26   | Batch Loss: 0.5508005619049072            | Epoch Loss: 0.5998151348187373            
Epoch: #11   | Batch: #27   | Batch Loss: 0.6570451259613037            | Epoch Loss: 0.6019347641203139            
Epoch: #11   | Batch: #28   | Batch Loss: 0.5823947787284851            | Epoch Loss: 0.6012369074991771            
Epoch: #11   | Batch: #29   | Batch Loss: 0.5380925536155701            | Epoch Loss: 0.5990595159859493            
Epoch: #11   | Batch: #30   | Batch Loss: 0.5697146654129028            | Epoch Loss: 0.5980813543001811            
Epoch: #11   | Batch: #31   | Batch Loss: 0.5703660249710083            | Epoch Loss: 0.5971873114185948            
Epoch: #11   | Batch: #32   | Batch Loss: 0.7335236072540283            | Epoch Loss: 0.6014478206634521            
Epoch: #11   | Batch: #33   | Batch Loss: 0.5757710337638855            | Epoch Loss: 0.6006697362119501            
Epoch: #11   | Batch: #34   | Batch Loss: 0.534645676612854             | Epoch Loss: 0.5987278521060944            
Epoch: #11   | Batch: #35   | Batch Loss: 0.6358664631843567            | Epoch Loss: 0.599788955279759             
Epoch: #11   | Batch: #36   | Batch Loss: 0.6985988020896912            | Epoch Loss: 0.6025336732467016            
Epoch: #11   | Batch: #37   | Batch Loss: 0.5858461260795593            | Epoch Loss: 0.6020826584584004            
Epoch: #11   | Batch: #38   | Batch Loss: 0.7199468612670898            | Epoch Loss: 0.6051843480059975            
Epoch: #11   | Batch: #39   | Batch Loss: 0.5908910036087036            | Epoch Loss: 0.6048178519958105            
Epoch: #11   | Batch: #40   | Batch Loss: 0.6482492685317993            | Epoch Loss: 0.6059036374092102            
Epoch: #11   | Batch: #41   | Batch Loss: 0.7108240127563477            | Epoch Loss: 0.6084626709542623            
Epoch: #11   | Batch: #42   | Batch Loss: 0.5633493065834045            | Epoch Loss: 0.6073885432311467            
Epoch: #11   | Batch: #43   | Batch Loss: 0.5673466324806213            | Epoch Loss: 0.6064573360043902            

Classifier Validation Epoch #11
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.85                          
Batch: #2    | Top-1 Accuracy: 0.8272727272727273            
Batch: #3    | Top-1 Accuracy: 0.8348484848484848            
Batch: #4    | Top-1 Accuracy: 0.8363636363636363            
Batch: #5    | Top-1 Accuracy: 0.8327272727272726            
Batch: #6    | Top-1 Accuracy: 0.834090909090909             
Batch: #7    | Top-1 Accuracy: 0.8311688311688311            
Batch: #8    | Top-1 Accuracy: 0.8295454545454546            
Batch: #9    | Top-1 Accuracy: 0.8308080808080809            
Batch: #10   | Top-1 Accuracy: 0.8331818181818182            
Batch: #11   | Top-1 Accuracy: 0.834297520661157             
Batch: #12   | Top-1 Accuracy: 0.8321969696969697            
Batch: #13   | Top-1 Accuracy: 0.8325174825174825            
Batch: #14   | Top-1 Accuracy: 0.8279220779220778            
Batch: #15   | Top-1 Accuracy: 0.8293939393939394            
Batch: #16   | Top-1 Accuracy: 0.8315340909090909            
Batch: #17   | Top-1 Accuracy: 0.8307486631016043            

Classifier Training Epoch #12
------------------------------------------
Epoch: #12   | Batch: #1    | Batch Loss: 0.6813279986381531            | Epoch Loss: 0.6813279986381531            
Epoch: #12   | Batch: #2    | Batch Loss: 0.591279923915863             | Epoch Loss: 0.6363039612770081            
Epoch: #12   | Batch: #3    | Batch Loss: 0.6022378206253052            | Epoch Loss: 0.6249485810597738            
Epoch: #12   | Batch: #4    | Batch Loss: 0.7895500063896179            | Epoch Loss: 0.6660989373922348            
Epoch: #12   | Batch: #5    | Batch Loss: 0.5777997970581055            | Epoch Loss: 0.6484391093254089            
Epoch: #12   | Batch: #6    | Batch Loss: 0.5688291788101196            | Epoch Loss: 0.6351707875728607            
Epoch: #12   | Batch: #7    | Batch Loss: 0.526174783706665             | Epoch Loss: 0.6195999298776899            
Epoch: #12   | Batch: #8    | Batch Loss: 0.7023303508758545            | Epoch Loss: 0.6299412325024605            
Epoch: #12   | Batch: #9    | Batch Loss: 0.5454025864601135            | Epoch Loss: 0.6205480496088663            
Epoch: #12   | Batch: #10   | Batch Loss: 0.6120731234550476            | Epoch Loss: 0.6197005569934845            
Epoch: #12   | Batch: #11   | Batch Loss: 0.5591112375259399            | Epoch Loss: 0.6141924370418895            
Epoch: #12   | Batch: #12   | Batch Loss: 0.5945456624031067            | Epoch Loss: 0.612555205821991             
Epoch: #12   | Batch: #13   | Batch Loss: 0.6787372827529907            | Epoch Loss: 0.6176461348166833            
Epoch: #12   | Batch: #14   | Batch Loss: 0.7137188911437988            | Epoch Loss: 0.6245084745543343            
Epoch: #12   | Batch: #15   | Batch Loss: 0.6174265146255493            | Epoch Loss: 0.6240363438924154            
Epoch: #12   | Batch: #16   | Batch Loss: 0.6612098813056946            | Epoch Loss: 0.6263596899807453            
Epoch: #12   | Batch: #17   | Batch Loss: 0.5498141646385193            | Epoch Loss: 0.6218570120194379            
Epoch: #12   | Batch: #18   | Batch Loss: 0.6471617221832275            | Epoch Loss: 0.6232628292507596            
Epoch: #12   | Batch: #19   | Batch Loss: 0.5890529751777649            | Epoch Loss: 0.6214623106153387            
Epoch: #12   | Batch: #20   | Batch Loss: 0.5648261904716492            | Epoch Loss: 0.6186305046081543            
Epoch: #12   | Batch: #21   | Batch Loss: 0.6233717203140259            | Epoch Loss: 0.6188562767846244            
Epoch: #12   | Batch: #22   | Batch Loss: 0.6626150012016296            | Epoch Loss: 0.62084530971267              
Epoch: #12   | Batch: #23   | Batch Loss: 0.6770617961883545            | Epoch Loss: 0.6232895047768302            
Epoch: #12   | Batch: #24   | Batch Loss: 0.5933843851089478            | Epoch Loss: 0.6220434581240019            
Epoch: #12   | Batch: #25   | Batch Loss: 0.585758626461029             | Epoch Loss: 0.6205920648574829            
Epoch: #12   | Batch: #26   | Batch Loss: 0.5342637896537781            | Epoch Loss: 0.6172717465804174            
Epoch: #12   | Batch: #27   | Batch Loss: 0.5021324753761292            | Epoch Loss: 0.6130073291284067            
Epoch: #12   | Batch: #28   | Batch Loss: 0.5414602160453796            | Epoch Loss: 0.6104520750897271            
Epoch: #12   | Batch: #29   | Batch Loss: 0.6668855547904968            | Epoch Loss: 0.6123980571483744            
Epoch: #12   | Batch: #30   | Batch Loss: 0.5952831506729126            | Epoch Loss: 0.6118275602658589            
Epoch: #12   | Batch: #31   | Batch Loss: 0.7092004418373108            | Epoch Loss: 0.6149686209617122            
Epoch: #12   | Batch: #32   | Batch Loss: 0.5839018821716309            | Epoch Loss: 0.6139977853745222            
Epoch: #12   | Batch: #33   | Batch Loss: 0.6433134078979492            | Epoch Loss: 0.6148861375722018            
Epoch: #12   | Batch: #34   | Batch Loss: 0.5783520936965942            | Epoch Loss: 0.613811606869978             
Epoch: #12   | Batch: #35   | Batch Loss: 0.5220369696617126            | Epoch Loss: 0.6111894743783134            
Epoch: #12   | Batch: #36   | Batch Loss: 0.6098314523696899            | Epoch Loss: 0.6111517515447404            
Epoch: #12   | Batch: #37   | Batch Loss: 0.565299391746521             | Epoch Loss: 0.6099124985772211            
Epoch: #12   | Batch: #38   | Batch Loss: 0.6472529768943787            | Epoch Loss: 0.610895142743462             
Epoch: #12   | Batch: #39   | Batch Loss: 0.5437194108963013            | Epoch Loss: 0.6091726880807143            
Epoch: #12   | Batch: #40   | Batch Loss: 0.7083682417869568            | Epoch Loss: 0.6116525769233704            
Epoch: #12   | Batch: #41   | Batch Loss: 0.42090126872062683           | Epoch Loss: 0.6070001059915962            
Epoch: #12   | Batch: #42   | Batch Loss: 0.6660248637199402            | Epoch Loss: 0.6084054573660805            
Epoch: #12   | Batch: #43   | Batch Loss: 0.6565711498260498            | Epoch Loss: 0.6095255897488705            

Classifier Validation Epoch #12
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8409090909090909            
Batch: #2    | Top-1 Accuracy: 0.8272727272727273            
Batch: #3    | Top-1 Accuracy: 0.8333333333333334            
Batch: #4    | Top-1 Accuracy: 0.825                         
Batch: #5    | Top-1 Accuracy: 0.8190909090909091            
Batch: #6    | Top-1 Accuracy: 0.8234848484848486            
Batch: #7    | Top-1 Accuracy: 0.8246753246753248            
Batch: #8    | Top-1 Accuracy: 0.8238636363636364            
Batch: #9    | Top-1 Accuracy: 0.8227272727272728            
Batch: #10   | Top-1 Accuracy: 0.8213636363636363            
Batch: #11   | Top-1 Accuracy: 0.8231404958677686            
Batch: #12   | Top-1 Accuracy: 0.8246212121212122            
Batch: #13   | Top-1 Accuracy: 0.8241258741258742            
Batch: #14   | Top-1 Accuracy: 0.8253246753246755            
Batch: #15   | Top-1 Accuracy: 0.8254545454545457            
Batch: #16   | Top-1 Accuracy: 0.8250000000000001            
Batch: #17   | Top-1 Accuracy: 0.8264705882352942            

Classifier Training Epoch #13
------------------------------------------
Epoch: #13   | Batch: #1    | Batch Loss: 0.6410292387008667            | Epoch Loss: 0.6410292387008667            
Epoch: #13   | Batch: #2    | Batch Loss: 0.6010318994522095            | Epoch Loss: 0.6210305690765381            
Epoch: #13   | Batch: #3    | Batch Loss: 0.5769813060760498            | Epoch Loss: 0.6063474814097086            
Epoch: #13   | Batch: #4    | Batch Loss: 0.5307596921920776            | Epoch Loss: 0.5874505341053009            
Epoch: #13   | Batch: #5    | Batch Loss: 0.6094721555709839            | Epoch Loss: 0.5918548583984375            
Epoch: #13   | Batch: #6    | Batch Loss: 0.6277754902839661            | Epoch Loss: 0.5978416303793589            
Epoch: #13   | Batch: #7    | Batch Loss: 0.5854569673538208            | Epoch Loss: 0.596072392804282             
Epoch: #13   | Batch: #8    | Batch Loss: 0.7028046250343323            | Epoch Loss: 0.6094139218330383            
Epoch: #13   | Batch: #9    | Batch Loss: 0.49662646651268005           | Epoch Loss: 0.5968819823529985            
Epoch: #13   | Batch: #10   | Batch Loss: 0.5394908785820007            | Epoch Loss: 0.5911428719758988            
Epoch: #13   | Batch: #11   | Batch Loss: 0.7516390681266785            | Epoch Loss: 0.6057334352623333            
Epoch: #13   | Batch: #12   | Batch Loss: 0.6049360036849976            | Epoch Loss: 0.6056669826308886            
Epoch: #13   | Batch: #13   | Batch Loss: 0.455447256565094             | Epoch Loss: 0.594111619087366             
Epoch: #13   | Batch: #14   | Batch Loss: 0.5944609045982361            | Epoch Loss: 0.5941365680524281            
Epoch: #13   | Batch: #15   | Batch Loss: 0.541078507900238             | Epoch Loss: 0.5905993640422821            
Epoch: #13   | Batch: #16   | Batch Loss: 0.5509775876998901            | Epoch Loss: 0.5881230030208826            
Epoch: #13   | Batch: #17   | Batch Loss: 0.6188583374023438            | Epoch Loss: 0.589930963866851             
Epoch: #13   | Batch: #18   | Batch Loss: 0.5682654976844788            | Epoch Loss: 0.5887273268567191            
Epoch: #13   | Batch: #19   | Batch Loss: 0.46830323338508606           | Epoch Loss: 0.5823892166740016            
Epoch: #13   | Batch: #20   | Batch Loss: 0.6764717698097229            | Epoch Loss: 0.5870933443307876            
Epoch: #13   | Batch: #21   | Batch Loss: 0.704262375831604             | Epoch Loss: 0.5926728220213027            
Epoch: #13   | Batch: #22   | Batch Loss: 0.5561623573303223            | Epoch Loss: 0.5910132554444399            
Epoch: #13   | Batch: #23   | Batch Loss: 0.5147122144699097            | Epoch Loss: 0.58769581888033              
Epoch: #13   | Batch: #24   | Batch Loss: 0.6000105142593384            | Epoch Loss: 0.5882089311877886            
Epoch: #13   | Batch: #25   | Batch Loss: 0.621945858001709             | Epoch Loss: 0.5895584082603454            
Epoch: #13   | Batch: #26   | Batch Loss: 0.5360390543937683            | Epoch Loss: 0.5874999715731695            
Epoch: #13   | Batch: #27   | Batch Loss: 0.6064954400062561            | Epoch Loss: 0.5882035074410615            
Epoch: #13   | Batch: #28   | Batch Loss: 0.6329400539398193            | Epoch Loss: 0.5898012412445885            
Epoch: #13   | Batch: #29   | Batch Loss: 0.7319416403770447            | Epoch Loss: 0.5947026343181215            
Epoch: #13   | Batch: #30   | Batch Loss: 0.6154454350471497            | Epoch Loss: 0.5953940610090892            
Epoch: #13   | Batch: #31   | Batch Loss: 0.5523373484611511            | Epoch Loss: 0.5940051347978653            
Epoch: #13   | Batch: #32   | Batch Loss: 0.5302286744117737            | Epoch Loss: 0.5920121204108               
Epoch: #13   | Batch: #33   | Batch Loss: 0.5359326601028442            | Epoch Loss: 0.5903127428257104            
Epoch: #13   | Batch: #34   | Batch Loss: 0.5769616365432739            | Epoch Loss: 0.5899200632291681            
Epoch: #13   | Batch: #35   | Batch Loss: 0.5016540288925171            | Epoch Loss: 0.5873981765338353            
Epoch: #13   | Batch: #36   | Batch Loss: 0.5090062022209167            | Epoch Loss: 0.5852206216918098            
Epoch: #13   | Batch: #37   | Batch Loss: 0.49472856521606445           | Epoch Loss: 0.5827748904357085            
Epoch: #13   | Batch: #38   | Batch Loss: 0.6055610775947571            | Epoch Loss: 0.583374526939894             
Epoch: #13   | Batch: #39   | Batch Loss: 0.6581642031669617            | Epoch Loss: 0.5852922109457163            
Epoch: #13   | Batch: #40   | Batch Loss: 0.5582610368728638            | Epoch Loss: 0.584616431593895             
Epoch: #13   | Batch: #41   | Batch Loss: 0.683124840259552             | Epoch Loss: 0.5870190757076915            
Epoch: #13   | Batch: #42   | Batch Loss: 0.5441858768463135            | Epoch Loss: 0.5859992376395634            
Epoch: #13   | Batch: #43   | Batch Loss: 0.6555426716804504            | Epoch Loss: 0.587616526803305             

Classifier Validation Epoch #13
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8590909090909091            
Batch: #2    | Top-1 Accuracy: 0.8454545454545455            
Batch: #3    | Top-1 Accuracy: 0.8409090909090908            
Batch: #4    | Top-1 Accuracy: 0.8443181818181817            
Batch: #5    | Top-1 Accuracy: 0.8363636363636363            
Batch: #6    | Top-1 Accuracy: 0.8318181818181817            
Batch: #7    | Top-1 Accuracy: 0.8337662337662337            
Batch: #8    | Top-1 Accuracy: 0.8301136363636363            
Batch: #9    | Top-1 Accuracy: 0.8267676767676767            
Batch: #10   | Top-1 Accuracy: 0.8299999999999998            
Batch: #11   | Top-1 Accuracy: 0.8322314049586775            
Batch: #12   | Top-1 Accuracy: 0.8344696969696969            
Batch: #13   | Top-1 Accuracy: 0.8339160839160839            
Batch: #14   | Top-1 Accuracy: 0.835064935064935             
Batch: #15   | Top-1 Accuracy: 0.8345454545454545            
Batch: #16   | Top-1 Accuracy: 0.8340909090909091            
Batch: #17   | Top-1 Accuracy: 0.8336898395721926            

Classifier Training Epoch #14
------------------------------------------
Epoch: #14   | Batch: #1    | Batch Loss: 0.5923863053321838            | Epoch Loss: 0.5923863053321838            
Epoch: #14   | Batch: #2    | Batch Loss: 0.5764232277870178            | Epoch Loss: 0.5844047665596008            
Epoch: #14   | Batch: #3    | Batch Loss: 0.7073528170585632            | Epoch Loss: 0.625387450059255             
Epoch: #14   | Batch: #4    | Batch Loss: 0.7467833161354065            | Epoch Loss: 0.6557364165782928            
Epoch: #14   | Batch: #5    | Batch Loss: 0.43259987235069275           | Epoch Loss: 0.6111091077327728            
Epoch: #14   | Batch: #6    | Batch Loss: 0.5678881406784058            | Epoch Loss: 0.6039056132237116            
Epoch: #14   | Batch: #7    | Batch Loss: 0.5366120934486389            | Epoch Loss: 0.5942922532558441            
Epoch: #14   | Batch: #8    | Batch Loss: 0.7004041075706482            | Epoch Loss: 0.6075562350451946            
Epoch: #14   | Batch: #9    | Batch Loss: 0.5898478627204895            | Epoch Loss: 0.6055886381202273            
Epoch: #14   | Batch: #10   | Batch Loss: 0.5090416669845581            | Epoch Loss: 0.5959339410066604            
Epoch: #14   | Batch: #11   | Batch Loss: 0.6443933248519897            | Epoch Loss: 0.6003393395380541            
Epoch: #14   | Batch: #12   | Batch Loss: 0.5465622544288635            | Epoch Loss: 0.5958579157789549            
Epoch: #14   | Batch: #13   | Batch Loss: 0.6000057458877563            | Epoch Loss: 0.596176979633478             
Epoch: #14   | Batch: #14   | Batch Loss: 0.7542955279350281            | Epoch Loss: 0.6074711616550174            
Epoch: #14   | Batch: #15   | Batch Loss: 0.5811090469360352            | Epoch Loss: 0.6057136873404185            
Epoch: #14   | Batch: #16   | Batch Loss: 0.6258621215820312            | Epoch Loss: 0.6069729644805193            
Epoch: #14   | Batch: #17   | Batch Loss: 0.5307782888412476            | Epoch Loss: 0.6024909247370327            
Epoch: #14   | Batch: #18   | Batch Loss: 0.6574944257736206            | Epoch Loss: 0.605546674794621             
Epoch: #14   | Batch: #19   | Batch Loss: 0.5587047934532166            | Epoch Loss: 0.6030813126187575            
Epoch: #14   | Batch: #20   | Batch Loss: 0.6160197257995605            | Epoch Loss: 0.6037282332777977            
Epoch: #14   | Batch: #21   | Batch Loss: 0.579256534576416             | Epoch Loss: 0.6025629142920176            
Epoch: #14   | Batch: #22   | Batch Loss: 0.5754814147949219            | Epoch Loss: 0.6013319370421496            
Epoch: #14   | Batch: #23   | Batch Loss: 0.6860799193382263            | Epoch Loss: 0.6050166319245878            
Epoch: #14   | Batch: #24   | Batch Loss: 0.6236573457717896            | Epoch Loss: 0.6057933283348879            
Epoch: #14   | Batch: #25   | Batch Loss: 0.568621039390564             | Epoch Loss: 0.6043064367771148            
Epoch: #14   | Batch: #26   | Batch Loss: 0.5922822952270508            | Epoch Loss: 0.6038439697944201            
Epoch: #14   | Batch: #27   | Batch Loss: 0.6593084335327148            | Epoch Loss: 0.6058982091921347            
Epoch: #14   | Batch: #28   | Batch Loss: 0.6315336227416992            | Epoch Loss: 0.6068137596760478            
Epoch: #14   | Batch: #29   | Batch Loss: 0.5540194511413574            | Epoch Loss: 0.6049932662782997            
Epoch: #14   | Batch: #30   | Batch Loss: 0.7856763601303101            | Epoch Loss: 0.6110160360733669            
Epoch: #14   | Batch: #31   | Batch Loss: 0.5402415990829468            | Epoch Loss: 0.6087329897188372            
Epoch: #14   | Batch: #32   | Batch Loss: 0.5707083344459534            | Epoch Loss: 0.6075447192415595            
Epoch: #14   | Batch: #33   | Batch Loss: 0.5849485397338867            | Epoch Loss: 0.6068599865292058            
Epoch: #14   | Batch: #34   | Batch Loss: 0.6727551817893982            | Epoch Loss: 0.6087980805074468            
Epoch: #14   | Batch: #35   | Batch Loss: 0.46109646558761597           | Epoch Loss: 0.6045780343668802            
Epoch: #14   | Batch: #36   | Batch Loss: 0.5682064294815063            | Epoch Loss: 0.6035677120089531            
Epoch: #14   | Batch: #37   | Batch Loss: 0.5237430334091187            | Epoch Loss: 0.6014102882630116            
Epoch: #14   | Batch: #38   | Batch Loss: 0.6964502930641174            | Epoch Loss: 0.6039113410209355            
Epoch: #14   | Batch: #39   | Batch Loss: 0.7367753386497498            | Epoch Loss: 0.6073181101909051            
Epoch: #14   | Batch: #40   | Batch Loss: 0.5112552046775818            | Epoch Loss: 0.604916537553072             
Epoch: #14   | Batch: #41   | Batch Loss: 0.5251310467720032            | Epoch Loss: 0.6029705499730459            
Epoch: #14   | Batch: #42   | Batch Loss: 0.6199644804000854            | Epoch Loss: 0.6033751673641659            
Epoch: #14   | Batch: #43   | Batch Loss: 0.557153582572937             | Epoch Loss: 0.6023002467876257            

Classifier Validation Epoch #14
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8272727272727273            
Batch: #2    | Top-1 Accuracy: 0.8409090909090908            
Batch: #3    | Top-1 Accuracy: 0.8545454545454545            
Batch: #4    | Top-1 Accuracy: 0.8352272727272727            
Batch: #5    | Top-1 Accuracy: 0.8363636363636363            
Batch: #6    | Top-1 Accuracy: 0.8318181818181817            
Batch: #7    | Top-1 Accuracy: 0.8344155844155843            
Batch: #8    | Top-1 Accuracy: 0.8318181818181818            
Batch: #9    | Top-1 Accuracy: 0.8368686868686868            
Batch: #10   | Top-1 Accuracy: 0.8390909090909091            
Batch: #11   | Top-1 Accuracy: 0.8376033057851239            
Batch: #12   | Top-1 Accuracy: 0.8359848484848484            
Batch: #13   | Top-1 Accuracy: 0.8405594405594404            
Batch: #14   | Top-1 Accuracy: 0.8415584415584415            
Batch: #15   | Top-1 Accuracy: 0.8403030303030302            
Batch: #16   | Top-1 Accuracy: 0.8417613636363637            
Batch: #17   | Top-1 Accuracy: 0.8411764705882353            

Classifier Training Epoch #15
------------------------------------------
Epoch: #15   | Batch: #1    | Batch Loss: 0.5888705849647522            | Epoch Loss: 0.5888705849647522            
Epoch: #15   | Batch: #2    | Batch Loss: 0.5283088088035583            | Epoch Loss: 0.5585896968841553            
Epoch: #15   | Batch: #3    | Batch Loss: 0.5781658291816711            | Epoch Loss: 0.5651150743166605            
Epoch: #15   | Batch: #4    | Batch Loss: 0.4466160237789154            | Epoch Loss: 0.5354903116822243            
Epoch: #15   | Batch: #5    | Batch Loss: 0.6601998209953308            | Epoch Loss: 0.5604322135448456            
Epoch: #15   | Batch: #6    | Batch Loss: 0.6582021117210388            | Epoch Loss: 0.5767271965742111            
Epoch: #15   | Batch: #7    | Batch Loss: 0.6234479546546936            | Epoch Loss: 0.5834015905857086            
Epoch: #15   | Batch: #8    | Batch Loss: 0.6382663249969482            | Epoch Loss: 0.5902596823871136            
Epoch: #15   | Batch: #9    | Batch Loss: 0.6142526268959045            | Epoch Loss: 0.5929255651103126            
Epoch: #15   | Batch: #10   | Batch Loss: 0.43412530422210693           | Epoch Loss: 0.577045539021492             
Epoch: #15   | Batch: #11   | Batch Loss: 0.5425892472267151            | Epoch Loss: 0.5739131488583304            
Epoch: #15   | Batch: #12   | Batch Loss: 0.4950256645679474            | Epoch Loss: 0.5673391918341318            
Epoch: #15   | Batch: #13   | Batch Loss: 0.5347272753715515            | Epoch Loss: 0.5648305828754718            
Epoch: #15   | Batch: #14   | Batch Loss: 0.539461076259613             | Epoch Loss: 0.5630184752600533            
Epoch: #15   | Batch: #15   | Batch Loss: 0.6599193215370178            | Epoch Loss: 0.5694785316785177            
Epoch: #15   | Batch: #16   | Batch Loss: 0.574180543422699             | Epoch Loss: 0.569772407412529             
Epoch: #15   | Batch: #17   | Batch Loss: 0.45289894938468933           | Epoch Loss: 0.5628974981167737            
Epoch: #15   | Batch: #18   | Batch Loss: 0.662880539894104             | Epoch Loss: 0.5684521115488477            
Epoch: #15   | Batch: #19   | Batch Loss: 0.6992645859718323            | Epoch Loss: 0.5753369786237416            
Epoch: #15   | Batch: #20   | Batch Loss: 0.6202998161315918            | Epoch Loss: 0.577585120499134             
Epoch: #15   | Batch: #21   | Batch Loss: 0.6590918302536011            | Epoch Loss: 0.5814663923922039            
Epoch: #15   | Batch: #22   | Batch Loss: 0.6290093660354614            | Epoch Loss: 0.5836274366487156            
Epoch: #15   | Batch: #23   | Batch Loss: 0.5430269837379456            | Epoch Loss: 0.5818621995656387            
Epoch: #15   | Batch: #24   | Batch Loss: 0.74028480052948              | Epoch Loss: 0.5884631412724654            
Epoch: #15   | Batch: #25   | Batch Loss: 0.5637077689170837            | Epoch Loss: 0.5874729263782501            
Epoch: #15   | Batch: #26   | Batch Loss: 0.7368943691253662            | Epoch Loss: 0.5932199049454469            
Epoch: #15   | Batch: #27   | Batch Loss: 0.6112087368965149            | Epoch Loss: 0.5938861579806717            
Epoch: #15   | Batch: #28   | Batch Loss: 0.5785003900527954            | Epoch Loss: 0.5933366662689618            
Epoch: #15   | Batch: #29   | Batch Loss: 0.4770617187023163            | Epoch Loss: 0.5893271853183878            
Epoch: #15   | Batch: #30   | Batch Loss: 0.7623649835586548            | Epoch Loss: 0.5950951119263966            
Epoch: #15   | Batch: #31   | Batch Loss: 0.5629473924636841            | Epoch Loss: 0.5940580887179221            
Epoch: #15   | Batch: #32   | Batch Loss: 0.5880135297775269            | Epoch Loss: 0.5938691962510347            
Epoch: #15   | Batch: #33   | Batch Loss: 0.6001675724983215            | Epoch Loss: 0.5940600561373162            
Epoch: #15   | Batch: #34   | Batch Loss: 0.5594391822814941            | Epoch Loss: 0.5930417951415566            
Epoch: #15   | Batch: #35   | Batch Loss: 0.7621863484382629            | Epoch Loss: 0.5978744966643197            
Epoch: #15   | Batch: #36   | Batch Loss: 0.7130649089813232            | Epoch Loss: 0.601074230339792             
Epoch: #15   | Batch: #37   | Batch Loss: 0.703813374042511             | Epoch Loss: 0.603850963953379             
Epoch: #15   | Batch: #38   | Batch Loss: 0.5441514253616333            | Epoch Loss: 0.6022799234641226            
Epoch: #15   | Batch: #39   | Batch Loss: 0.5075989365577698            | Epoch Loss: 0.5998522058511392            
Epoch: #15   | Batch: #40   | Batch Loss: 0.6393483877182007            | Epoch Loss: 0.6008396103978157            
Epoch: #15   | Batch: #41   | Batch Loss: 0.5357900261878967            | Epoch Loss: 0.5992530351731835            
Epoch: #15   | Batch: #42   | Batch Loss: 0.548637330532074             | Epoch Loss: 0.5980478993483952            
Epoch: #15   | Batch: #43   | Batch Loss: 0.6012447476387024            | Epoch Loss: 0.5981222446574721            

Classifier Validation Epoch #15
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.7954545454545454            
Batch: #2    | Top-1 Accuracy: 0.8                           
Batch: #3    | Top-1 Accuracy: 0.8075757575757576            
Batch: #4    | Top-1 Accuracy: 0.8102272727272728            
Batch: #5    | Top-1 Accuracy: 0.8154545454545454            
Batch: #6    | Top-1 Accuracy: 0.8196969696969697            
Batch: #7    | Top-1 Accuracy: 0.82012987012987              
Batch: #8    | Top-1 Accuracy: 0.8159090909090909            
Batch: #9    | Top-1 Accuracy: 0.8207070707070707            
Batch: #10   | Top-1 Accuracy: 0.8213636363636365            
Batch: #11   | Top-1 Accuracy: 0.8214876033057852            
Batch: #12   | Top-1 Accuracy: 0.8242424242424242            
Batch: #13   | Top-1 Accuracy: 0.8248251748251748            
Batch: #14   | Top-1 Accuracy: 0.8256493506493506            
Batch: #15   | Top-1 Accuracy: 0.8233333333333334            
Batch: #16   | Top-1 Accuracy: 0.8247159090909091            
Batch: #17   | Top-1 Accuracy: 0.8248663101604279            

Classifier Training Epoch #16
------------------------------------------
Epoch: #16   | Batch: #1    | Batch Loss: 0.5334061980247498            | Epoch Loss: 0.5334061980247498            
Epoch: #16   | Batch: #2    | Batch Loss: 0.553991436958313             | Epoch Loss: 0.5436988174915314            
Epoch: #16   | Batch: #3    | Batch Loss: 0.5153845548629761            | Epoch Loss: 0.5342607299486796            
Epoch: #16   | Batch: #4    | Batch Loss: 0.49796685576438904           | Epoch Loss: 0.525187261402607             
Epoch: #16   | Batch: #5    | Batch Loss: 0.6849691867828369            | Epoch Loss: 0.557143646478653             
Epoch: #16   | Batch: #6    | Batch Loss: 0.6071219444274902            | Epoch Loss: 0.5654733628034592            
Epoch: #16   | Batch: #7    | Batch Loss: 0.5673041343688965            | Epoch Loss: 0.5657349015985217            
Epoch: #16   | Batch: #8    | Batch Loss: 0.6126593351364136            | Epoch Loss: 0.5716004557907581            
Epoch: #16   | Batch: #9    | Batch Loss: 0.6132799983024597            | Epoch Loss: 0.5762315160698361            
Epoch: #16   | Batch: #10   | Batch Loss: 0.6721906065940857            | Epoch Loss: 0.585827425122261             
Epoch: #16   | Batch: #11   | Batch Loss: 0.5220495462417603            | Epoch Loss: 0.5800294361331246            
Epoch: #16   | Batch: #12   | Batch Loss: 0.6031073331832886            | Epoch Loss: 0.5819525942206383            
Epoch: #16   | Batch: #13   | Batch Loss: 0.545789361000061             | Epoch Loss: 0.5791708070498246            
Epoch: #16   | Batch: #14   | Batch Loss: 0.7004532814025879            | Epoch Loss: 0.5878338409321648            
Epoch: #16   | Batch: #15   | Batch Loss: 0.6029601097106934            | Epoch Loss: 0.5888422588507335            
Epoch: #16   | Batch: #16   | Batch Loss: 0.5564377307891846            | Epoch Loss: 0.5868169758468866            
Epoch: #16   | Batch: #17   | Batch Loss: 0.5870892405509949            | Epoch Loss: 0.5868329914177165            
Epoch: #16   | Batch: #18   | Batch Loss: 0.47248953580856323           | Epoch Loss: 0.580480577217208             
Epoch: #16   | Batch: #19   | Batch Loss: 0.5576833486557007            | Epoch Loss: 0.5792807230823919            
Epoch: #16   | Batch: #20   | Batch Loss: 0.5065892934799194            | Epoch Loss: 0.5756461516022682            
Epoch: #16   | Batch: #21   | Batch Loss: 0.5237887501716614            | Epoch Loss: 0.573176751534144             
Epoch: #16   | Batch: #22   | Batch Loss: 0.49968788027763367           | Epoch Loss: 0.5698363482952118            
Epoch: #16   | Batch: #23   | Batch Loss: 0.5712872743606567            | Epoch Loss: 0.5698994320371876            
Epoch: #16   | Batch: #24   | Batch Loss: 0.5386737585067749            | Epoch Loss: 0.5685983623067538            
Epoch: #16   | Batch: #25   | Batch Loss: 0.6980733275413513            | Epoch Loss: 0.5737773609161377            
Epoch: #16   | Batch: #26   | Batch Loss: 0.5527647137641907            | Epoch Loss: 0.5729691821795243            
Epoch: #16   | Batch: #27   | Batch Loss: 0.6373946666717529            | Epoch Loss: 0.5753553112347921            
Epoch: #16   | Batch: #28   | Batch Loss: 0.5560595989227295            | Epoch Loss: 0.5746661786522184            
Epoch: #16   | Batch: #29   | Batch Loss: 0.6056357026100159            | Epoch Loss: 0.5757340932714528            
Epoch: #16   | Batch: #30   | Batch Loss: 0.4940571188926697            | Epoch Loss: 0.5730115274588267            
Epoch: #16   | Batch: #31   | Batch Loss: 0.8199978470802307            | Epoch Loss: 0.5809788280917753            
Epoch: #16   | Batch: #32   | Batch Loss: 0.5618607401847839            | Epoch Loss: 0.5803813878446817            
Epoch: #16   | Batch: #33   | Batch Loss: 0.6468014717102051            | Epoch Loss: 0.5823941176587885            
Epoch: #16   | Batch: #34   | Batch Loss: 0.6827667951583862            | Epoch Loss: 0.5853462552323061            
Epoch: #16   | Batch: #35   | Batch Loss: 0.6157233715057373            | Epoch Loss: 0.5862141728401185            
Epoch: #16   | Batch: #36   | Batch Loss: 0.7725092768669128            | Epoch Loss: 0.5913890368408627            
Epoch: #16   | Batch: #37   | Batch Loss: 0.6477978229522705            | Epoch Loss: 0.5929135986276575            
Epoch: #16   | Batch: #38   | Batch Loss: 0.7326235771179199            | Epoch Loss: 0.5965901770089802            
Epoch: #16   | Batch: #39   | Batch Loss: 0.6265543103218079            | Epoch Loss: 0.5973584881195655            
Epoch: #16   | Batch: #40   | Batch Loss: 0.4964788258075714            | Epoch Loss: 0.5948364965617656            
Epoch: #16   | Batch: #41   | Batch Loss: 0.7999055981636047            | Epoch Loss: 0.5998381819666886            
Epoch: #16   | Batch: #42   | Batch Loss: 0.6260303258895874            | Epoch Loss: 0.6004618044410434            
Epoch: #16   | Batch: #43   | Batch Loss: 0.5971666574478149            | Epoch Loss: 0.6003851731156193            

Classifier Validation Epoch #16
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8727272727272727            
Batch: #2    | Top-1 Accuracy: 0.8613636363636363            
Batch: #3    | Top-1 Accuracy: 0.85                          
Batch: #4    | Top-1 Accuracy: 0.8511363636363636            
Batch: #5    | Top-1 Accuracy: 0.8436363636363635            
Batch: #6    | Top-1 Accuracy: 0.8454545454545453            
Batch: #7    | Top-1 Accuracy: 0.8422077922077921            
Batch: #8    | Top-1 Accuracy: 0.8403409090909091            
Batch: #9    | Top-1 Accuracy: 0.8338383838383838            
Batch: #10   | Top-1 Accuracy: 0.8359090909090909            
Batch: #11   | Top-1 Accuracy: 0.8388429752066116            
Batch: #12   | Top-1 Accuracy: 0.8412878787878787            
Batch: #13   | Top-1 Accuracy: 0.8419580419580418            
Batch: #14   | Top-1 Accuracy: 0.8422077922077921            
Batch: #15   | Top-1 Accuracy: 0.8427272727272725            
Batch: #16   | Top-1 Accuracy: 0.840625                      
Batch: #17   | Top-1 Accuracy: 0.8401069518716577            

Classifier Training Epoch #17
------------------------------------------
Epoch: #17   | Batch: #1    | Batch Loss: 0.5451571941375732            | Epoch Loss: 0.5451571941375732            
Epoch: #17   | Batch: #2    | Batch Loss: 0.5652061104774475            | Epoch Loss: 0.5551816523075104            
Epoch: #17   | Batch: #3    | Batch Loss: 0.6226106882095337            | Epoch Loss: 0.5776579976081848            
Epoch: #17   | Batch: #4    | Batch Loss: 0.6184625625610352            | Epoch Loss: 0.5878591388463974            
Epoch: #17   | Batch: #5    | Batch Loss: 0.5469009280204773            | Epoch Loss: 0.5796674966812134            
Epoch: #17   | Batch: #6    | Batch Loss: 0.49834775924682617           | Epoch Loss: 0.5661142071088155            
Epoch: #17   | Batch: #7    | Batch Loss: 0.7167647480964661            | Epoch Loss: 0.5876357129641941            
Epoch: #17   | Batch: #8    | Batch Loss: 0.4735312759876251            | Epoch Loss: 0.573372658342123             
Epoch: #17   | Batch: #9    | Batch Loss: 0.5372162461280823            | Epoch Loss: 0.5693552792072296            
Epoch: #17   | Batch: #10   | Batch Loss: 0.6471810936927795            | Epoch Loss: 0.5771378606557847            
Epoch: #17   | Batch: #11   | Batch Loss: 0.720229983329773             | Epoch Loss: 0.590146235444329             
Epoch: #17   | Batch: #12   | Batch Loss: 0.6719855666160583            | Epoch Loss: 0.5969661797086397            
Epoch: #17   | Batch: #13   | Batch Loss: 0.6158920526504517            | Epoch Loss: 0.5984220160887792            
Epoch: #17   | Batch: #14   | Batch Loss: 0.4966145157814026            | Epoch Loss: 0.5911500517811094            
Epoch: #17   | Batch: #15   | Batch Loss: 0.5823941230773926            | Epoch Loss: 0.5905663232008617            
Epoch: #17   | Batch: #16   | Batch Loss: 0.4936659336090088            | Epoch Loss: 0.5845100488513708            
Epoch: #17   | Batch: #17   | Batch Loss: 0.5066021084785461            | Epoch Loss: 0.5799272288294399            
Epoch: #17   | Batch: #18   | Batch Loss: 0.5858066082000732            | Epoch Loss: 0.5802538610166974            
Epoch: #17   | Batch: #19   | Batch Loss: 0.6276525259017944            | Epoch Loss: 0.5827485275895972            
Epoch: #17   | Batch: #20   | Batch Loss: 0.6809018850326538            | Epoch Loss: 0.5876561954617501            
Epoch: #17   | Batch: #21   | Batch Loss: 0.567514181137085             | Epoch Loss: 0.5866970519224802            
Epoch: #17   | Batch: #22   | Batch Loss: 0.49921315908432007           | Epoch Loss: 0.5827205113389275            
Epoch: #17   | Batch: #23   | Batch Loss: 0.7229118943214417            | Epoch Loss: 0.5888157888599064            
Epoch: #17   | Batch: #24   | Batch Loss: 0.6770596504211426            | Epoch Loss: 0.5924926164249579            
Epoch: #17   | Batch: #25   | Batch Loss: 0.6041551232337952            | Epoch Loss: 0.5929591166973114            
Epoch: #17   | Batch: #26   | Batch Loss: 0.6675218939781189            | Epoch Loss: 0.5958269158234963            
Epoch: #17   | Batch: #27   | Batch Loss: 0.5779702663421631            | Epoch Loss: 0.5951655584352987            
Epoch: #17   | Batch: #28   | Batch Loss: 0.583577036857605             | Epoch Loss: 0.5947516826646668            
Epoch: #17   | Batch: #29   | Batch Loss: 0.40778011083602905           | Epoch Loss: 0.588304387084369             
Epoch: #17   | Batch: #30   | Batch Loss: 0.5536580681800842            | Epoch Loss: 0.5871495097875595            
Epoch: #17   | Batch: #31   | Batch Loss: 0.52418452501297              | Epoch Loss: 0.5851183812464437            
Epoch: #17   | Batch: #32   | Batch Loss: 0.5646951794624329            | Epoch Loss: 0.5844801561906934            
Epoch: #17   | Batch: #33   | Batch Loss: 0.5828999876976013            | Epoch Loss: 0.5844322722969633            
Epoch: #17   | Batch: #34   | Batch Loss: 0.7661009430885315            | Epoch Loss: 0.5897754684967154            
Epoch: #17   | Batch: #35   | Batch Loss: 0.615223228931427             | Epoch Loss: 0.5905025473662785            
Epoch: #17   | Batch: #36   | Batch Loss: 0.5112930536270142            | Epoch Loss: 0.588302283651299             
Epoch: #17   | Batch: #37   | Batch Loss: 0.5741379261016846            | Epoch Loss: 0.5879194631769851            
Epoch: #17   | Batch: #38   | Batch Loss: 0.6756781935691833            | Epoch Loss: 0.590228903450464             
Epoch: #17   | Batch: #39   | Batch Loss: 0.5876955986022949            | Epoch Loss: 0.5901639469158955            
Epoch: #17   | Batch: #40   | Batch Loss: 0.6727108359336853            | Epoch Loss: 0.5922276191413403            
Epoch: #17   | Batch: #41   | Batch Loss: 0.5057534575462341            | Epoch Loss: 0.5901184932487767            
Epoch: #17   | Batch: #42   | Batch Loss: 0.7114909887313843            | Epoch Loss: 0.5930083145697912            
Epoch: #17   | Batch: #43   | Batch Loss: 0.5347022414207458            | Epoch Loss: 0.5916523593802785            

Classifier Validation Epoch #17
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8636363636363636            
Batch: #2    | Top-1 Accuracy: 0.8454545454545455            
Batch: #3    | Top-1 Accuracy: 0.8424242424242424            
Batch: #4    | Top-1 Accuracy: 0.8443181818181819            
Batch: #5    | Top-1 Accuracy: 0.840909090909091             
Batch: #6    | Top-1 Accuracy: 0.8371212121212123            
Batch: #7    | Top-1 Accuracy: 0.8383116883116885            
Batch: #8    | Top-1 Accuracy: 0.8369318181818182            
Batch: #9    | Top-1 Accuracy: 0.8363636363636364            
Batch: #10   | Top-1 Accuracy: 0.8363636363636363            
Batch: #11   | Top-1 Accuracy: 0.8376033057851239            
Batch: #12   | Top-1 Accuracy: 0.8329545454545454            
Batch: #13   | Top-1 Accuracy: 0.8360139860139859            
Batch: #14   | Top-1 Accuracy: 0.8357142857142856            
Batch: #15   | Top-1 Accuracy: 0.8351515151515152            
Batch: #16   | Top-1 Accuracy: 0.8360795454545454            
Batch: #17   | Top-1 Accuracy: 0.8342245989304813            

Classifier Training Epoch #18
------------------------------------------
Epoch: #18   | Batch: #1    | Batch Loss: 0.5578791499137878            | Epoch Loss: 0.5578791499137878            
Epoch: #18   | Batch: #2    | Batch Loss: 0.5463675856590271            | Epoch Loss: 0.5521233677864075            
Epoch: #18   | Batch: #3    | Batch Loss: 0.535193681716919             | Epoch Loss: 0.546480139096578             
Epoch: #18   | Batch: #4    | Batch Loss: 0.5324532389640808            | Epoch Loss: 0.5429734140634537            
Epoch: #18   | Batch: #5    | Batch Loss: 0.6898863315582275            | Epoch Loss: 0.5723559975624084            
Epoch: #18   | Batch: #6    | Batch Loss: 0.6613672971725464            | Epoch Loss: 0.5871912141640981            
Epoch: #18   | Batch: #7    | Batch Loss: 0.5153281092643738            | Epoch Loss: 0.5769250563212803            
Epoch: #18   | Batch: #8    | Batch Loss: 0.6074942946434021            | Epoch Loss: 0.5807462111115456            
Epoch: #18   | Batch: #9    | Batch Loss: 0.42555364966392517           | Epoch Loss: 0.5635025931729211            
Epoch: #18   | Batch: #10   | Batch Loss: 0.5066779851913452            | Epoch Loss: 0.5578201323747635            
Epoch: #18   | Batch: #11   | Batch Loss: 0.5646326541900635            | Epoch Loss: 0.5584394525397908            
Epoch: #18   | Batch: #12   | Batch Loss: 0.5318811535835266            | Epoch Loss: 0.5562262609601021            
Epoch: #18   | Batch: #13   | Batch Loss: 0.5830497741699219            | Epoch Loss: 0.5582896081300882            
Epoch: #18   | Batch: #14   | Batch Loss: 0.4379929304122925            | Epoch Loss: 0.5496969882931028            
Epoch: #18   | Batch: #15   | Batch Loss: 0.6095197796821594            | Epoch Loss: 0.5536851743857066            
Epoch: #18   | Batch: #16   | Batch Loss: 0.4826248288154602            | Epoch Loss: 0.5492439027875662            
Epoch: #18   | Batch: #17   | Batch Loss: 0.5823686718940735            | Epoch Loss: 0.5511924186173607            
Epoch: #18   | Batch: #18   | Batch Loss: 0.6118413805961609            | Epoch Loss: 0.5545618053939607            
Epoch: #18   | Batch: #19   | Batch Loss: 0.5006725192070007            | Epoch Loss: 0.5517255271735945            
Epoch: #18   | Batch: #20   | Batch Loss: 0.4364888668060303            | Epoch Loss: 0.5459636941552162            
Epoch: #18   | Batch: #21   | Batch Loss: 0.628398060798645             | Epoch Loss: 0.5498891401858557            
Epoch: #18   | Batch: #22   | Batch Loss: 0.5389308929443359            | Epoch Loss: 0.5493910380385139            
Epoch: #18   | Batch: #23   | Batch Loss: 0.6300120949745178            | Epoch Loss: 0.5528963013835575            
Epoch: #18   | Batch: #24   | Batch Loss: 0.5556004047393799            | Epoch Loss: 0.5530089723567168            
Epoch: #18   | Batch: #25   | Batch Loss: 0.5528814792633057            | Epoch Loss: 0.5530038726329803            
Epoch: #18   | Batch: #26   | Batch Loss: 0.6156930327415466            | Epoch Loss: 0.5554149941756175            
Epoch: #18   | Batch: #27   | Batch Loss: 0.446226567029953             | Epoch Loss: 0.5513709783554077            
Epoch: #18   | Batch: #28   | Batch Loss: 0.6293792724609375            | Epoch Loss: 0.5541569888591766            
Epoch: #18   | Batch: #29   | Batch Loss: 0.5852090716362               | Epoch Loss: 0.5552277503342464            
Epoch: #18   | Batch: #30   | Batch Loss: 0.6254404187202454            | Epoch Loss: 0.5575681726137797            
Epoch: #18   | Batch: #31   | Batch Loss: 0.5922994017601013            | Epoch Loss: 0.5586885348443063            
Epoch: #18   | Batch: #32   | Batch Loss: 0.5518547892570496            | Epoch Loss: 0.5584749802947044            
Epoch: #18   | Batch: #33   | Batch Loss: 0.6306271553039551            | Epoch Loss: 0.5606614098404393            
Epoch: #18   | Batch: #34   | Batch Loss: 0.6272872090339661            | Epoch Loss: 0.5626209921696607            
Epoch: #18   | Batch: #35   | Batch Loss: 0.6772893667221069            | Epoch Loss: 0.5658972314425877            
Epoch: #18   | Batch: #36   | Batch Loss: 0.6939668655395508            | Epoch Loss: 0.5694547212786145            
Epoch: #18   | Batch: #37   | Batch Loss: 0.6573402881622314            | Epoch Loss: 0.5718300068700636            
Epoch: #18   | Batch: #38   | Batch Loss: 0.5701820254325867            | Epoch Loss: 0.5717866389374984            
Epoch: #18   | Batch: #39   | Batch Loss: 0.5406333804130554            | Epoch Loss: 0.5709878374368716            
Epoch: #18   | Batch: #40   | Batch Loss: 0.7563955187797546            | Epoch Loss: 0.5756230294704437            
Epoch: #18   | Batch: #41   | Batch Loss: 0.5845758318901062            | Epoch Loss: 0.5758413905050697            
Epoch: #18   | Batch: #42   | Batch Loss: 0.5236584544181824            | Epoch Loss: 0.5745989396458581            
Epoch: #18   | Batch: #43   | Batch Loss: 0.6102531552314758            | Epoch Loss: 0.5754281074501747            

Classifier Validation Epoch #18
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8272727272727273            
Batch: #2    | Top-1 Accuracy: 0.8363636363636364            
Batch: #3    | Top-1 Accuracy: 0.8484848484848485            
Batch: #4    | Top-1 Accuracy: 0.8454545454545455            
Batch: #5    | Top-1 Accuracy: 0.8445454545454545            
Batch: #6    | Top-1 Accuracy: 0.8439393939393939            
Batch: #7    | Top-1 Accuracy: 0.8428571428571427            
Batch: #8    | Top-1 Accuracy: 0.8414772727272727            
Batch: #9    | Top-1 Accuracy: 0.8388888888888889            
Batch: #10   | Top-1 Accuracy: 0.8422727272727272            
Batch: #11   | Top-1 Accuracy: 0.8400826446280991            
Batch: #12   | Top-1 Accuracy: 0.8409090909090908            
Batch: #13   | Top-1 Accuracy: 0.8419580419580419            
Batch: #14   | Top-1 Accuracy: 0.8422077922077922            
Batch: #15   | Top-1 Accuracy: 0.8412121212121213            
Batch: #16   | Top-1 Accuracy: 0.840625                      
Batch: #17   | Top-1 Accuracy: 0.8406417112299466            

Classifier Training Epoch #19
------------------------------------------
Epoch: #19   | Batch: #1    | Batch Loss: 0.5740258693695068            | Epoch Loss: 0.5740258693695068            
Epoch: #19   | Batch: #2    | Batch Loss: 0.4660598039627075            | Epoch Loss: 0.5200428366661072            
Epoch: #19   | Batch: #3    | Batch Loss: 0.4894750118255615            | Epoch Loss: 0.5098535617192587            
Epoch: #19   | Batch: #4    | Batch Loss: 0.4885043501853943            | Epoch Loss: 0.5045162588357925            
Epoch: #19   | Batch: #5    | Batch Loss: 0.5708231329917908            | Epoch Loss: 0.5177776336669921            
Epoch: #19   | Batch: #6    | Batch Loss: 0.7346699237823486            | Epoch Loss: 0.5539263486862183            
Epoch: #19   | Batch: #7    | Batch Loss: 0.39343762397766113           | Epoch Loss: 0.5309993880135673            
Epoch: #19   | Batch: #8    | Batch Loss: 0.441201388835907             | Epoch Loss: 0.5197746381163597            
Epoch: #19   | Batch: #9    | Batch Loss: 0.5382062196731567            | Epoch Loss: 0.5218225916226705            
Epoch: #19   | Batch: #10   | Batch Loss: 0.5648511648178101            | Epoch Loss: 0.5261254489421845            
Epoch: #19   | Batch: #11   | Batch Loss: 0.5119912028312683            | Epoch Loss: 0.5248405174775557            
Epoch: #19   | Batch: #12   | Batch Loss: 0.5687832832336426            | Epoch Loss: 0.5285024146238962            
Epoch: #19   | Batch: #13   | Batch Loss: 0.5609937310218811            | Epoch Loss: 0.5310017466545105            
Epoch: #19   | Batch: #14   | Batch Loss: 0.49049821496009827           | Epoch Loss: 0.5281086372477668            
Epoch: #19   | Batch: #15   | Batch Loss: 0.6300325393676758            | Epoch Loss: 0.5349035640557607            
Epoch: #19   | Batch: #16   | Batch Loss: 0.5320302844047546            | Epoch Loss: 0.5347239840775728            
Epoch: #19   | Batch: #17   | Batch Loss: 0.541917622089386             | Epoch Loss: 0.5351471392547383            
Epoch: #19   | Batch: #18   | Batch Loss: 0.6084936857223511            | Epoch Loss: 0.5392219473918279            
Epoch: #19   | Batch: #19   | Batch Loss: 0.42283210158348083           | Epoch Loss: 0.5330961660334939            
Epoch: #19   | Batch: #20   | Batch Loss: 0.5792527198791504            | Epoch Loss: 0.5354039937257766            
Epoch: #19   | Batch: #21   | Batch Loss: 0.5723206996917725            | Epoch Loss: 0.5371619321051098            
Epoch: #19   | Batch: #22   | Batch Loss: 0.6210096478462219            | Epoch Loss: 0.540973191911524             
Epoch: #19   | Batch: #23   | Batch Loss: 0.6030658483505249            | Epoch Loss: 0.5436728726262632            
Epoch: #19   | Batch: #24   | Batch Loss: 0.5067641139030457            | Epoch Loss: 0.5421350076794624            
Epoch: #19   | Batch: #25   | Batch Loss: 0.6769185066223145            | Epoch Loss: 0.5475263476371766            
Epoch: #19   | Batch: #26   | Batch Loss: 0.4481081962585449            | Epoch Loss: 0.5437025725841522            
Epoch: #19   | Batch: #27   | Batch Loss: 0.5809884667396545            | Epoch Loss: 0.5450835316269486            
Epoch: #19   | Batch: #28   | Batch Loss: 0.72843337059021              | Epoch Loss: 0.5516317401613507            
Epoch: #19   | Batch: #29   | Batch Loss: 0.6254695057868958            | Epoch Loss: 0.5541778700105076            
Epoch: #19   | Batch: #30   | Batch Loss: 0.6981070041656494            | Epoch Loss: 0.558975507815679             
Epoch: #19   | Batch: #31   | Batch Loss: 0.5241907238960266            | Epoch Loss: 0.5578534180118192            
Epoch: #19   | Batch: #32   | Batch Loss: 0.5177362561225891            | Epoch Loss: 0.5565997567027807            
Epoch: #19   | Batch: #33   | Batch Loss: 0.5713056921958923            | Epoch Loss: 0.5570453911116628            
Epoch: #19   | Batch: #34   | Batch Loss: 0.5980235934257507            | Epoch Loss: 0.5582506323561949            
Epoch: #19   | Batch: #35   | Batch Loss: 0.7433315515518188            | Epoch Loss: 0.563538658618927             
Epoch: #19   | Batch: #36   | Batch Loss: 0.6280211806297302            | Epoch Loss: 0.5653298397858938            
Epoch: #19   | Batch: #37   | Batch Loss: 0.5967732071876526            | Epoch Loss: 0.5661796605264818            
Epoch: #19   | Batch: #38   | Batch Loss: 0.6503227949142456            | Epoch Loss: 0.5683939535366861            
Epoch: #19   | Batch: #39   | Batch Loss: 0.6026347279548645            | Epoch Loss: 0.5692719221115112            
Epoch: #19   | Batch: #40   | Batch Loss: 0.6376423835754395            | Epoch Loss: 0.5709811836481095            
Epoch: #19   | Batch: #41   | Batch Loss: 0.6260105967521667            | Epoch Loss: 0.5723233644555255            
Epoch: #19   | Batch: #42   | Batch Loss: 0.44431841373443604           | Epoch Loss: 0.5692756275335947            
Epoch: #19   | Batch: #43   | Batch Loss: 0.5156611800193787            | Epoch Loss: 0.5680287799169851            

Classifier Validation Epoch #19
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.7863636363636364            
Batch: #2    | Top-1 Accuracy: 0.8022727272727272            
Batch: #3    | Top-1 Accuracy: 0.8136363636363636            
Batch: #4    | Top-1 Accuracy: 0.8147727272727273            
Batch: #5    | Top-1 Accuracy: 0.8245454545454546            
Batch: #6    | Top-1 Accuracy: 0.831060606060606             
Batch: #7    | Top-1 Accuracy: 0.8331168831168831            
Batch: #8    | Top-1 Accuracy: 0.8284090909090909            
Batch: #9    | Top-1 Accuracy: 0.8262626262626261            
Batch: #10   | Top-1 Accuracy: 0.8263636363636364            
Batch: #11   | Top-1 Accuracy: 0.824793388429752             
Batch: #12   | Top-1 Accuracy: 0.8238636363636364            
Batch: #13   | Top-1 Accuracy: 0.8283216783216785            
Batch: #14   | Top-1 Accuracy: 0.827922077922078             
Batch: #15   | Top-1 Accuracy: 0.8293939393939395            
Batch: #16   | Top-1 Accuracy: 0.8292613636363637            
Batch: #17   | Top-1 Accuracy: 0.8296791443850268            

Classifier Training Epoch #20
------------------------------------------
Epoch: #20   | Batch: #1    | Batch Loss: 0.5931795239448547            | Epoch Loss: 0.5931795239448547            
Epoch: #20   | Batch: #2    | Batch Loss: 0.6195093393325806            | Epoch Loss: 0.6063444316387177            
Epoch: #20   | Batch: #3    | Batch Loss: 0.5557029843330383            | Epoch Loss: 0.5894639492034912            
Epoch: #20   | Batch: #4    | Batch Loss: 0.6153423190116882            | Epoch Loss: 0.5959335416555405            
Epoch: #20   | Batch: #5    | Batch Loss: 0.4910362958908081            | Epoch Loss: 0.574954092502594             
Epoch: #20   | Batch: #6    | Batch Loss: 0.5628001689910889            | Epoch Loss: 0.5729284385840098            
Epoch: #20   | Batch: #7    | Batch Loss: 0.5017597675323486            | Epoch Loss: 0.5627614855766296            
Epoch: #20   | Batch: #8    | Batch Loss: 0.6293052434921265            | Epoch Loss: 0.5710794553160667            
Epoch: #20   | Batch: #9    | Batch Loss: 0.5912458896636963            | Epoch Loss: 0.5733201702435812            
Epoch: #20   | Batch: #10   | Batch Loss: 0.5526111125946045            | Epoch Loss: 0.5712492644786835            
Epoch: #20   | Batch: #11   | Batch Loss: 0.49733561277389526           | Epoch Loss: 0.56452984159643              
Epoch: #20   | Batch: #12   | Batch Loss: 0.5568210482597351            | Epoch Loss: 0.5638874421517054            
Epoch: #20   | Batch: #13   | Batch Loss: 0.6150977611541748            | Epoch Loss: 0.5678266974595877            
Epoch: #20   | Batch: #14   | Batch Loss: 0.5319340229034424            | Epoch Loss: 0.5652629349912915            
Epoch: #20   | Batch: #15   | Batch Loss: 0.6553108096122742            | Epoch Loss: 0.5712661266326904            
Epoch: #20   | Batch: #16   | Batch Loss: 0.6051435470581055            | Epoch Loss: 0.5733834654092789            
Epoch: #20   | Batch: #17   | Batch Loss: 0.6700752377510071            | Epoch Loss: 0.5790712167234982            
Epoch: #20   | Batch: #18   | Batch Loss: 0.6875413656234741            | Epoch Loss: 0.5850973361068301            
Epoch: #20   | Batch: #19   | Batch Loss: 0.4645811915397644            | Epoch Loss: 0.5787543811296162            
Epoch: #20   | Batch: #20   | Batch Loss: 0.5324438810348511            | Epoch Loss: 0.5764388561248779            
Epoch: #20   | Batch: #21   | Batch Loss: 0.633574903011322             | Epoch Loss: 0.5791596202623277            
Epoch: #20   | Batch: #22   | Batch Loss: 0.5794240832328796            | Epoch Loss: 0.5791716413064436            
Epoch: #20   | Batch: #23   | Batch Loss: 0.511233389377594             | Epoch Loss: 0.5762178042660588            
Epoch: #20   | Batch: #24   | Batch Loss: 0.4897373914718628            | Epoch Loss: 0.5726144537329674            
Epoch: #20   | Batch: #25   | Batch Loss: 0.4899754226207733            | Epoch Loss: 0.5693088924884796            
Epoch: #20   | Batch: #26   | Batch Loss: 0.6559686660766602            | Epoch Loss: 0.5726419607034097            
Epoch: #20   | Batch: #27   | Batch Loss: 0.5242819786071777            | Epoch Loss: 0.570850850255401             
Epoch: #20   | Batch: #28   | Batch Loss: 0.5466691255569458            | Epoch Loss: 0.5699872172304562            
Epoch: #20   | Batch: #29   | Batch Loss: 0.5430968999862671            | Epoch Loss: 0.5690599649116911            
Epoch: #20   | Batch: #30   | Batch Loss: 0.6980637311935425            | Epoch Loss: 0.5733600904544195            
Epoch: #20   | Batch: #31   | Batch Loss: 0.593467652797699             | Epoch Loss: 0.5740087214977511            
Epoch: #20   | Batch: #32   | Batch Loss: 0.6107339859008789            | Epoch Loss: 0.5751563860103488            
Epoch: #20   | Batch: #33   | Batch Loss: 0.5628423690795898            | Epoch Loss: 0.5747832339821439            
Epoch: #20   | Batch: #34   | Batch Loss: 0.6151262521743774            | Epoch Loss: 0.5759697933407391            
Epoch: #20   | Batch: #35   | Batch Loss: 0.5941637754440308            | Epoch Loss: 0.5764896214008332            
Epoch: #20   | Batch: #36   | Batch Loss: 0.4971669316291809            | Epoch Loss: 0.5742862133516206            
Epoch: #20   | Batch: #37   | Batch Loss: 0.6432241201400757            | Epoch Loss: 0.5761494000215788            
Epoch: #20   | Batch: #38   | Batch Loss: 0.5132940411567688            | Epoch Loss: 0.5744953116303996            
Epoch: #20   | Batch: #39   | Batch Loss: 0.5256257653236389            | Epoch Loss: 0.5732422463404827            
Epoch: #20   | Batch: #40   | Batch Loss: 0.5844243764877319            | Epoch Loss: 0.5735217995941639            
Epoch: #20   | Batch: #41   | Batch Loss: 0.5904006361961365            | Epoch Loss: 0.5739334785356754            
Epoch: #20   | Batch: #42   | Batch Loss: 0.5113419890403748            | Epoch Loss: 0.5724432049762636            
Epoch: #20   | Batch: #43   | Batch Loss: 0.6225597262382507            | Epoch Loss: 0.5736087054707283            

Classifier Validation Epoch #20
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8409090909090909            
Batch: #2    | Top-1 Accuracy: 0.8318181818181818            
Batch: #3    | Top-1 Accuracy: 0.8181818181818182            
Batch: #4    | Top-1 Accuracy: 0.825                         
Batch: #5    | Top-1 Accuracy: 0.8245454545454546            
Batch: #6    | Top-1 Accuracy: 0.8303030303030304            
Batch: #7    | Top-1 Accuracy: 0.8324675324675326            
Batch: #8    | Top-1 Accuracy: 0.8306818181818182            
Batch: #9    | Top-1 Accuracy: 0.8303030303030303            
Batch: #10   | Top-1 Accuracy: 0.8313636363636364            
Batch: #11   | Top-1 Accuracy: 0.8334710743801653            
Batch: #12   | Top-1 Accuracy: 0.8359848484848484            
Batch: #13   | Top-1 Accuracy: 0.8363636363636364            
Batch: #14   | Top-1 Accuracy: 0.8376623376623377            
Batch: #15   | Top-1 Accuracy: 0.8345454545454545            
Batch: #16   | Top-1 Accuracy: 0.8349431818181818            
Batch: #17   | Top-1 Accuracy: 0.8358288770053476            

Classifier Training Epoch #21
------------------------------------------
Epoch: #21   | Batch: #1    | Batch Loss: 0.535608172416687             | Epoch Loss: 0.535608172416687             
Epoch: #21   | Batch: #2    | Batch Loss: 0.629769504070282             | Epoch Loss: 0.5826888382434845            
Epoch: #21   | Batch: #3    | Batch Loss: 0.614163339138031             | Epoch Loss: 0.5931803385416666            
Epoch: #21   | Batch: #4    | Batch Loss: 0.63033127784729              | Epoch Loss: 0.6024680733680725            
Epoch: #21   | Batch: #5    | Batch Loss: 0.5654041767120361            | Epoch Loss: 0.5950552940368652            
Epoch: #21   | Batch: #6    | Batch Loss: 0.5248256921768188            | Epoch Loss: 0.5833503603935242            
Epoch: #21   | Batch: #7    | Batch Loss: 0.7028563022613525            | Epoch Loss: 0.6004226378032139            
Epoch: #21   | Batch: #8    | Batch Loss: 0.5567147135734558            | Epoch Loss: 0.5949591472744942            
Epoch: #21   | Batch: #9    | Batch Loss: 0.6264737844467163            | Epoch Loss: 0.5984607736269633            
Epoch: #21   | Batch: #10   | Batch Loss: 0.6109618544578552            | Epoch Loss: 0.5997108817100525            
Epoch: #21   | Batch: #11   | Batch Loss: 0.489846795797348             | Epoch Loss: 0.5897232375361703            
Epoch: #21   | Batch: #12   | Batch Loss: 0.6614357829093933            | Epoch Loss: 0.5956992829839388            
Epoch: #21   | Batch: #13   | Batch Loss: 0.45591139793395996           | Epoch Loss: 0.584946368749325             
Epoch: #21   | Batch: #14   | Batch Loss: 0.6788026094436646            | Epoch Loss: 0.5916503859417779            
Epoch: #21   | Batch: #15   | Batch Loss: 0.5551722645759583            | Epoch Loss: 0.5892185111840565            
Epoch: #21   | Batch: #16   | Batch Loss: 0.6259629130363464            | Epoch Loss: 0.5915150362998247            
Epoch: #21   | Batch: #17   | Batch Loss: 0.5625361204147339            | Epoch Loss: 0.589810394188937             
Epoch: #21   | Batch: #18   | Batch Loss: 0.5827246308326721            | Epoch Loss: 0.5894167406691445            
Epoch: #21   | Batch: #19   | Batch Loss: 0.5826680064201355            | Epoch Loss: 0.589061544129723             
Epoch: #21   | Batch: #20   | Batch Loss: 0.543731689453125             | Epoch Loss: 0.586795051395893             
Epoch: #21   | Batch: #21   | Batch Loss: 0.5335069894790649            | Epoch Loss: 0.5842575246379489            
Epoch: #21   | Batch: #22   | Batch Loss: 0.7455966472625732            | Epoch Loss: 0.5915911211208864            
Epoch: #21   | Batch: #23   | Batch Loss: 0.5821444392204285            | Epoch Loss: 0.5911803958208665            
Epoch: #21   | Batch: #24   | Batch Loss: 0.5904353857040405            | Epoch Loss: 0.5911493537326654            
Epoch: #21   | Batch: #25   | Batch Loss: 0.5970898270606995            | Epoch Loss: 0.5913869726657868            
Epoch: #21   | Batch: #26   | Batch Loss: 0.6563481688499451            | Epoch Loss: 0.5938854802113313            
Epoch: #21   | Batch: #27   | Batch Loss: 0.5270284414291382            | Epoch Loss: 0.5914092935897686            
Epoch: #21   | Batch: #28   | Batch Loss: 0.5364508628845215            | Epoch Loss: 0.5894464924931526            
Epoch: #21   | Batch: #29   | Batch Loss: 0.5640780925750732            | Epoch Loss: 0.5885717200821844            
Epoch: #21   | Batch: #30   | Batch Loss: 0.6336581707000732            | Epoch Loss: 0.5900746017694474            
Epoch: #21   | Batch: #31   | Batch Loss: 0.5260099172592163            | Epoch Loss: 0.5880079990433108            
Epoch: #21   | Batch: #32   | Batch Loss: 0.6000312566757202            | Epoch Loss: 0.5883837258443236            
Epoch: #21   | Batch: #33   | Batch Loss: 0.4629741311073303            | Epoch Loss: 0.5845834350947178            
Epoch: #21   | Batch: #34   | Batch Loss: 0.619565486907959             | Epoch Loss: 0.5856123189715778            
Epoch: #21   | Batch: #35   | Batch Loss: 0.43952640891075134           | Epoch Loss: 0.5814384358269827            
Epoch: #21   | Batch: #36   | Batch Loss: 0.664375901222229             | Epoch Loss: 0.583742254310184             
Epoch: #21   | Batch: #37   | Batch Loss: 0.6290669441223145            | Epoch Loss: 0.5849672459267281            
Epoch: #21   | Batch: #38   | Batch Loss: 0.6394879221916199            | Epoch Loss: 0.5864020005652779            
Epoch: #21   | Batch: #39   | Batch Loss: 0.6418752074241638            | Epoch Loss: 0.5878243904847366            
Epoch: #21   | Batch: #40   | Batch Loss: 0.5034205913543701            | Epoch Loss: 0.5857142955064774            
Epoch: #21   | Batch: #41   | Batch Loss: 0.5717476010322571            | Epoch Loss: 0.5853736444217402            
Epoch: #21   | Batch: #42   | Batch Loss: 0.6104848980903625            | Epoch Loss: 0.5859715314138503            
Epoch: #21   | Batch: #43   | Batch Loss: 0.49705901741981506           | Epoch Loss: 0.5839037985302681            

Classifier Validation Epoch #21
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.85                          
Batch: #2    | Top-1 Accuracy: 0.825                         
Batch: #3    | Top-1 Accuracy: 0.8318181818181817            
Batch: #4    | Top-1 Accuracy: 0.8397727272727272            
Batch: #5    | Top-1 Accuracy: 0.8381818181818181            
Batch: #6    | Top-1 Accuracy: 0.8356060606060606            
Batch: #7    | Top-1 Accuracy: 0.8370129870129871            
Batch: #8    | Top-1 Accuracy: 0.8392045454545454            
Batch: #9    | Top-1 Accuracy: 0.8348484848484847            
Batch: #10   | Top-1 Accuracy: 0.8363636363636363            
Batch: #11   | Top-1 Accuracy: 0.8338842975206611            
Batch: #12   | Top-1 Accuracy: 0.834469696969697             
Batch: #13   | Top-1 Accuracy: 0.8346153846153845            
Batch: #14   | Top-1 Accuracy: 0.8360389610389609            
Batch: #15   | Top-1 Accuracy: 0.8369696969696968            
Batch: #16   | Top-1 Accuracy: 0.834659090909091             
Batch: #17   | Top-1 Accuracy: 0.8336898395721926            

Classifier Training Epoch #22
------------------------------------------
Epoch: #22   | Batch: #1    | Batch Loss: 0.5439377427101135            | Epoch Loss: 0.5439377427101135            
Epoch: #22   | Batch: #2    | Batch Loss: 0.5596815347671509            | Epoch Loss: 0.5518096387386322            
Epoch: #22   | Batch: #3    | Batch Loss: 0.654075026512146             | Epoch Loss: 0.5858981013298035            
Epoch: #22   | Batch: #4    | Batch Loss: 0.43020591139793396           | Epoch Loss: 0.5469750538468361            
Epoch: #22   | Batch: #5    | Batch Loss: 0.49053260684013367           | Epoch Loss: 0.5356865644454956            
Epoch: #22   | Batch: #6    | Batch Loss: 0.5709620118141174            | Epoch Loss: 0.5415658056735992            
Epoch: #22   | Batch: #7    | Batch Loss: 0.5403443574905396            | Epoch Loss: 0.5413913130760193            
Epoch: #22   | Batch: #8    | Batch Loss: 0.5753888487815857            | Epoch Loss: 0.5456410050392151            
Epoch: #22   | Batch: #9    | Batch Loss: 0.5701127648353577            | Epoch Loss: 0.5483600894610087            
Epoch: #22   | Batch: #10   | Batch Loss: 0.5885077714920044            | Epoch Loss: 0.5523748576641083            
Epoch: #22   | Batch: #11   | Batch Loss: 0.602632999420166             | Epoch Loss: 0.5569437796419318            
Epoch: #22   | Batch: #12   | Batch Loss: 0.5973484516143799            | Epoch Loss: 0.5603108356396357            
Epoch: #22   | Batch: #13   | Batch Loss: 0.47742903232574463           | Epoch Loss: 0.5539353123077979            
Epoch: #22   | Batch: #14   | Batch Loss: 0.46648576855659485           | Epoch Loss: 0.5476889163255692            
Epoch: #22   | Batch: #15   | Batch Loss: 0.7029542326927185            | Epoch Loss: 0.5580399374167124            
Epoch: #22   | Batch: #16   | Batch Loss: 0.6312470436096191            | Epoch Loss: 0.5626153815537691            
Epoch: #22   | Batch: #17   | Batch Loss: 0.6934306621551514            | Epoch Loss: 0.5703103980597328            
Epoch: #22   | Batch: #18   | Batch Loss: 0.5855723023414612            | Epoch Loss: 0.57115828163094              
Epoch: #22   | Batch: #19   | Batch Loss: 0.5884180665016174            | Epoch Loss: 0.5720666913609755            
Epoch: #22   | Batch: #20   | Batch Loss: 0.42969217896461487           | Epoch Loss: 0.5649479657411576            
Epoch: #22   | Batch: #21   | Batch Loss: 0.5423737168312073            | Epoch Loss: 0.5638730015073504            
Epoch: #22   | Batch: #22   | Batch Loss: 0.5845707058906555            | Epoch Loss: 0.564813806252046             
Epoch: #22   | Batch: #23   | Batch Loss: 0.45088839530944824           | Epoch Loss: 0.5598605275154114            
Epoch: #22   | Batch: #24   | Batch Loss: 0.6393713355064392            | Epoch Loss: 0.5631734778483709            
Epoch: #22   | Batch: #25   | Batch Loss: 0.630489706993103             | Epoch Loss: 0.5658661270141602            
Epoch: #22   | Batch: #26   | Batch Loss: 0.5121241211891174            | Epoch Loss: 0.56379912679012              
Epoch: #22   | Batch: #27   | Batch Loss: 0.552872896194458             | Epoch Loss: 0.5633944515828733            
Epoch: #22   | Batch: #28   | Batch Loss: 0.5324277281761169            | Epoch Loss: 0.5622884971754891            
Epoch: #22   | Batch: #29   | Batch Loss: 0.5358943343162537            | Epoch Loss: 0.561378353628619             
Epoch: #22   | Batch: #30   | Batch Loss: 0.6290303468704224            | Epoch Loss: 0.5636334200700124            
Epoch: #22   | Batch: #31   | Batch Loss: 0.536669135093689             | Epoch Loss: 0.5627636044256149            
Epoch: #22   | Batch: #32   | Batch Loss: 0.5410899519920349            | Epoch Loss: 0.5620863027870655            
Epoch: #22   | Batch: #33   | Batch Loss: 0.5993202924728394            | Epoch Loss: 0.5632146055048163            
Epoch: #22   | Batch: #34   | Batch Loss: 0.6175915598869324            | Epoch Loss: 0.5648139276925255            
Epoch: #22   | Batch: #35   | Batch Loss: 0.5825801491737366            | Epoch Loss: 0.5653215340205602            
Epoch: #22   | Batch: #36   | Batch Loss: 0.6086351275444031            | Epoch Loss: 0.5665246893962225            
Epoch: #22   | Batch: #37   | Batch Loss: 0.5874916315078735            | Epoch Loss: 0.5670913635073481            
Epoch: #22   | Batch: #38   | Batch Loss: 0.5540668368339539            | Epoch Loss: 0.5667486128054167            
Epoch: #22   | Batch: #39   | Batch Loss: 0.628051221370697             | Epoch Loss: 0.5683204745635008            
Epoch: #22   | Batch: #40   | Batch Loss: 0.5573492646217346            | Epoch Loss: 0.5680461943149566            
Epoch: #22   | Batch: #41   | Batch Loss: 0.5313091278076172            | Epoch Loss: 0.5671501683025826            
Epoch: #22   | Batch: #42   | Batch Loss: 0.5541166067123413            | Epoch Loss: 0.5668398454075768            
Epoch: #22   | Batch: #43   | Batch Loss: 0.6336126923561096            | Epoch Loss: 0.5683927023133566            

Classifier Validation Epoch #22
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8272727272727273            
Batch: #2    | Top-1 Accuracy: 0.8295454545454546            
Batch: #3    | Top-1 Accuracy: 0.8257575757575758            
Batch: #4    | Top-1 Accuracy: 0.8329545454545455            
Batch: #5    | Top-1 Accuracy: 0.8309090909090908            
Batch: #6    | Top-1 Accuracy: 0.8333333333333334            
Batch: #7    | Top-1 Accuracy: 0.8298701298701298            
Batch: #8    | Top-1 Accuracy: 0.828409090909091             
Batch: #9    | Top-1 Accuracy: 0.8272727272727274            
Batch: #10   | Top-1 Accuracy: 0.8281818181818184            
Batch: #11   | Top-1 Accuracy: 0.831404958677686             
Batch: #12   | Top-1 Accuracy: 0.8325757575757576            
Batch: #13   | Top-1 Accuracy: 0.8353146853146853            
Batch: #14   | Top-1 Accuracy: 0.8379870129870131            
Batch: #15   | Top-1 Accuracy: 0.8360606060606061            
Batch: #16   | Top-1 Accuracy: 0.8360795454545454            
Batch: #17   | Top-1 Accuracy: 0.8347593582887701            

Classifier Training Epoch #23
------------------------------------------
Epoch: #23   | Batch: #1    | Batch Loss: 0.6170744299888611            | Epoch Loss: 0.6170744299888611            
Epoch: #23   | Batch: #2    | Batch Loss: 0.5436358451843262            | Epoch Loss: 0.5803551375865936            
Epoch: #23   | Batch: #3    | Batch Loss: 0.5579360723495483            | Epoch Loss: 0.5728821158409119            
Epoch: #23   | Batch: #4    | Batch Loss: 0.4860655963420868            | Epoch Loss: 0.5511779859662056            
Epoch: #23   | Batch: #5    | Batch Loss: 0.523687481880188             | Epoch Loss: 0.5456798851490021            
Epoch: #23   | Batch: #6    | Batch Loss: 0.5169299840927124            | Epoch Loss: 0.5408882349729538            
Epoch: #23   | Batch: #7    | Batch Loss: 0.5259951949119568            | Epoch Loss: 0.5387606578213828            
Epoch: #23   | Batch: #8    | Batch Loss: 0.5589303374290466            | Epoch Loss: 0.5412818677723408            
Epoch: #23   | Batch: #9    | Batch Loss: 0.6048832535743713            | Epoch Loss: 0.5483486884170108            
Epoch: #23   | Batch: #10   | Batch Loss: 0.5043456554412842            | Epoch Loss: 0.5439483851194382            
Epoch: #23   | Batch: #11   | Batch Loss: 0.4967019855976105            | Epoch Loss: 0.5396532578901811            
Epoch: #23   | Batch: #12   | Batch Loss: 0.6685834527015686            | Epoch Loss: 0.5503974407911301            
Epoch: #23   | Batch: #13   | Batch Loss: 0.5521966814994812            | Epoch Loss: 0.5505358439225417            
Epoch: #23   | Batch: #14   | Batch Loss: 0.695671021938324             | Epoch Loss: 0.5609026423522404            
Epoch: #23   | Batch: #15   | Batch Loss: 0.5006618499755859            | Epoch Loss: 0.5568865895271301            
Epoch: #23   | Batch: #16   | Batch Loss: 0.5907390117645264            | Epoch Loss: 0.5590023659169674            
Epoch: #23   | Batch: #17   | Batch Loss: 0.5959746837615967            | Epoch Loss: 0.5611772081431221            
Epoch: #23   | Batch: #18   | Batch Loss: 0.6146893501281738            | Epoch Loss: 0.5641501049200693            
Epoch: #23   | Batch: #19   | Batch Loss: 0.5977694988250732            | Epoch Loss: 0.5659195467045433            
Epoch: #23   | Batch: #20   | Batch Loss: 0.4831407070159912            | Epoch Loss: 0.5617806047201157            
Epoch: #23   | Batch: #21   | Batch Loss: 0.4993313252925873            | Epoch Loss: 0.558806829509281             
Epoch: #23   | Batch: #22   | Batch Loss: 0.5528474450111389            | Epoch Loss: 0.5585359483957291            
Epoch: #23   | Batch: #23   | Batch Loss: 0.5571190714836121            | Epoch Loss: 0.558474345051724             
Epoch: #23   | Batch: #24   | Batch Loss: 0.5698992013931274            | Epoch Loss: 0.5589503807326158            
Epoch: #23   | Batch: #25   | Batch Loss: 0.5973193645477295            | Epoch Loss: 0.5604851400852203            
Epoch: #23   | Batch: #26   | Batch Loss: 0.6088321805000305            | Epoch Loss: 0.5623446416396362            
Epoch: #23   | Batch: #27   | Batch Loss: 0.5514128804206848            | Epoch Loss: 0.5619397615944898            
Epoch: #23   | Batch: #28   | Batch Loss: 0.6358901262283325            | Epoch Loss: 0.5645808460456985            
Epoch: #23   | Batch: #29   | Batch Loss: 0.5703554749488831            | Epoch Loss: 0.564779971180291             
Epoch: #23   | Batch: #30   | Batch Loss: 0.7191929221153259            | Epoch Loss: 0.5699270695447922            
Epoch: #23   | Batch: #31   | Batch Loss: 0.5548242330551147            | Epoch Loss: 0.5694398812709316            
Epoch: #23   | Batch: #32   | Batch Loss: 0.5300890207290649            | Epoch Loss: 0.5682101668789983            
Epoch: #23   | Batch: #33   | Batch Loss: 0.4581397771835327            | Epoch Loss: 0.5648747005245902            
Epoch: #23   | Batch: #34   | Batch Loss: 0.5808082818984985            | Epoch Loss: 0.5653433352708817            
Epoch: #23   | Batch: #35   | Batch Loss: 0.6626180410385132            | Epoch Loss: 0.5681226125785283            
Epoch: #23   | Batch: #36   | Batch Loss: 0.6289660930633545            | Epoch Loss: 0.5698127092586623            
Epoch: #23   | Batch: #37   | Batch Loss: 0.593055784702301             | Epoch Loss: 0.5704409004868688            
Epoch: #23   | Batch: #38   | Batch Loss: 0.6950094699859619            | Epoch Loss: 0.5737190207368449            
Epoch: #23   | Batch: #39   | Batch Loss: 0.5075372457504272            | Epoch Loss: 0.5720220521474496            
Epoch: #23   | Batch: #40   | Batch Loss: 0.6094462871551514            | Epoch Loss: 0.5729576580226421            
Epoch: #23   | Batch: #41   | Batch Loss: 0.647239089012146             | Epoch Loss: 0.5747694002418984            
Epoch: #23   | Batch: #42   | Batch Loss: 0.5003745555877686            | Epoch Loss: 0.5729980944168               
Epoch: #23   | Batch: #43   | Batch Loss: 0.6046573519706726            | Epoch Loss: 0.5737343562203784            

Classifier Validation Epoch #23
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8227272727272728            
Batch: #2    | Top-1 Accuracy: 0.8090909090909091            
Batch: #3    | Top-1 Accuracy: 0.8196969696969697            
Batch: #4    | Top-1 Accuracy: 0.8272727272727273            
Batch: #5    | Top-1 Accuracy: 0.8327272727272728            
Batch: #6    | Top-1 Accuracy: 0.8401515151515152            
Batch: #7    | Top-1 Accuracy: 0.8383116883116883            
Batch: #8    | Top-1 Accuracy: 0.8375                        
Batch: #9    | Top-1 Accuracy: 0.8393939393939394            
Batch: #10   | Top-1 Accuracy: 0.8427272727272728            
Batch: #11   | Top-1 Accuracy: 0.8450413223140497            
Batch: #12   | Top-1 Accuracy: 0.8450757575757577            
Batch: #13   | Top-1 Accuracy: 0.8423076923076924            
Batch: #14   | Top-1 Accuracy: 0.8431818181818181            
Batch: #15   | Top-1 Accuracy: 0.843939393939394             
Batch: #16   | Top-1 Accuracy: 0.84375                       
Batch: #17   | Top-1 Accuracy: 0.8457219251336898            

Classifier Training Epoch #24
------------------------------------------
Epoch: #24   | Batch: #1    | Batch Loss: 0.5337779521942139            | Epoch Loss: 0.5337779521942139            
Epoch: #24   | Batch: #2    | Batch Loss: 0.5928090810775757            | Epoch Loss: 0.5632935166358948            
Epoch: #24   | Batch: #3    | Batch Loss: 0.5569244027137756            | Epoch Loss: 0.5611704786618551            
Epoch: #24   | Batch: #4    | Batch Loss: 0.4966792166233063            | Epoch Loss: 0.5450476631522179            
Epoch: #24   | Batch: #5    | Batch Loss: 0.6052154302597046            | Epoch Loss: 0.5570812165737152            
Epoch: #24   | Batch: #6    | Batch Loss: 0.6168800592422485            | Epoch Loss: 0.5670476903518041            
Epoch: #24   | Batch: #7    | Batch Loss: 0.575547456741333             | Epoch Loss: 0.5682619426931653            
Epoch: #24   | Batch: #8    | Batch Loss: 0.5473321676254272            | Epoch Loss: 0.5656457208096981            
Epoch: #24   | Batch: #9    | Batch Loss: 0.5739783048629761            | Epoch Loss: 0.5665715634822845            
Epoch: #24   | Batch: #10   | Batch Loss: 0.5656739473342896            | Epoch Loss: 0.5664818018674851            
Epoch: #24   | Batch: #11   | Batch Loss: 0.5256891250610352            | Epoch Loss: 0.5627733767032623            
Epoch: #24   | Batch: #12   | Batch Loss: 0.498623251914978             | Epoch Loss: 0.5574275329709053            
Epoch: #24   | Batch: #13   | Batch Loss: 0.533494770526886             | Epoch Loss: 0.5555865512444422            
Epoch: #24   | Batch: #14   | Batch Loss: 0.5069161057472229            | Epoch Loss: 0.5521100908517838            
Epoch: #24   | Batch: #15   | Batch Loss: 0.504700779914856             | Epoch Loss: 0.5489494701226553            
Epoch: #24   | Batch: #16   | Batch Loss: 0.5834953188896179            | Epoch Loss: 0.5511085856705904            
Epoch: #24   | Batch: #17   | Batch Loss: 0.5351913571357727            | Epoch Loss: 0.5501722781097188            
Epoch: #24   | Batch: #18   | Batch Loss: 0.5470430254936218            | Epoch Loss: 0.5499984307421578            
Epoch: #24   | Batch: #19   | Batch Loss: 0.5516523122787476            | Epoch Loss: 0.5500854771388205            
Epoch: #24   | Batch: #20   | Batch Loss: 0.633397102355957             | Epoch Loss: 0.5542510583996773            
Epoch: #24   | Batch: #21   | Batch Loss: 0.6325570940971375            | Epoch Loss: 0.5579799172424135            
Epoch: #24   | Batch: #22   | Batch Loss: 0.6418300867080688            | Epoch Loss: 0.5617912885817614            
Epoch: #24   | Batch: #23   | Batch Loss: 0.5873910784721375            | Epoch Loss: 0.5629043229248213            
Epoch: #24   | Batch: #24   | Batch Loss: 0.5635336637496948            | Epoch Loss: 0.562930545459191             
Epoch: #24   | Batch: #25   | Batch Loss: 0.5387732982635498            | Epoch Loss: 0.5619642555713653            
Epoch: #24   | Batch: #26   | Batch Loss: 0.47005000710487366           | Epoch Loss: 0.558429092168808             
Epoch: #24   | Batch: #27   | Batch Loss: 0.5500572323799133            | Epoch Loss: 0.5581190232877378            
Epoch: #24   | Batch: #28   | Batch Loss: 0.5974719524383545            | Epoch Loss: 0.559524485043117             
Epoch: #24   | Batch: #29   | Batch Loss: 0.6202114224433899            | Epoch Loss: 0.5616171380569195            
Epoch: #24   | Batch: #30   | Batch Loss: 0.4492192566394806            | Epoch Loss: 0.5578705420096716            
Epoch: #24   | Batch: #31   | Batch Loss: 0.5270431041717529            | Epoch Loss: 0.556876108531029             
Epoch: #24   | Batch: #32   | Batch Loss: 0.5041358470916748            | Epoch Loss: 0.5552279753610492            
Epoch: #24   | Batch: #33   | Batch Loss: 0.5538728833198547            | Epoch Loss: 0.5551869119658615            
Epoch: #24   | Batch: #34   | Batch Loss: 0.6535964012145996            | Epoch Loss: 0.5580813087084714            
Epoch: #24   | Batch: #35   | Batch Loss: 0.594398021697998             | Epoch Loss: 0.5591189290796007            
Epoch: #24   | Batch: #36   | Batch Loss: 0.658776581287384             | Epoch Loss: 0.5618871971964836            
Epoch: #24   | Batch: #37   | Batch Loss: 0.6614501476287842            | Epoch Loss: 0.564578087748708             
Epoch: #24   | Batch: #38   | Batch Loss: 0.4812104403972626            | Epoch Loss: 0.562384202292091             
Epoch: #24   | Batch: #39   | Batch Loss: 0.5564123392105103            | Epoch Loss: 0.5622310775976914            
Epoch: #24   | Batch: #40   | Batch Loss: 0.5615572929382324            | Epoch Loss: 0.562214232981205             
Epoch: #24   | Batch: #41   | Batch Loss: 0.5374622941017151            | Epoch Loss: 0.5616105271548759            
Epoch: #24   | Batch: #42   | Batch Loss: 0.4617480933666229            | Epoch Loss: 0.5592328501599175            
Epoch: #24   | Batch: #43   | Batch Loss: 0.5541926622390747            | Epoch Loss: 0.5591156364873399            

Classifier Validation Epoch #24
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8318181818181818            
Batch: #2    | Top-1 Accuracy: 0.8409090909090908            
Batch: #3    | Top-1 Accuracy: 0.8409090909090908            
Batch: #4    | Top-1 Accuracy: 0.8386363636363636            
Batch: #5    | Top-1 Accuracy: 0.8372727272727273            
Batch: #6    | Top-1 Accuracy: 0.8386363636363637            
Batch: #7    | Top-1 Accuracy: 0.8415584415584416            
Batch: #8    | Top-1 Accuracy: 0.8392045454545455            
Batch: #9    | Top-1 Accuracy: 0.8434343434343434            
Batch: #10   | Top-1 Accuracy: 0.8440909090909091            
Batch: #11   | Top-1 Accuracy: 0.8417355371900826            
Batch: #12   | Top-1 Accuracy: 0.8431818181818183            
Batch: #13   | Top-1 Accuracy: 0.8360139860139861            
Batch: #14   | Top-1 Accuracy: 0.8350649350649351            
Batch: #15   | Top-1 Accuracy: 0.8345454545454546            
Batch: #16   | Top-1 Accuracy: 0.8363636363636364            
Batch: #17   | Top-1 Accuracy: 0.8331550802139037            

Classifier Training Epoch #25
------------------------------------------
Epoch: #25   | Batch: #1    | Batch Loss: 0.4401474595069885            | Epoch Loss: 0.4401474595069885            
Epoch: #25   | Batch: #2    | Batch Loss: 0.45268574357032776           | Epoch Loss: 0.44641660153865814           
Epoch: #25   | Batch: #3    | Batch Loss: 0.654194712638855             | Epoch Loss: 0.5156759719053904            
Epoch: #25   | Batch: #4    | Batch Loss: 0.5704439878463745            | Epoch Loss: 0.5293679758906364            
Epoch: #25   | Batch: #5    | Batch Loss: 0.4652926027774811            | Epoch Loss: 0.5165529012680053            
Epoch: #25   | Batch: #6    | Batch Loss: 0.5749210119247437            | Epoch Loss: 0.526280919710795             
Epoch: #25   | Batch: #7    | Batch Loss: 0.6209855675697327            | Epoch Loss: 0.5398101551192147            
Epoch: #25   | Batch: #8    | Batch Loss: 0.7802650928497314            | Epoch Loss: 0.5698670223355293            
Epoch: #25   | Batch: #9    | Batch Loss: 0.5374717116355896            | Epoch Loss: 0.5662675433688693            
Epoch: #25   | Batch: #10   | Batch Loss: 0.4914151728153229            | Epoch Loss: 0.5587823063135147            
Epoch: #25   | Batch: #11   | Batch Loss: 0.5856622457504272            | Epoch Loss: 0.5612259371714159            
Epoch: #25   | Batch: #12   | Batch Loss: 0.5872453451156616            | Epoch Loss: 0.5633942211667696            
Epoch: #25   | Batch: #13   | Batch Loss: 0.4789871573448181            | Epoch Loss: 0.5569013701035426            
Epoch: #25   | Batch: #14   | Batch Loss: 0.5760400891304016            | Epoch Loss: 0.558268421462604             
Epoch: #25   | Batch: #15   | Batch Loss: 0.5634525418281555            | Epoch Loss: 0.558614029486974             
Epoch: #25   | Batch: #16   | Batch Loss: 0.5620995759963989            | Epoch Loss: 0.5588318761438131            
Epoch: #25   | Batch: #17   | Batch Loss: 0.47983112931251526           | Epoch Loss: 0.5541847733890309            
Epoch: #25   | Batch: #18   | Batch Loss: 0.6036694049835205            | Epoch Loss: 0.5569339195887247            
Epoch: #25   | Batch: #19   | Batch Loss: 0.6167657971382141            | Epoch Loss: 0.56008296577554              
Epoch: #25   | Batch: #20   | Batch Loss: 0.5187200307846069            | Epoch Loss: 0.5580148190259934            
Epoch: #25   | Batch: #21   | Batch Loss: 0.41821759939193726           | Epoch Loss: 0.5513578085672288            
Epoch: #25   | Batch: #22   | Batch Loss: 0.7205663323402405            | Epoch Loss: 0.5590491051023657            
Epoch: #25   | Batch: #23   | Batch Loss: 0.5295984745025635            | Epoch Loss: 0.5577686429023743            
Epoch: #25   | Batch: #24   | Batch Loss: 0.4270067512989044            | Epoch Loss: 0.5523202307522297            
Epoch: #25   | Batch: #25   | Batch Loss: 0.6375038623809814            | Epoch Loss: 0.5557275760173798            
Epoch: #25   | Batch: #26   | Batch Loss: 0.4956660568714142            | Epoch Loss: 0.5534175175886887            
Epoch: #25   | Batch: #27   | Batch Loss: 0.5805525183677673            | Epoch Loss: 0.5544225176175436            
Epoch: #25   | Batch: #28   | Batch Loss: 0.4762395918369293            | Epoch Loss: 0.5516302702682359            
Epoch: #25   | Batch: #29   | Batch Loss: 0.5254302620887756            | Epoch Loss: 0.5507268217103235            
Epoch: #25   | Batch: #30   | Batch Loss: 0.4286774694919586            | Epoch Loss: 0.5466585099697113            
Epoch: #25   | Batch: #31   | Batch Loss: 0.5025272965431213            | Epoch Loss: 0.5452349224398213            
Epoch: #25   | Batch: #32   | Batch Loss: 0.5323511958122253            | Epoch Loss: 0.5448323059827089            
Epoch: #25   | Batch: #33   | Batch Loss: 0.5047490000724792            | Epoch Loss: 0.5436176603490656            
Epoch: #25   | Batch: #34   | Batch Loss: 0.4901873469352722            | Epoch Loss: 0.5420461805427775            
Epoch: #25   | Batch: #35   | Batch Loss: 0.6336899399757385            | Epoch Loss: 0.5446645736694335            
Epoch: #25   | Batch: #36   | Batch Loss: 0.6630608439445496            | Epoch Loss: 0.5479533589548535            
Epoch: #25   | Batch: #37   | Batch Loss: 0.48285406827926636           | Epoch Loss: 0.546193918666324             
Epoch: #25   | Batch: #38   | Batch Loss: 0.4884837865829468            | Epoch Loss: 0.5446752309799194            
Epoch: #25   | Batch: #39   | Batch Loss: 0.5120313763618469            | Epoch Loss: 0.5438382090666355            
Epoch: #25   | Batch: #40   | Batch Loss: 0.6122143268585205            | Epoch Loss: 0.5455476120114326            
Epoch: #25   | Batch: #41   | Batch Loss: 0.5094547867774963            | Epoch Loss: 0.5446672992008489            
Epoch: #25   | Batch: #42   | Batch Loss: 0.5275470614433289            | Epoch Loss: 0.5442596744923365            
Epoch: #25   | Batch: #43   | Batch Loss: 0.5898284316062927            | Epoch Loss: 0.5453194130298703            

Classifier Validation Epoch #25
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8181818181818182            
Batch: #2    | Top-1 Accuracy: 0.8272727272727273            
Batch: #3    | Top-1 Accuracy: 0.8303030303030302            
Batch: #4    | Top-1 Accuracy: 0.8397727272727272            
Batch: #5    | Top-1 Accuracy: 0.8363636363636363            
Batch: #6    | Top-1 Accuracy: 0.8356060606060606            
Batch: #7    | Top-1 Accuracy: 0.8318181818181818            
Batch: #8    | Top-1 Accuracy: 0.8340909090909091            
Batch: #9    | Top-1 Accuracy: 0.8393939393939394            
Batch: #10   | Top-1 Accuracy: 0.8413636363636364            
Batch: #11   | Top-1 Accuracy: 0.8359504132231405            
Batch: #12   | Top-1 Accuracy: 0.8356060606060606            
Batch: #13   | Top-1 Accuracy: 0.8363636363636364            
Batch: #14   | Top-1 Accuracy: 0.8344155844155844            
Batch: #15   | Top-1 Accuracy: 0.8354545454545454            
Batch: #16   | Top-1 Accuracy: 0.8352272727272727            
Batch: #17   | Top-1 Accuracy: 0.8358288770053476            

Classifier Training Epoch #26
------------------------------------------
Epoch: #26   | Batch: #1    | Batch Loss: 0.5320727825164795            | Epoch Loss: 0.5320727825164795            
Epoch: #26   | Batch: #2    | Batch Loss: 0.5277392864227295            | Epoch Loss: 0.5299060344696045            
Epoch: #26   | Batch: #3    | Batch Loss: 0.4951297342777252            | Epoch Loss: 0.5183139344056448            
Epoch: #26   | Batch: #4    | Batch Loss: 0.47428515553474426           | Epoch Loss: 0.5073067396879196            
Epoch: #26   | Batch: #5    | Batch Loss: 0.4526844322681427            | Epoch Loss: 0.49638227820396424           
Epoch: #26   | Batch: #6    | Batch Loss: 0.5719886422157288            | Epoch Loss: 0.5089833388725916            
Epoch: #26   | Batch: #7    | Batch Loss: 0.6697208881378174            | Epoch Loss: 0.531945845910481             
Epoch: #26   | Batch: #8    | Batch Loss: 0.6196038126945496            | Epoch Loss: 0.5429030917584896            
Epoch: #26   | Batch: #9    | Batch Loss: 0.4979952871799469            | Epoch Loss: 0.5379133356942071            
Epoch: #26   | Batch: #10   | Batch Loss: 0.5048418641090393            | Epoch Loss: 0.5346061885356903            
Epoch: #26   | Batch: #11   | Batch Loss: 0.5205367803573608            | Epoch Loss: 0.5333271514285695            
Epoch: #26   | Batch: #12   | Batch Loss: 0.5056872367858887            | Epoch Loss: 0.531023825208346             
Epoch: #26   | Batch: #13   | Batch Loss: 0.610200822353363             | Epoch Loss: 0.5371143634502704            
Epoch: #26   | Batch: #14   | Batch Loss: 0.562526524066925             | Epoch Loss: 0.5389295177800315            
Epoch: #26   | Batch: #15   | Batch Loss: 0.5630051493644714            | Epoch Loss: 0.5405345598856608            
Epoch: #26   | Batch: #16   | Batch Loss: 0.46177905797958374           | Epoch Loss: 0.535612341016531             
Epoch: #26   | Batch: #17   | Batch Loss: 0.4597642421722412            | Epoch Loss: 0.5311506881433374            
Epoch: #26   | Batch: #18   | Batch Loss: 0.5026905536651611            | Epoch Loss: 0.5295695695612166            
Epoch: #26   | Batch: #19   | Batch Loss: 0.5945848226547241            | Epoch Loss: 0.5329914249871907            
Epoch: #26   | Batch: #20   | Batch Loss: 0.5603338479995728            | Epoch Loss: 0.5343585461378098            
Epoch: #26   | Batch: #21   | Batch Loss: 0.49527403712272644           | Epoch Loss: 0.5324973790418535            
Epoch: #26   | Batch: #22   | Batch Loss: 0.4541446566581726            | Epoch Loss: 0.528935891660777             
Epoch: #26   | Batch: #23   | Batch Loss: 0.4194504916667938            | Epoch Loss: 0.5241756568784299            
Epoch: #26   | Batch: #24   | Batch Loss: 0.485263854265213             | Epoch Loss: 0.5225543317695459            
Epoch: #26   | Batch: #25   | Batch Loss: 0.5519490838050842            | Epoch Loss: 0.5237301218509675            
Epoch: #26   | Batch: #26   | Batch Loss: 0.5683249235153198            | Epoch Loss: 0.5254453065303656            
Epoch: #26   | Batch: #27   | Batch Loss: 0.5833598971366882            | Epoch Loss: 0.5275902913676368            
Epoch: #26   | Batch: #28   | Batch Loss: 0.6055935025215149            | Epoch Loss: 0.5303761203374181            
Epoch: #26   | Batch: #29   | Batch Loss: 0.4512858986854553            | Epoch Loss: 0.5276488713149367            
Epoch: #26   | Batch: #30   | Batch Loss: 0.3964230418205261            | Epoch Loss: 0.5232746769984563            
Epoch: #26   | Batch: #31   | Batch Loss: 0.6121314167976379            | Epoch Loss: 0.5261410234435913            
Epoch: #26   | Batch: #32   | Batch Loss: 0.6431118249893188            | Epoch Loss: 0.5297963609918952            
Epoch: #26   | Batch: #33   | Batch Loss: 0.5453619956970215            | Epoch Loss: 0.5302680468920505            
Epoch: #26   | Batch: #34   | Batch Loss: 0.5447832942008972            | Epoch Loss: 0.530694965930546             
Epoch: #26   | Batch: #35   | Batch Loss: 0.6044006943702698            | Epoch Loss: 0.5328008438859667            
Epoch: #26   | Batch: #36   | Batch Loss: 0.5860564708709717            | Epoch Loss: 0.5342801668577724            
Epoch: #26   | Batch: #37   | Batch Loss: 0.5004758238792419            | Epoch Loss: 0.5333665359664608            
Epoch: #26   | Batch: #38   | Batch Loss: 0.7195481061935425            | Epoch Loss: 0.5382660509724366            
Epoch: #26   | Batch: #39   | Batch Loss: 0.6221451759338379            | Epoch Loss: 0.5404167977663187            
Epoch: #26   | Batch: #40   | Batch Loss: 0.5094959139823914            | Epoch Loss: 0.5396437756717205            
Epoch: #26   | Batch: #41   | Batch Loss: 0.6748285293579102            | Epoch Loss: 0.5429409647860178            
Epoch: #26   | Batch: #42   | Batch Loss: 0.5617517828941345            | Epoch Loss: 0.5433888414076397            
Epoch: #26   | Batch: #43   | Batch Loss: 0.5699870586395264            | Epoch Loss: 0.5440074045990788            

Classifier Validation Epoch #26
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8136363636363636            
Batch: #2    | Top-1 Accuracy: 0.8090909090909091            
Batch: #3    | Top-1 Accuracy: 0.8287878787878787            
Batch: #4    | Top-1 Accuracy: 0.8352272727272727            
Batch: #5    | Top-1 Accuracy: 0.8336363636363636            
Batch: #6    | Top-1 Accuracy: 0.834090909090909             
Batch: #7    | Top-1 Accuracy: 0.833116883116883             
Batch: #8    | Top-1 Accuracy: 0.8363636363636364            
Batch: #9    | Top-1 Accuracy: 0.8363636363636364            
Batch: #10   | Top-1 Accuracy: 0.8359090909090909            
Batch: #11   | Top-1 Accuracy: 0.8351239669421489            
Batch: #12   | Top-1 Accuracy: 0.8378787878787879            
Batch: #13   | Top-1 Accuracy: 0.8356643356643356            
Batch: #14   | Top-1 Accuracy: 0.8392857142857143            
Batch: #15   | Top-1 Accuracy: 0.8390909090909091            
Batch: #16   | Top-1 Accuracy: 0.8394886363636362            
Batch: #17   | Top-1 Accuracy: 0.8393048128342244            

Classifier Training Epoch #27
------------------------------------------
Epoch: #27   | Batch: #1    | Batch Loss: 0.6010590195655823            | Epoch Loss: 0.6010590195655823            
Epoch: #27   | Batch: #2    | Batch Loss: 0.5969569683074951            | Epoch Loss: 0.5990079939365387            
Epoch: #27   | Batch: #3    | Batch Loss: 0.5418104529380798            | Epoch Loss: 0.5799421469370524            
Epoch: #27   | Batch: #4    | Batch Loss: 0.6607740521430969            | Epoch Loss: 0.6001501232385635            
Epoch: #27   | Batch: #5    | Batch Loss: 0.540337085723877             | Epoch Loss: 0.5881875157356262            
Epoch: #27   | Batch: #6    | Batch Loss: 0.487338125705719             | Epoch Loss: 0.571379284063975             
Epoch: #27   | Batch: #7    | Batch Loss: 0.4344530999660492            | Epoch Loss: 0.5518184006214142            
Epoch: #27   | Batch: #8    | Batch Loss: 0.46804317831993103           | Epoch Loss: 0.5413464978337288            
Epoch: #27   | Batch: #9    | Batch Loss: 0.4949289858341217            | Epoch Loss: 0.5361889965004392            
Epoch: #27   | Batch: #10   | Batch Loss: 0.48991480469703674           | Epoch Loss: 0.5315615773200989            
Epoch: #27   | Batch: #11   | Batch Loss: 0.4878828823566437            | Epoch Loss: 0.5275907868688757            
Epoch: #27   | Batch: #12   | Batch Loss: 0.5049375891685486            | Epoch Loss: 0.5257030203938484            
Epoch: #27   | Batch: #13   | Batch Loss: 0.6453767418861389            | Epoch Loss: 0.5349086912778708            
Epoch: #27   | Batch: #14   | Batch Loss: 0.3877967596054077            | Epoch Loss: 0.5244006961584091            
Epoch: #27   | Batch: #15   | Batch Loss: 0.5350142121315002            | Epoch Loss: 0.5251082638899486            
Epoch: #27   | Batch: #16   | Batch Loss: 0.5737199187278748            | Epoch Loss: 0.5281464923173189            
Epoch: #27   | Batch: #17   | Batch Loss: 0.512537956237793             | Epoch Loss: 0.5272283431361703            
Epoch: #27   | Batch: #18   | Batch Loss: 0.47776076197624207           | Epoch Loss: 0.524480144182841             
Epoch: #27   | Batch: #19   | Batch Loss: 0.5287469029426575            | Epoch Loss: 0.5247047104333576            
Epoch: #27   | Batch: #20   | Batch Loss: 0.544020414352417             | Epoch Loss: 0.5256704956293106            
Epoch: #27   | Batch: #21   | Batch Loss: 0.691389262676239             | Epoch Loss: 0.5335618654886881            
Epoch: #27   | Batch: #22   | Batch Loss: 0.5788655281066895            | Epoch Loss: 0.5356211228804155            
Epoch: #27   | Batch: #23   | Batch Loss: 0.46775904297828674           | Epoch Loss: 0.5326705976672794            
Epoch: #27   | Batch: #24   | Batch Loss: 0.5602598786354065            | Epoch Loss: 0.5338201510409514            
Epoch: #27   | Batch: #25   | Batch Loss: 0.4465565085411072            | Epoch Loss: 0.5303296053409576            
Epoch: #27   | Batch: #26   | Batch Loss: 0.5148106813430786            | Epoch Loss: 0.5297327236487315            
Epoch: #27   | Batch: #27   | Batch Loss: 0.6868904829025269            | Epoch Loss: 0.5355533813988721            
Epoch: #27   | Batch: #28   | Batch Loss: 0.6442782878875732            | Epoch Loss: 0.5394364137734685            
Epoch: #27   | Batch: #29   | Batch Loss: 0.5789165496826172            | Epoch Loss: 0.5407977977703358            
Epoch: #27   | Batch: #30   | Batch Loss: 0.48709210753440857           | Epoch Loss: 0.5390076080958048            
Epoch: #27   | Batch: #31   | Batch Loss: 0.5272805094718933            | Epoch Loss: 0.5386293145918077            
Epoch: #27   | Batch: #32   | Batch Loss: 0.5732105374336243            | Epoch Loss: 0.5397099778056145            
Epoch: #27   | Batch: #33   | Batch Loss: 0.5942036509513855            | Epoch Loss: 0.5413613012342742            
Epoch: #27   | Batch: #34   | Batch Loss: 0.6683329939842224            | Epoch Loss: 0.5450957627857432            
Epoch: #27   | Batch: #35   | Batch Loss: 0.4331938326358795            | Epoch Loss: 0.5418985647814615            
Epoch: #27   | Batch: #36   | Batch Loss: 0.4926709830760956            | Epoch Loss: 0.5405311319563124            
Epoch: #27   | Batch: #37   | Batch Loss: 0.5769835114479065            | Epoch Loss: 0.5415163314020311            
Epoch: #27   | Batch: #38   | Batch Loss: 0.49666300415992737           | Epoch Loss: 0.5403359806851337            
Epoch: #27   | Batch: #39   | Batch Loss: 0.5636767745018005            | Epoch Loss: 0.5409344625778687            
Epoch: #27   | Batch: #40   | Batch Loss: 0.5114342570304871            | Epoch Loss: 0.5401969574391842            
Epoch: #27   | Batch: #41   | Batch Loss: 0.5214423537254333            | Epoch Loss: 0.5397395280803122            
Epoch: #27   | Batch: #42   | Batch Loss: 0.5371865630149841            | Epoch Loss: 0.5396787431978044            
Epoch: #27   | Batch: #43   | Batch Loss: 0.6957288384437561            | Epoch Loss: 0.5433078151802684            

Classifier Validation Epoch #27
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8681818181818182            
Batch: #2    | Top-1 Accuracy: 0.8704545454545454            
Batch: #3    | Top-1 Accuracy: 0.8651515151515151            
Batch: #4    | Top-1 Accuracy: 0.8545454545454545            
Batch: #5    | Top-1 Accuracy: 0.8545454545454545            
Batch: #6    | Top-1 Accuracy: 0.8484848484848485            
Batch: #7    | Top-1 Accuracy: 0.8441558441558442            
Batch: #8    | Top-1 Accuracy: 0.8477272727272727            
Batch: #9    | Top-1 Accuracy: 0.8469696969696969            
Batch: #10   | Top-1 Accuracy: 0.8459090909090909            
Batch: #11   | Top-1 Accuracy: 0.8425619834710744            
Batch: #12   | Top-1 Accuracy: 0.8416666666666667            
Batch: #13   | Top-1 Accuracy: 0.8426573426573426            
Batch: #14   | Top-1 Accuracy: 0.8457792207792207            
Batch: #15   | Top-1 Accuracy: 0.8457575757575757            
Batch: #16   | Top-1 Accuracy: 0.84375                       
Batch: #17   | Top-1 Accuracy: 0.8441176470588235            

Classifier Training Epoch #28
------------------------------------------
Epoch: #28   | Batch: #1    | Batch Loss: 0.6088650226593018            | Epoch Loss: 0.6088650226593018            
Epoch: #28   | Batch: #2    | Batch Loss: 0.45886483788490295           | Epoch Loss: 0.5338649302721024            
Epoch: #28   | Batch: #3    | Batch Loss: 0.4220511019229889            | Epoch Loss: 0.4965936541557312            
Epoch: #28   | Batch: #4    | Batch Loss: 0.6311386227607727            | Epoch Loss: 0.5302298963069916            
Epoch: #28   | Batch: #5    | Batch Loss: 0.5048521757125854            | Epoch Loss: 0.5251543521881104            
Epoch: #28   | Batch: #6    | Batch Loss: 0.5577324032783508            | Epoch Loss: 0.5305840273698171            
Epoch: #28   | Batch: #7    | Batch Loss: 0.5813145637512207            | Epoch Loss: 0.5378312468528748            
Epoch: #28   | Batch: #8    | Batch Loss: 0.5803788304328918            | Epoch Loss: 0.5431496948003769            
Epoch: #28   | Batch: #9    | Batch Loss: 0.4605056345462799            | Epoch Loss: 0.5339670214388106            
Epoch: #28   | Batch: #10   | Batch Loss: 0.5404481887817383            | Epoch Loss: 0.5346151381731034            
Epoch: #28   | Batch: #11   | Batch Loss: 0.5548239350318909            | Epoch Loss: 0.5364523015239022            
Epoch: #28   | Batch: #12   | Batch Loss: 0.5616635680198669            | Epoch Loss: 0.5385532403985659            
Epoch: #28   | Batch: #13   | Batch Loss: 0.634310781955719             | Epoch Loss: 0.5459192051337316            
Epoch: #28   | Batch: #14   | Batch Loss: 0.6095114350318909            | Epoch Loss: 0.5504615072693143            
Epoch: #28   | Batch: #15   | Batch Loss: 0.5981974601745605            | Epoch Loss: 0.5536439041296641            
Epoch: #28   | Batch: #16   | Batch Loss: 0.4143630862236023            | Epoch Loss: 0.5449388530105352            
Epoch: #28   | Batch: #17   | Batch Loss: 0.6150716543197632            | Epoch Loss: 0.5490643119110781            
Epoch: #28   | Batch: #18   | Batch Loss: 0.5942485332489014            | Epoch Loss: 0.551574546429846             
Epoch: #28   | Batch: #19   | Batch Loss: 0.4440952241420746            | Epoch Loss: 0.5459177399936476            
Epoch: #28   | Batch: #20   | Batch Loss: 0.5465457439422607            | Epoch Loss: 0.5459491401910782            
Epoch: #28   | Batch: #21   | Batch Loss: 0.4906955063343048            | Epoch Loss: 0.5433180147693271            
Epoch: #28   | Batch: #22   | Batch Loss: 0.45271241664886475           | Epoch Loss: 0.5391995784911242            
Epoch: #28   | Batch: #23   | Batch Loss: 0.6508538126945496            | Epoch Loss: 0.5440541104130123            
Epoch: #28   | Batch: #24   | Batch Loss: 0.5439833402633667            | Epoch Loss: 0.544051161656777             
Epoch: #28   | Batch: #25   | Batch Loss: 0.5931854248046875            | Epoch Loss: 0.5460165321826935            
Epoch: #28   | Batch: #26   | Batch Loss: 0.39937639236450195           | Epoch Loss: 0.5403765268050708            
Epoch: #28   | Batch: #27   | Batch Loss: 0.5120055675506592            | Epoch Loss: 0.5393257505363889            
Epoch: #28   | Batch: #28   | Batch Loss: 0.6084591746330261            | Epoch Loss: 0.541794801396983             
Epoch: #28   | Batch: #29   | Batch Loss: 0.6201879978179932            | Epoch Loss: 0.5444980150666731            
Epoch: #28   | Batch: #30   | Batch Loss: 0.41751205921173096           | Epoch Loss: 0.5402651498715083            
Epoch: #28   | Batch: #31   | Batch Loss: 0.6284577250480652            | Epoch Loss: 0.5431100716513972            
Epoch: #28   | Batch: #32   | Batch Loss: 0.5542142987251282            | Epoch Loss: 0.5434570787474513            
Epoch: #28   | Batch: #33   | Batch Loss: 0.5831661820411682            | Epoch Loss: 0.544660384907867             
Epoch: #28   | Batch: #34   | Batch Loss: 0.7213020920753479            | Epoch Loss: 0.5498557292363223            
Epoch: #28   | Batch: #35   | Batch Loss: 0.5125377774238586            | Epoch Loss: 0.5487895020416804            
Epoch: #28   | Batch: #36   | Batch Loss: 0.5588428378105164            | Epoch Loss: 0.5490687613685926            
Epoch: #28   | Batch: #37   | Batch Loss: 0.40678441524505615           | Epoch Loss: 0.5452232385003889            
Epoch: #28   | Batch: #38   | Batch Loss: 0.5358285903930664            | Epoch Loss: 0.5449760109186172            
Epoch: #28   | Batch: #39   | Batch Loss: 0.5995525121688843            | Epoch Loss: 0.5463754083865728            
Epoch: #28   | Batch: #40   | Batch Loss: 0.5955750346183777            | Epoch Loss: 0.5476053990423679            
Epoch: #28   | Batch: #41   | Batch Loss: 0.48953643441200256           | Epoch Loss: 0.5461890828318712            
Epoch: #28   | Batch: #42   | Batch Loss: 0.3765426278114319            | Epoch Loss: 0.5421498815218607            
Epoch: #28   | Batch: #43   | Batch Loss: 0.6552502512931824            | Epoch Loss: 0.5447801226793334            

Classifier Validation Epoch #28
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8545454545454545            
Batch: #2    | Top-1 Accuracy: 0.8477272727272727            
Batch: #3    | Top-1 Accuracy: 0.8378787878787879            
Batch: #4    | Top-1 Accuracy: 0.8363636363636364            
Batch: #5    | Top-1 Accuracy: 0.8363636363636363            
Batch: #6    | Top-1 Accuracy: 0.8393939393939394            
Batch: #7    | Top-1 Accuracy: 0.8370129870129871            
Batch: #8    | Top-1 Accuracy: 0.8374999999999999            
Batch: #9    | Top-1 Accuracy: 0.8378787878787878            
Batch: #10   | Top-1 Accuracy: 0.8395454545454545            
Batch: #11   | Top-1 Accuracy: 0.8429752066115701            
Batch: #12   | Top-1 Accuracy: 0.8424242424242423            
Batch: #13   | Top-1 Accuracy: 0.8440559440559439            
Batch: #14   | Top-1 Accuracy: 0.8448051948051946            
Batch: #15   | Top-1 Accuracy: 0.8433333333333332            
Batch: #16   | Top-1 Accuracy: 0.8448863636363636            
Batch: #17   | Top-1 Accuracy: 0.8427807486631016            

Classifier Training Epoch #29
------------------------------------------
Epoch: #29   | Batch: #1    | Batch Loss: 0.5627396106719971            | Epoch Loss: 0.5627396106719971            
Epoch: #29   | Batch: #2    | Batch Loss: 0.5178447365760803            | Epoch Loss: 0.5402921736240387            
Epoch: #29   | Batch: #3    | Batch Loss: 0.5763978362083435            | Epoch Loss: 0.5523273944854736            
Epoch: #29   | Batch: #4    | Batch Loss: 0.5333108901977539            | Epoch Loss: 0.5475732684135437            
Epoch: #29   | Batch: #5    | Batch Loss: 0.5558665990829468            | Epoch Loss: 0.5492319345474244            
Epoch: #29   | Batch: #6    | Batch Loss: 0.4332566261291504            | Epoch Loss: 0.529902716477712             
Epoch: #29   | Batch: #7    | Batch Loss: 0.6709009408950806            | Epoch Loss: 0.5500453199659076            
Epoch: #29   | Batch: #8    | Batch Loss: 0.6744663715362549            | Epoch Loss: 0.5655979514122009            
Epoch: #29   | Batch: #9    | Batch Loss: 0.5267434120178223            | Epoch Loss: 0.561280780368381             
Epoch: #29   | Batch: #10   | Batch Loss: 0.48476457595825195           | Epoch Loss: 0.5536291599273682            
Epoch: #29   | Batch: #11   | Batch Loss: 0.5082411766052246            | Epoch Loss: 0.5495029796253551            
Epoch: #29   | Batch: #12   | Batch Loss: 0.5106226205825806            | Epoch Loss: 0.5462629497051239            
Epoch: #29   | Batch: #13   | Batch Loss: 0.5541950464248657            | Epoch Loss: 0.5468731109912579            
Epoch: #29   | Batch: #14   | Batch Loss: 0.5420783758163452            | Epoch Loss: 0.5465306299073356            
Epoch: #29   | Batch: #15   | Batch Loss: 0.43465524911880493           | Epoch Loss: 0.5390722711881002            
Epoch: #29   | Batch: #16   | Batch Loss: 0.5983678698539734            | Epoch Loss: 0.5427782461047173            
Epoch: #29   | Batch: #17   | Batch Loss: 0.40593039989471436           | Epoch Loss: 0.5347283727982465            
Epoch: #29   | Batch: #18   | Batch Loss: 0.5238301157951355            | Epoch Loss: 0.5341229140758514            
Epoch: #29   | Batch: #19   | Batch Loss: 0.5992065668106079            | Epoch Loss: 0.5375483694829439            
Epoch: #29   | Batch: #20   | Batch Loss: 0.4565695822238922            | Epoch Loss: 0.5334994301199913            
Epoch: #29   | Batch: #21   | Batch Loss: 0.4624987244606018            | Epoch Loss: 0.5301184441362109            
Epoch: #29   | Batch: #22   | Batch Loss: 0.5283042788505554            | Epoch Loss: 0.5300359820777719            
Epoch: #29   | Batch: #23   | Batch Loss: 0.5637264847755432            | Epoch Loss: 0.5315007865428925            
Epoch: #29   | Batch: #24   | Batch Loss: 0.432195246219635             | Epoch Loss: 0.5273630556960901            
Epoch: #29   | Batch: #25   | Batch Loss: 0.6291799545288086            | Epoch Loss: 0.5314357316493988            
Epoch: #29   | Batch: #26   | Batch Loss: 0.4421965777873993            | Epoch Loss: 0.5280034565008603            
Epoch: #29   | Batch: #27   | Batch Loss: 0.5933092832565308            | Epoch Loss: 0.5304221908251444            
Epoch: #29   | Batch: #28   | Batch Loss: 0.6180898547172546            | Epoch Loss: 0.5335531788212913            
Epoch: #29   | Batch: #29   | Batch Loss: 0.49428215622901917           | Epoch Loss: 0.5321990056284542            
Epoch: #29   | Batch: #30   | Batch Loss: 0.6567661166191101            | Epoch Loss: 0.5363512426614762            
Epoch: #29   | Batch: #31   | Batch Loss: 0.7328608632087708            | Epoch Loss: 0.5426902626791308            
Epoch: #29   | Batch: #32   | Batch Loss: 0.6021023392677307            | Epoch Loss: 0.5445468900725245            
Epoch: #29   | Batch: #33   | Batch Loss: 0.45763617753982544           | Epoch Loss: 0.5419132321169882            
Epoch: #29   | Batch: #34   | Batch Loss: 0.5411515235900879            | Epoch Loss: 0.5418908289250206            
Epoch: #29   | Batch: #35   | Batch Loss: 0.5250896215438843            | Epoch Loss: 0.5414107944284167            
Epoch: #29   | Batch: #36   | Batch Loss: 0.5345948934555054            | Epoch Loss: 0.5412214638458358            
Epoch: #29   | Batch: #37   | Batch Loss: 0.5578160881996155            | Epoch Loss: 0.5416699672067488            
Epoch: #29   | Batch: #38   | Batch Loss: 0.5132045149803162            | Epoch Loss: 0.5409208763586847            
Epoch: #29   | Batch: #39   | Batch Loss: 0.5987229943275452            | Epoch Loss: 0.5424029819476299            
Epoch: #29   | Batch: #40   | Batch Loss: 0.5428996086120605            | Epoch Loss: 0.5424153976142406            
Epoch: #29   | Batch: #41   | Batch Loss: 0.5335310697555542            | Epoch Loss: 0.542198706690858             
Epoch: #29   | Batch: #42   | Batch Loss: 0.5448664426803589            | Epoch Loss: 0.5422622242144176            
Epoch: #29   | Batch: #43   | Batch Loss: 0.715126633644104             | Epoch Loss: 0.546282326759294             

Classifier Validation Epoch #29
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8363636363636363            
Batch: #2    | Top-1 Accuracy: 0.834090909090909             
Batch: #3    | Top-1 Accuracy: 0.8318181818181817            
Batch: #4    | Top-1 Accuracy: 0.8329545454545454            
Batch: #5    | Top-1 Accuracy: 0.8281818181818181            
Batch: #6    | Top-1 Accuracy: 0.8303030303030302            
Batch: #7    | Top-1 Accuracy: 0.8389610389610389            
Batch: #8    | Top-1 Accuracy: 0.8403409090909091            
Batch: #9    | Top-1 Accuracy: 0.8414141414141414            
Batch: #10   | Top-1 Accuracy: 0.8404545454545455            
Batch: #11   | Top-1 Accuracy: 0.8363636363636363            
Batch: #12   | Top-1 Accuracy: 0.8386363636363635            
Batch: #13   | Top-1 Accuracy: 0.8381118881118881            
Batch: #14   | Top-1 Accuracy: 0.8383116883116883            
Batch: #15   | Top-1 Accuracy: 0.8387878787878787            
Batch: #16   | Top-1 Accuracy: 0.8383522727272726            
Batch: #17   | Top-1 Accuracy: 0.8390374331550801            

Classifier Training Epoch #30
------------------------------------------
Epoch: #30   | Batch: #1    | Batch Loss: 0.6376971006393433            | Epoch Loss: 0.6376971006393433            
Epoch: #30   | Batch: #2    | Batch Loss: 0.4558013081550598            | Epoch Loss: 0.5467492043972015            
Epoch: #30   | Batch: #3    | Batch Loss: 0.4814857244491577            | Epoch Loss: 0.5249947110811869            
Epoch: #30   | Batch: #4    | Batch Loss: 0.47179076075553894           | Epoch Loss: 0.5116937234997749            
Epoch: #30   | Batch: #5    | Batch Loss: 0.5846329927444458            | Epoch Loss: 0.5262815773487091            
Epoch: #30   | Batch: #6    | Batch Loss: 0.49471333622932434           | Epoch Loss: 0.5210202038288116            
Epoch: #30   | Batch: #7    | Batch Loss: 0.6156975030899048            | Epoch Loss: 0.534545532294682             
Epoch: #30   | Batch: #8    | Batch Loss: 0.6112170815467834            | Epoch Loss: 0.5441294759511948            
Epoch: #30   | Batch: #9    | Batch Loss: 0.5154924988746643            | Epoch Loss: 0.5409475896093581            
Epoch: #30   | Batch: #10   | Batch Loss: 0.6444635987281799            | Epoch Loss: 0.5512991905212402            
Epoch: #30   | Batch: #11   | Batch Loss: 0.4186437427997589            | Epoch Loss: 0.5392396043647419            
Epoch: #30   | Batch: #12   | Batch Loss: 0.5599238872528076            | Epoch Loss: 0.540963294605414             
Epoch: #30   | Batch: #13   | Batch Loss: 0.5159772634506226            | Epoch Loss: 0.5390412922088916            
Epoch: #30   | Batch: #14   | Batch Loss: 0.7159827947616577            | Epoch Loss: 0.5516799709626606            
Epoch: #30   | Batch: #15   | Batch Loss: 0.6106764078140259            | Epoch Loss: 0.5556130667527517            
Epoch: #30   | Batch: #16   | Batch Loss: 0.5160248875617981            | Epoch Loss: 0.5531388055533171            
Epoch: #30   | Batch: #17   | Batch Loss: 0.6035351753234863            | Epoch Loss: 0.5561032978927388            
Epoch: #30   | Batch: #18   | Batch Loss: 0.5064154863357544            | Epoch Loss: 0.5533428639173508            
Epoch: #30   | Batch: #19   | Batch Loss: 0.5599626302719116            | Epoch Loss: 0.553691272672854             
Epoch: #30   | Batch: #20   | Batch Loss: 0.5130369663238525            | Epoch Loss: 0.5516585573554039            
Epoch: #30   | Batch: #21   | Batch Loss: 0.462145060300827             | Epoch Loss: 0.5473960098766145            
Epoch: #30   | Batch: #22   | Batch Loss: 0.5478963255882263            | Epoch Loss: 0.5474187514998696            
Epoch: #30   | Batch: #23   | Batch Loss: 0.5568027496337891            | Epoch Loss: 0.5478267514187357            
Epoch: #30   | Batch: #24   | Batch Loss: 0.5446709990501404            | Epoch Loss: 0.5476952617367109            
Epoch: #30   | Batch: #25   | Batch Loss: 0.5936341285705566            | Epoch Loss: 0.5495328164100647            
Epoch: #30   | Batch: #26   | Batch Loss: 0.5545834302902222            | Epoch Loss: 0.5497270707900708            
Epoch: #30   | Batch: #27   | Batch Loss: 0.5366160273551941            | Epoch Loss: 0.549241476588779             
Epoch: #30   | Batch: #28   | Batch Loss: 0.6278440952301025            | Epoch Loss: 0.5520487129688263            
Epoch: #30   | Batch: #29   | Batch Loss: 0.4366835057735443            | Epoch Loss: 0.5480706023758856            
Epoch: #30   | Batch: #30   | Batch Loss: 0.503017246723175             | Epoch Loss: 0.5465688238541285            
Epoch: #30   | Batch: #31   | Batch Loss: 0.590757429599762             | Epoch Loss: 0.547994262749149             
Epoch: #30   | Batch: #32   | Batch Loss: 0.5349401831626892            | Epoch Loss: 0.5475863227620721            
Epoch: #30   | Batch: #33   | Batch Loss: 0.47275030612945557           | Epoch Loss: 0.5453185646822958            
Epoch: #30   | Batch: #34   | Batch Loss: 0.5364044308662415            | Epoch Loss: 0.5450563842759413            
Epoch: #30   | Batch: #35   | Batch Loss: 0.4494575262069702            | Epoch Loss: 0.5423249883311135            
Epoch: #30   | Batch: #36   | Batch Loss: 0.45954611897468567           | Epoch Loss: 0.540025575293435             
Epoch: #30   | Batch: #37   | Batch Loss: 0.5366420745849609            | Epoch Loss: 0.5399341293283411            
Epoch: #30   | Batch: #38   | Batch Loss: 0.581610918045044             | Epoch Loss: 0.5410308869261491            
Epoch: #30   | Batch: #39   | Batch Loss: 0.6411287188529968            | Epoch Loss: 0.5435974980011965            
Epoch: #30   | Batch: #40   | Batch Loss: 0.45614463090896606           | Epoch Loss: 0.5414111763238907            
Epoch: #30   | Batch: #41   | Batch Loss: 0.5290559530258179            | Epoch Loss: 0.5411098294141816            
Epoch: #30   | Batch: #42   | Batch Loss: 0.48150360584259033           | Epoch Loss: 0.539690633614858             
Epoch: #30   | Batch: #43   | Batch Loss: 0.580025315284729             | Epoch Loss: 0.5406286494676457            

Classifier Validation Epoch #30
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8136363636363636            
Batch: #2    | Top-1 Accuracy: 0.8204545454545454            
Batch: #3    | Top-1 Accuracy: 0.8363636363636363            
Batch: #4    | Top-1 Accuracy: 0.8386363636363636            
Batch: #5    | Top-1 Accuracy: 0.8354545454545456            
Batch: #6    | Top-1 Accuracy: 0.8318181818181819            
Batch: #7    | Top-1 Accuracy: 0.8318181818181819            
Batch: #8    | Top-1 Accuracy: 0.8323863636363635            
Batch: #9    | Top-1 Accuracy: 0.8348484848484847            
Batch: #10   | Top-1 Accuracy: 0.8327272727272726            
Batch: #11   | Top-1 Accuracy: 0.8351239669421487            
Batch: #12   | Top-1 Accuracy: 0.8352272727272726            
Batch: #13   | Top-1 Accuracy: 0.8363636363636362            
Batch: #14   | Top-1 Accuracy: 0.8392857142857142            
Batch: #15   | Top-1 Accuracy: 0.8427272727272725            
Batch: #16   | Top-1 Accuracy: 0.8417613636363637            
Batch: #17   | Top-1 Accuracy: 0.8425133689839573            

Classifier Training Epoch #31
------------------------------------------
Epoch: #31   | Batch: #1    | Batch Loss: 0.5778449773788452            | Epoch Loss: 0.5778449773788452            
Epoch: #31   | Batch: #2    | Batch Loss: 0.45217767357826233           | Epoch Loss: 0.5150113254785538            
Epoch: #31   | Batch: #3    | Batch Loss: 0.4002542197704315            | Epoch Loss: 0.4767589569091797            
Epoch: #31   | Batch: #4    | Batch Loss: 0.4973403513431549            | Epoch Loss: 0.4819043055176735            
Epoch: #31   | Batch: #5    | Batch Loss: 0.4337092936038971            | Epoch Loss: 0.4722653031349182            
Epoch: #31   | Batch: #6    | Batch Loss: 0.48092183470726013           | Epoch Loss: 0.4737080583969752            
Epoch: #31   | Batch: #7    | Batch Loss: 0.43510138988494873           | Epoch Loss: 0.4681928200381143            
Epoch: #31   | Batch: #8    | Batch Loss: 0.5634815096855164            | Epoch Loss: 0.48010390624403954           
Epoch: #31   | Batch: #9    | Batch Loss: 0.4718960225582123            | Epoch Loss: 0.4791919191678365            
Epoch: #31   | Batch: #10   | Batch Loss: 0.585578203201294             | Epoch Loss: 0.4898305475711823            
Epoch: #31   | Batch: #11   | Batch Loss: 0.5404388308525085            | Epoch Loss: 0.4944313005967574            
Epoch: #31   | Batch: #12   | Batch Loss: 0.6363744139671326            | Epoch Loss: 0.506259893377622             
Epoch: #31   | Batch: #13   | Batch Loss: 0.6213469505310059            | Epoch Loss: 0.5151127439278823            
Epoch: #31   | Batch: #14   | Batch Loss: 0.5422589182853699            | Epoch Loss: 0.5170517563819885            
Epoch: #31   | Batch: #15   | Batch Loss: 0.5260046124458313            | Epoch Loss: 0.5176486134529114            
Epoch: #31   | Batch: #16   | Batch Loss: 0.4939916729927063            | Epoch Loss: 0.5161700546741486            
Epoch: #31   | Batch: #17   | Batch Loss: 0.502323567867279             | Epoch Loss: 0.515355555450215             
Epoch: #31   | Batch: #18   | Batch Loss: 0.7244197726249695            | Epoch Loss: 0.5269702341821458            
Epoch: #31   | Batch: #19   | Batch Loss: 0.5770313143730164            | Epoch Loss: 0.5296050278764022            
Epoch: #31   | Batch: #20   | Batch Loss: 0.6037630438804626            | Epoch Loss: 0.5333129286766052            
Epoch: #31   | Batch: #21   | Batch Loss: 0.4810224771499634            | Epoch Loss: 0.5308229071753365            
Epoch: #31   | Batch: #22   | Batch Loss: 0.45958155393600464           | Epoch Loss: 0.527584663846276             
Epoch: #31   | Batch: #23   | Batch Loss: 0.511142373085022             | Epoch Loss: 0.526869781639265             
Epoch: #31   | Batch: #24   | Batch Loss: 0.47886717319488525           | Epoch Loss: 0.5248696729540825            
Epoch: #31   | Batch: #25   | Batch Loss: 0.5753423571586609            | Epoch Loss: 0.5268885803222656            
Epoch: #31   | Batch: #26   | Batch Loss: 0.5325027704238892            | Epoch Loss: 0.5271045107107896            
Epoch: #31   | Batch: #27   | Batch Loss: 0.6124929785728455            | Epoch Loss: 0.5302670465575324            
Epoch: #31   | Batch: #28   | Batch Loss: 0.5988350510597229            | Epoch Loss: 0.5327159038611821            
Epoch: #31   | Batch: #29   | Batch Loss: 0.5691927671432495            | Epoch Loss: 0.5339737267329775            
Epoch: #31   | Batch: #30   | Batch Loss: 0.5294424295425415            | Epoch Loss: 0.5338226834932963            
Epoch: #31   | Batch: #31   | Batch Loss: 0.5271637439727783            | Epoch Loss: 0.5336078789926344            
Epoch: #31   | Batch: #32   | Batch Loss: 0.6408283710479736            | Epoch Loss: 0.5369585193693638            
Epoch: #31   | Batch: #33   | Batch Loss: 0.5254107117652893            | Epoch Loss: 0.536608585805604             
Epoch: #31   | Batch: #34   | Batch Loss: 0.5296323299407959            | Epoch Loss: 0.5364034018095802            
Epoch: #31   | Batch: #35   | Batch Loss: 0.5535901188850403            | Epoch Loss: 0.536894450868879             
Epoch: #31   | Batch: #36   | Batch Loss: 0.6416928172111511            | Epoch Loss: 0.5398055166006088            
Epoch: #31   | Batch: #37   | Batch Loss: 0.5691073536872864            | Epoch Loss: 0.540597458143492             
Epoch: #31   | Batch: #38   | Batch Loss: 0.425353080034256             | Epoch Loss: 0.5375647113511437            
Epoch: #31   | Batch: #39   | Batch Loss: 0.48757031559944153           | Epoch Loss: 0.5362828037677667            
Epoch: #31   | Batch: #40   | Batch Loss: 0.5224899649620056            | Epoch Loss: 0.5359379827976227            
Epoch: #31   | Batch: #41   | Batch Loss: 0.507400631904602             | Epoch Loss: 0.5352419498490124            
Epoch: #31   | Batch: #42   | Batch Loss: 0.6503112316131592            | Epoch Loss: 0.5379816946529207            
Epoch: #31   | Batch: #43   | Batch Loss: 0.6341928243637085            | Epoch Loss: 0.5402191627857297            

Classifier Validation Epoch #31
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8590909090909091            
Batch: #2    | Top-1 Accuracy: 0.8590909090909091            
Batch: #3    | Top-1 Accuracy: 0.8651515151515152            
Batch: #4    | Top-1 Accuracy: 0.8579545454545455            
Batch: #5    | Top-1 Accuracy: 0.860909090909091             
Batch: #6    | Top-1 Accuracy: 0.8606060606060607            
Batch: #7    | Top-1 Accuracy: 0.8636363636363636            
Batch: #8    | Top-1 Accuracy: 0.8596590909090909            
Batch: #9    | Top-1 Accuracy: 0.8555555555555555            
Batch: #10   | Top-1 Accuracy: 0.8522727272727272            
Batch: #11   | Top-1 Accuracy: 0.8512396694214875            
Batch: #12   | Top-1 Accuracy: 0.8511363636363636            
Batch: #13   | Top-1 Accuracy: 0.8503496503496504            
Batch: #14   | Top-1 Accuracy: 0.8525974025974027            
Batch: #15   | Top-1 Accuracy: 0.8503030303030303            
Batch: #16   | Top-1 Accuracy: 0.8508522727272727            
Batch: #17   | Top-1 Accuracy: 0.8483957219251337            

Classifier Training Epoch #32
------------------------------------------
Epoch: #32   | Batch: #1    | Batch Loss: 0.5897229909896851            | Epoch Loss: 0.5897229909896851            
Epoch: #32   | Batch: #2    | Batch Loss: 0.5521523952484131            | Epoch Loss: 0.5709376931190491            
Epoch: #32   | Batch: #3    | Batch Loss: 0.6529799103736877            | Epoch Loss: 0.5982850988705953            
Epoch: #32   | Batch: #4    | Batch Loss: 0.6505960822105408            | Epoch Loss: 0.6113628447055817            
Epoch: #32   | Batch: #5    | Batch Loss: 0.4292392134666443            | Epoch Loss: 0.5749381184577942            
Epoch: #32   | Batch: #6    | Batch Loss: 0.4057908058166504            | Epoch Loss: 0.5467468996842703            
Epoch: #32   | Batch: #7    | Batch Loss: 0.5184382796287537            | Epoch Loss: 0.5427028111049107            
Epoch: #32   | Batch: #8    | Batch Loss: 0.4348410665988922            | Epoch Loss: 0.5292200930416584            
Epoch: #32   | Batch: #9    | Batch Loss: 0.4638241231441498            | Epoch Loss: 0.5219538741641574            
Epoch: #32   | Batch: #10   | Batch Loss: 0.5639418959617615            | Epoch Loss: 0.5261526763439178            
Epoch: #32   | Batch: #11   | Batch Loss: 0.5486502647399902            | Epoch Loss: 0.5281979116526517            
Epoch: #32   | Batch: #12   | Batch Loss: 0.5098046660423279            | Epoch Loss: 0.5266651411851248            
Epoch: #32   | Batch: #13   | Batch Loss: 0.5582126379013062            | Epoch Loss: 0.5290918717017541            
Epoch: #32   | Batch: #14   | Batch Loss: 0.597627580165863             | Epoch Loss: 0.5339872794491904            
Epoch: #32   | Batch: #15   | Batch Loss: 0.4490353763103485            | Epoch Loss: 0.5283238192399343            
Epoch: #32   | Batch: #16   | Batch Loss: 0.5860264897346497            | Epoch Loss: 0.531930236145854             
Epoch: #32   | Batch: #17   | Batch Loss: 0.5963943600654602            | Epoch Loss: 0.5357222434352426            
Epoch: #32   | Batch: #18   | Batch Loss: 0.46024972200393677           | Epoch Loss: 0.5315293255779479            
Epoch: #32   | Batch: #19   | Batch Loss: 0.5264167189598083            | Epoch Loss: 0.5312602410190984            
Epoch: #32   | Batch: #20   | Batch Loss: 0.4741212725639343            | Epoch Loss: 0.5284032925963402            
Epoch: #32   | Batch: #21   | Batch Loss: 0.6336657404899597            | Epoch Loss: 0.533415790115084             
Epoch: #32   | Batch: #22   | Batch Loss: 0.4335186779499054            | Epoch Loss: 0.528875012289394             
Epoch: #32   | Batch: #23   | Batch Loss: 0.5539970397949219            | Epoch Loss: 0.5299672743548518            
Epoch: #32   | Batch: #24   | Batch Loss: 0.6067862510681152            | Epoch Loss: 0.5331680650512377            
Epoch: #32   | Batch: #25   | Batch Loss: 0.511387050151825             | Epoch Loss: 0.5322968244552613            
Epoch: #32   | Batch: #26   | Batch Loss: 0.5975024700164795            | Epoch Loss: 0.5348047338999234            
Epoch: #32   | Batch: #27   | Batch Loss: 0.6243259906768799            | Epoch Loss: 0.5381203360027738            
Epoch: #32   | Batch: #28   | Batch Loss: 0.5525388121604919            | Epoch Loss: 0.5386352815798351            
Epoch: #32   | Batch: #29   | Batch Loss: 0.4944504499435425            | Epoch Loss: 0.537111666695825             
Epoch: #32   | Batch: #30   | Batch Loss: 0.5388659834861755            | Epoch Loss: 0.53717014392217              
Epoch: #32   | Batch: #31   | Batch Loss: 0.6129379868507385            | Epoch Loss: 0.5396142678876077            
Epoch: #32   | Batch: #32   | Batch Loss: 0.6633503437042236            | Epoch Loss: 0.543481020256877             
Epoch: #32   | Batch: #33   | Batch Loss: 0.5487205982208252            | Epoch Loss: 0.5436397953466936            
Epoch: #32   | Batch: #34   | Batch Loss: 0.5120412707328796            | Epoch Loss: 0.542710426975699             
Epoch: #32   | Batch: #35   | Batch Loss: 0.6182221174240112            | Epoch Loss: 0.5448679038456508            
Epoch: #32   | Batch: #36   | Batch Loss: 0.4284251034259796            | Epoch Loss: 0.5416333816117711            
Epoch: #32   | Batch: #37   | Batch Loss: 0.5914232134819031            | Epoch Loss: 0.5429790527433962            
Epoch: #32   | Batch: #38   | Batch Loss: 0.5506579875946045            | Epoch Loss: 0.5431811299763227            
Epoch: #32   | Batch: #39   | Batch Loss: 0.5632687211036682            | Epoch Loss: 0.5436961964154855            
Epoch: #32   | Batch: #40   | Batch Loss: 0.575588047504425             | Epoch Loss: 0.544493492692709             
Epoch: #32   | Batch: #41   | Batch Loss: 0.5982014536857605            | Epoch Loss: 0.5458034429608322            
Epoch: #32   | Batch: #42   | Batch Loss: 0.5304520130157471            | Epoch Loss: 0.5454379327240444            
Epoch: #32   | Batch: #43   | Batch Loss: 0.5297381281852722            | Epoch Loss: 0.5450728209905846            

Classifier Validation Epoch #32
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8681818181818182            
Batch: #2    | Top-1 Accuracy: 0.8431818181818183            
Batch: #3    | Top-1 Accuracy: 0.8378787878787879            
Batch: #4    | Top-1 Accuracy: 0.8409090909090909            
Batch: #5    | Top-1 Accuracy: 0.8400000000000001            
Batch: #6    | Top-1 Accuracy: 0.8439393939393939            
Batch: #7    | Top-1 Accuracy: 0.8409090909090909            
Batch: #8    | Top-1 Accuracy: 0.8409090909090908            
Batch: #9    | Top-1 Accuracy: 0.8388888888888888            
Batch: #10   | Top-1 Accuracy: 0.8381818181818181            
Batch: #11   | Top-1 Accuracy: 0.8363636363636363            
Batch: #12   | Top-1 Accuracy: 0.8356060606060606            
Batch: #13   | Top-1 Accuracy: 0.8377622377622378            
Batch: #14   | Top-1 Accuracy: 0.8373376623376624            
Batch: #15   | Top-1 Accuracy: 0.8366666666666667            
Batch: #16   | Top-1 Accuracy: 0.8383522727272728            
Batch: #17   | Top-1 Accuracy: 0.8355614973262032            

Classifier Training Epoch #33
------------------------------------------
Epoch: #33   | Batch: #1    | Batch Loss: 0.45557141304016113           | Epoch Loss: 0.45557141304016113           
Epoch: #33   | Batch: #2    | Batch Loss: 0.5471794605255127            | Epoch Loss: 0.5013754367828369            
Epoch: #33   | Batch: #3    | Batch Loss: 0.44371092319488525           | Epoch Loss: 0.4821539322535197            
Epoch: #33   | Batch: #4    | Batch Loss: 0.6509337425231934            | Epoch Loss: 0.5243488848209381            
Epoch: #33   | Batch: #5    | Batch Loss: 0.47323569655418396           | Epoch Loss: 0.5141262471675873            
Epoch: #33   | Batch: #6    | Batch Loss: 0.4512532949447632            | Epoch Loss: 0.5036474217971166            
Epoch: #33   | Batch: #7    | Batch Loss: 0.5510714054107666            | Epoch Loss: 0.5104222765990666            
Epoch: #33   | Batch: #8    | Batch Loss: 0.4497387707233429            | Epoch Loss: 0.5028368383646011            
Epoch: #33   | Batch: #9    | Batch Loss: 0.4960007965564728            | Epoch Loss: 0.502077278163698             
Epoch: #33   | Batch: #10   | Batch Loss: 0.541987419128418             | Epoch Loss: 0.50606829226017              
Epoch: #33   | Batch: #11   | Batch Loss: 0.4964326322078705            | Epoch Loss: 0.5051923231645064            
Epoch: #33   | Batch: #12   | Batch Loss: 0.5231590270996094            | Epoch Loss: 0.5066895484924316            
Epoch: #33   | Batch: #13   | Batch Loss: 0.5415727496147156            | Epoch Loss: 0.5093728716556842            
Epoch: #33   | Batch: #14   | Batch Loss: 0.4731293022632599            | Epoch Loss: 0.5067840452705111            
Epoch: #33   | Batch: #15   | Batch Loss: 0.4774068295955658            | Epoch Loss: 0.5048255642255147            
Epoch: #33   | Batch: #16   | Batch Loss: 0.5544782876968384            | Epoch Loss: 0.5079288594424725            
Epoch: #33   | Batch: #17   | Batch Loss: 0.5473353862762451            | Epoch Loss: 0.5102468904326943            
Epoch: #33   | Batch: #18   | Batch Loss: 0.5968512296676636            | Epoch Loss: 0.5150582426124148            
Epoch: #33   | Batch: #19   | Batch Loss: 0.480112224817276             | Epoch Loss: 0.5132189785179339            
Epoch: #33   | Batch: #20   | Batch Loss: 0.48710617423057556           | Epoch Loss: 0.511913338303566             
Epoch: #33   | Batch: #21   | Batch Loss: 0.5605785250663757            | Epoch Loss: 0.5142307281494141            
Epoch: #33   | Batch: #22   | Batch Loss: 0.5182859301567078            | Epoch Loss: 0.514415055513382             
Epoch: #33   | Batch: #23   | Batch Loss: 0.5072956681251526            | Epoch Loss: 0.514105516931285             
Epoch: #33   | Batch: #24   | Batch Loss: 0.505014955997467             | Epoch Loss: 0.5137267435590426            
Epoch: #33   | Batch: #25   | Batch Loss: 0.4658132791519165            | Epoch Loss: 0.5118102049827575            
Epoch: #33   | Batch: #26   | Batch Loss: 0.5477351546287537            | Epoch Loss: 0.5131919338152959            
Epoch: #33   | Batch: #27   | Batch Loss: 0.5474454760551453            | Epoch Loss: 0.5144605835278829            
Epoch: #33   | Batch: #28   | Batch Loss: 0.6253156065940857            | Epoch Loss: 0.518419691494533             
Epoch: #33   | Batch: #29   | Batch Loss: 0.5271672606468201            | Epoch Loss: 0.5187213318101291            
Epoch: #33   | Batch: #30   | Batch Loss: 0.558831512928009             | Epoch Loss: 0.5200583378473917            
Epoch: #33   | Batch: #31   | Batch Loss: 0.5920311212539673            | Epoch Loss: 0.5223800405379264            
Epoch: #33   | Batch: #32   | Batch Loss: 0.5638686418533325            | Epoch Loss: 0.5236765593290329            
Epoch: #33   | Batch: #33   | Batch Loss: 0.4912695586681366            | Epoch Loss: 0.5226945290059755            
Epoch: #33   | Batch: #34   | Batch Loss: 0.5986064672470093            | Epoch Loss: 0.5249272330718882            
Epoch: #33   | Batch: #35   | Batch Loss: 0.49558642506599426           | Epoch Loss: 0.5240889242717198            
Epoch: #33   | Batch: #36   | Batch Loss: 0.4953100085258484            | Epoch Loss: 0.5232895099454455            
Epoch: #33   | Batch: #37   | Batch Loss: 0.6098519563674927            | Epoch Loss: 0.5256290355244199            
Epoch: #33   | Batch: #38   | Batch Loss: 0.5908262133598328            | Epoch Loss: 0.5273447507306149            
Epoch: #33   | Batch: #39   | Batch Loss: 0.6282637119293213            | Epoch Loss: 0.5299324164023766            
Epoch: #33   | Batch: #40   | Batch Loss: 0.4693048894405365            | Epoch Loss: 0.5284167282283306            
Epoch: #33   | Batch: #41   | Batch Loss: 0.6090015769004822            | Epoch Loss: 0.5303822123422856            
Epoch: #33   | Batch: #42   | Batch Loss: 0.479570209980011             | Epoch Loss: 0.5291724027622313            
Epoch: #33   | Batch: #43   | Batch Loss: 0.48856866359710693           | Epoch Loss: 0.5282281297583913            

Classifier Validation Epoch #33
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8409090909090909            
Batch: #2    | Top-1 Accuracy: 0.8545454545454545            
Batch: #3    | Top-1 Accuracy: 0.8318181818181817            
Batch: #4    | Top-1 Accuracy: 0.8352272727272727            
Batch: #5    | Top-1 Accuracy: 0.8318181818181818            
Batch: #6    | Top-1 Accuracy: 0.8340909090909091            
Batch: #7    | Top-1 Accuracy: 0.82987012987013              
Batch: #8    | Top-1 Accuracy: 0.8295454545454546            
Batch: #9    | Top-1 Accuracy: 0.8328282828282829            
Batch: #10   | Top-1 Accuracy: 0.8327272727272728            
Batch: #11   | Top-1 Accuracy: 0.834297520661157             
Batch: #12   | Top-1 Accuracy: 0.8359848484848484            
Batch: #13   | Top-1 Accuracy: 0.8356643356643356            
Batch: #14   | Top-1 Accuracy: 0.8402597402597403            
Batch: #15   | Top-1 Accuracy: 0.8396969696969697            
Batch: #16   | Top-1 Accuracy: 0.8400568181818182            
Batch: #17   | Top-1 Accuracy: 0.8382352941176471            

Classifier Training Epoch #34
------------------------------------------
Epoch: #34   | Batch: #1    | Batch Loss: 0.5883145332336426            | Epoch Loss: 0.5883145332336426            
Epoch: #34   | Batch: #2    | Batch Loss: 0.5881683826446533            | Epoch Loss: 0.588241457939148             
Epoch: #34   | Batch: #3    | Batch Loss: 0.5669220089912415            | Epoch Loss: 0.5811349749565125            
Epoch: #34   | Batch: #4    | Batch Loss: 0.48449963331222534           | Epoch Loss: 0.5569761395454407            
Epoch: #34   | Batch: #5    | Batch Loss: 0.5261198878288269            | Epoch Loss: 0.5508048892021179            
Epoch: #34   | Batch: #6    | Batch Loss: 0.5280275344848633            | Epoch Loss: 0.5470086634159088            
Epoch: #34   | Batch: #7    | Batch Loss: 0.5388930439949036            | Epoch Loss: 0.5458492892129081            
Epoch: #34   | Batch: #8    | Batch Loss: 0.5413181781768799            | Epoch Loss: 0.5452829003334045            
Epoch: #34   | Batch: #9    | Batch Loss: 0.6128687858581543            | Epoch Loss: 0.5527924431694878            
Epoch: #34   | Batch: #10   | Batch Loss: 0.4618760645389557            | Epoch Loss: 0.5437008053064346            
Epoch: #34   | Batch: #11   | Batch Loss: 0.5636616945266724            | Epoch Loss: 0.5455154315991835            
Epoch: #34   | Batch: #12   | Batch Loss: 0.47180119156837463           | Epoch Loss: 0.5393725782632828            
Epoch: #34   | Batch: #13   | Batch Loss: 0.6200982332229614            | Epoch Loss: 0.5455822440294119            
Epoch: #34   | Batch: #14   | Batch Loss: 0.42678478360176086           | Epoch Loss: 0.5370967111417225            
Epoch: #34   | Batch: #15   | Batch Loss: 0.37561896443367004           | Epoch Loss: 0.5263315280278523            
Epoch: #34   | Batch: #16   | Batch Loss: 0.5659934878349304            | Epoch Loss: 0.5288104005157948            
Epoch: #34   | Batch: #17   | Batch Loss: 0.4840174913406372            | Epoch Loss: 0.5261755235054913            
Epoch: #34   | Batch: #18   | Batch Loss: 0.5468816161155701            | Epoch Loss: 0.527325861983829             
Epoch: #34   | Batch: #19   | Batch Loss: 0.5740401744842529            | Epoch Loss: 0.5297845100101671            
Epoch: #34   | Batch: #20   | Batch Loss: 0.5397600531578064            | Epoch Loss: 0.5302832871675491            
Epoch: #34   | Batch: #21   | Batch Loss: 0.5555518269538879            | Epoch Loss: 0.5314865509668986            
Epoch: #34   | Batch: #22   | Batch Loss: 0.5163941979408264            | Epoch Loss: 0.5308005349202589            
Epoch: #34   | Batch: #23   | Batch Loss: 0.5466442108154297            | Epoch Loss: 0.531489390393962             
Epoch: #34   | Batch: #24   | Batch Loss: 0.643898606300354             | Epoch Loss: 0.536173107723395             
Epoch: #34   | Batch: #25   | Batch Loss: 0.5357431173324585            | Epoch Loss: 0.5361559081077576            
Epoch: #34   | Batch: #26   | Batch Loss: 0.5453034043312073            | Epoch Loss: 0.5365077348855826            
Epoch: #34   | Batch: #27   | Batch Loss: 0.6000183820724487            | Epoch Loss: 0.5388599810776887            
Epoch: #34   | Batch: #28   | Batch Loss: 0.5061607360839844            | Epoch Loss: 0.5376921508993421            
Epoch: #34   | Batch: #29   | Batch Loss: 0.5499441623687744            | Epoch Loss: 0.5381146340534605            
Epoch: #34   | Batch: #30   | Batch Loss: 0.5278182029724121            | Epoch Loss: 0.5377714196840923            
Epoch: #34   | Batch: #31   | Batch Loss: 0.465276300907135             | Epoch Loss: 0.5354328674654807            
Epoch: #34   | Batch: #32   | Batch Loss: 0.8071752786636353            | Epoch Loss: 0.543924817815423             
Epoch: #34   | Batch: #33   | Batch Loss: 0.5103264451026917            | Epoch Loss: 0.5429066853089766            
Epoch: #34   | Batch: #34   | Batch Loss: 0.5638457536697388            | Epoch Loss: 0.5435225402607637            
Epoch: #34   | Batch: #35   | Batch Loss: 0.4409453570842743            | Epoch Loss: 0.5405917635985783            
Epoch: #34   | Batch: #36   | Batch Loss: 0.5679032206535339            | Epoch Loss: 0.5413504151834382            
Epoch: #34   | Batch: #37   | Batch Loss: 0.5438745617866516            | Epoch Loss: 0.5414186353619034            
Epoch: #34   | Batch: #38   | Batch Loss: 0.42814016342163086           | Epoch Loss: 0.5384376229424226            
Epoch: #34   | Batch: #39   | Batch Loss: 0.5175501108169556            | Epoch Loss: 0.5379020457084362            
Epoch: #34   | Batch: #40   | Batch Loss: 0.7847380042076111            | Epoch Loss: 0.5440729446709156            
Epoch: #34   | Batch: #41   | Batch Loss: 0.6247065663337708            | Epoch Loss: 0.5460396183700096            
Epoch: #34   | Batch: #42   | Batch Loss: 0.4835374653339386            | Epoch Loss: 0.5445514718691508            
Epoch: #34   | Batch: #43   | Batch Loss: 0.4147472679615021            | Epoch Loss: 0.5415327694526938            

Classifier Validation Epoch #34
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8318181818181818            
Batch: #2    | Top-1 Accuracy: 0.8568181818181818            
Batch: #3    | Top-1 Accuracy: 0.853030303030303             
Batch: #4    | Top-1 Accuracy: 0.8522727272727273            
Batch: #5    | Top-1 Accuracy: 0.8527272727272728            
Batch: #6    | Top-1 Accuracy: 0.8484848484848485            
Batch: #7    | Top-1 Accuracy: 0.8448051948051949            
Batch: #8    | Top-1 Accuracy: 0.8448863636363636            
Batch: #9    | Top-1 Accuracy: 0.8439393939393939            
Batch: #10   | Top-1 Accuracy: 0.845                         
Batch: #11   | Top-1 Accuracy: 0.8429752066115701            
Batch: #12   | Top-1 Accuracy: 0.8443181818181817            
Batch: #13   | Top-1 Accuracy: 0.8440559440559441            
Batch: #14   | Top-1 Accuracy: 0.8454545454545455            
Batch: #15   | Top-1 Accuracy: 0.8427272727272727            
Batch: #16   | Top-1 Accuracy: 0.8420454545454545            
Batch: #17   | Top-1 Accuracy: 0.8427807486631016            

Classifier Training Epoch #35
------------------------------------------
Epoch: #35   | Batch: #1    | Batch Loss: 0.4962555766105652            | Epoch Loss: 0.4962555766105652            
Epoch: #35   | Batch: #2    | Batch Loss: 0.5919015407562256            | Epoch Loss: 0.5440785586833954            
Epoch: #35   | Batch: #3    | Batch Loss: 0.5798753499984741            | Epoch Loss: 0.5560108224550883            
Epoch: #35   | Batch: #4    | Batch Loss: 0.5734233260154724            | Epoch Loss: 0.5603639483451843            
Epoch: #35   | Batch: #5    | Batch Loss: 0.4340963065624237            | Epoch Loss: 0.5351104199886322            
Epoch: #35   | Batch: #6    | Batch Loss: 0.42463743686676025           | Epoch Loss: 0.5166982561349869            
Epoch: #35   | Batch: #7    | Batch Loss: 0.561975359916687             | Epoch Loss: 0.5231664138180869            
Epoch: #35   | Batch: #8    | Batch Loss: 0.489192396402359             | Epoch Loss: 0.5189196616411209            
Epoch: #35   | Batch: #9    | Batch Loss: 0.6258341670036316            | Epoch Loss: 0.5307990511258444            
Epoch: #35   | Batch: #10   | Batch Loss: 0.45925673842430115           | Epoch Loss: 0.52364481985569              
Epoch: #35   | Batch: #11   | Batch Loss: 0.4900771379470825            | Epoch Loss: 0.520593212409453             
Epoch: #35   | Batch: #12   | Batch Loss: 0.5924843549728394            | Epoch Loss: 0.5265841409564018            
Epoch: #35   | Batch: #13   | Batch Loss: 0.4486393332481384            | Epoch Loss: 0.5205883865173047            
Epoch: #35   | Batch: #14   | Batch Loss: 0.45603832602500916           | Epoch Loss: 0.5159776679107121            
Epoch: #35   | Batch: #15   | Batch Loss: 0.4773993492126465            | Epoch Loss: 0.5134057799975077            
Epoch: #35   | Batch: #16   | Batch Loss: 0.5344628095626831            | Epoch Loss: 0.5147218443453312            
Epoch: #35   | Batch: #17   | Batch Loss: 0.6600193381309509            | Epoch Loss: 0.5232687557444853            
Epoch: #35   | Batch: #18   | Batch Loss: 0.4561159312725067            | Epoch Loss: 0.5195380432738198            
Epoch: #35   | Batch: #19   | Batch Loss: 0.5307878851890564            | Epoch Loss: 0.520130140216727             
Epoch: #35   | Batch: #20   | Batch Loss: 0.505719780921936             | Epoch Loss: 0.5194096222519875            
Epoch: #35   | Batch: #21   | Batch Loss: 0.5530859231948853            | Epoch Loss: 0.5210132556302207            
Epoch: #35   | Batch: #22   | Batch Loss: 0.5788418054580688            | Epoch Loss: 0.5236418260769411            
Epoch: #35   | Batch: #23   | Batch Loss: 0.3734610676765442            | Epoch Loss: 0.5171122278856195            
Epoch: #35   | Batch: #24   | Batch Loss: 0.6126909255981445            | Epoch Loss: 0.5210946736236414            
Epoch: #35   | Batch: #25   | Batch Loss: 0.5367457866668701            | Epoch Loss: 0.5217207181453705            
Epoch: #35   | Batch: #26   | Batch Loss: 0.5409766435623169            | Epoch Loss: 0.5224613306614069            
Epoch: #35   | Batch: #27   | Batch Loss: 0.5824173092842102            | Epoch Loss: 0.5246819224622514            
Epoch: #35   | Batch: #28   | Batch Loss: 0.47599178552627563           | Epoch Loss: 0.5229429890002523            
Epoch: #35   | Batch: #29   | Batch Loss: 0.5786533951759338            | Epoch Loss: 0.524864037489069             
Epoch: #35   | Batch: #30   | Batch Loss: 0.5339981317520142            | Epoch Loss: 0.5251685072978337            
Epoch: #35   | Batch: #31   | Batch Loss: 0.49105703830718994           | Epoch Loss: 0.5240681373303936            
Epoch: #35   | Batch: #32   | Batch Loss: 0.344157338142395             | Epoch Loss: 0.5184459248557687            
Epoch: #35   | Batch: #33   | Batch Loss: 0.5833342671394348            | Epoch Loss: 0.5204122382583041            
Epoch: #35   | Batch: #34   | Batch Loss: 0.6135767102241516            | Epoch Loss: 0.5231523697867113            
Epoch: #35   | Batch: #35   | Batch Loss: 0.6525478959083557            | Epoch Loss: 0.5268493848187583            
Epoch: #35   | Batch: #36   | Batch Loss: 0.5619863867759705            | Epoch Loss: 0.5278254126509031            
Epoch: #35   | Batch: #37   | Batch Loss: 0.4580366015434265            | Epoch Loss: 0.5259392285669172            
Epoch: #35   | Batch: #38   | Batch Loss: 0.5569303631782532            | Epoch Loss: 0.5267547847408998            
Epoch: #35   | Batch: #39   | Batch Loss: 0.5051228404045105            | Epoch Loss: 0.5262001195015051            
Epoch: #35   | Batch: #40   | Batch Loss: 0.584351122379303             | Epoch Loss: 0.5276538945734501            
Epoch: #35   | Batch: #41   | Batch Loss: 0.5778509378433228            | Epoch Loss: 0.5288782127019835            
Epoch: #35   | Batch: #42   | Batch Loss: 0.4497968852519989            | Epoch Loss: 0.5269953239531744            
Epoch: #35   | Batch: #43   | Batch Loss: 0.4993264079093933            | Epoch Loss: 0.5263518607893656            

Classifier Validation Epoch #35
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8181818181818182            
Batch: #2    | Top-1 Accuracy: 0.8136363636363637            
Batch: #3    | Top-1 Accuracy: 0.8181818181818182            
Batch: #4    | Top-1 Accuracy: 0.8147727272727273            
Batch: #5    | Top-1 Accuracy: 0.8218181818181819            
Batch: #6    | Top-1 Accuracy: 0.8234848484848486            
Batch: #7    | Top-1 Accuracy: 0.8240259740259741            
Batch: #8    | Top-1 Accuracy: 0.8193181818181818            
Batch: #9    | Top-1 Accuracy: 0.8202020202020202            
Batch: #10   | Top-1 Accuracy: 0.82                          
Batch: #11   | Top-1 Accuracy: 0.8173553719008264            
Batch: #12   | Top-1 Accuracy: 0.8178030303030303            
Batch: #13   | Top-1 Accuracy: 0.8185314685314685            
Batch: #14   | Top-1 Accuracy: 0.8224025974025974            
Batch: #15   | Top-1 Accuracy: 0.8245454545454545            
Batch: #16   | Top-1 Accuracy: 0.825                         
Batch: #17   | Top-1 Accuracy: 0.8235294117647058            

Classifier Training Epoch #36
------------------------------------------
Epoch: #36   | Batch: #1    | Batch Loss: 0.6190655827522278            | Epoch Loss: 0.6190655827522278            
Epoch: #36   | Batch: #2    | Batch Loss: 0.4787977635860443            | Epoch Loss: 0.548931673169136             
Epoch: #36   | Batch: #3    | Batch Loss: 0.623947262763977             | Epoch Loss: 0.5739368697007498            
Epoch: #36   | Batch: #4    | Batch Loss: 0.5987482666969299            | Epoch Loss: 0.5801397189497948            
Epoch: #36   | Batch: #5    | Batch Loss: 0.43955424427986145           | Epoch Loss: 0.5520226240158081            
Epoch: #36   | Batch: #6    | Batch Loss: 0.5203431844711304            | Epoch Loss: 0.5467427174250284            
Epoch: #36   | Batch: #7    | Batch Loss: 0.7417204976081848            | Epoch Loss: 0.5745966860226223            
Epoch: #36   | Batch: #8    | Batch Loss: 0.6285750865936279            | Epoch Loss: 0.581343986093998             
Epoch: #36   | Batch: #9    | Batch Loss: 0.5832505226135254            | Epoch Loss: 0.5815558234850565            
Epoch: #36   | Batch: #10   | Batch Loss: 0.4874204099178314            | Epoch Loss: 0.5721422821283341            
Epoch: #36   | Batch: #11   | Batch Loss: 0.49600279331207275           | Epoch Loss: 0.5652205104177649            
Epoch: #36   | Batch: #12   | Batch Loss: 0.5820379853248596            | Epoch Loss: 0.5666219666600227            
Epoch: #36   | Batch: #13   | Batch Loss: 0.5626066327095032            | Epoch Loss: 0.5663130948176751            
Epoch: #36   | Batch: #14   | Batch Loss: 0.5068672299385071            | Epoch Loss: 0.5620669616120202            
Epoch: #36   | Batch: #15   | Batch Loss: 0.48139238357543945           | Epoch Loss: 0.5566886564095815            
Epoch: #36   | Batch: #16   | Batch Loss: 0.46691572666168213           | Epoch Loss: 0.5510778483003378            
Epoch: #36   | Batch: #17   | Batch Loss: 0.4722072184085846            | Epoch Loss: 0.5464383994831759            
Epoch: #36   | Batch: #18   | Batch Loss: 0.5430125594139099            | Epoch Loss: 0.5462480750348833            
Epoch: #36   | Batch: #19   | Batch Loss: 0.5590040683746338            | Epoch Loss: 0.5469194431053964            
Epoch: #36   | Batch: #20   | Batch Loss: 0.6549771428108215            | Epoch Loss: 0.5523223280906677            
Epoch: #36   | Batch: #21   | Batch Loss: 0.4521178901195526            | Epoch Loss: 0.5475506881872813            
Epoch: #36   | Batch: #22   | Batch Loss: 0.5972808003425598            | Epoch Loss: 0.5498111478307031            
Epoch: #36   | Batch: #23   | Batch Loss: 0.6104698777198792            | Epoch Loss: 0.5524484839128412            
Epoch: #36   | Batch: #24   | Batch Loss: 0.6369550824165344            | Epoch Loss: 0.5559695921838284            
Epoch: #36   | Batch: #25   | Batch Loss: 0.538740873336792             | Epoch Loss: 0.5552804434299469            
Epoch: #36   | Batch: #26   | Batch Loss: 0.6009351015090942            | Epoch Loss: 0.5570363918176064            
Epoch: #36   | Batch: #27   | Batch Loss: 0.5520634651184082            | Epoch Loss: 0.5568522093472658            
Epoch: #36   | Batch: #28   | Batch Loss: 0.6157428622245789            | Epoch Loss: 0.558955446950027             
Epoch: #36   | Batch: #29   | Batch Loss: 0.6345682144165039            | Epoch Loss: 0.5615627837592158            
Epoch: #36   | Batch: #30   | Batch Loss: 0.4419156014919281            | Epoch Loss: 0.5575745443503062            
Epoch: #36   | Batch: #31   | Batch Loss: 0.5016058683395386            | Epoch Loss: 0.5557691031886686            
Epoch: #36   | Batch: #32   | Batch Loss: 0.48612362146377563           | Epoch Loss: 0.5535926818847656            
Epoch: #36   | Batch: #33   | Batch Loss: 0.45634225010871887           | Epoch Loss: 0.5506456991036733            
Epoch: #36   | Batch: #34   | Batch Loss: 0.5338977575302124            | Epoch Loss: 0.5501531125868068            
Epoch: #36   | Batch: #35   | Batch Loss: 0.5540711879730225            | Epoch Loss: 0.5502650575978415            
Epoch: #36   | Batch: #36   | Batch Loss: 0.48002663254737854           | Epoch Loss: 0.5483139902353287            
Epoch: #36   | Batch: #37   | Batch Loss: 0.508292019367218             | Epoch Loss: 0.5472323153470013            
Epoch: #36   | Batch: #38   | Batch Loss: 0.45482438802719116           | Epoch Loss: 0.5448005277859537            
Epoch: #36   | Batch: #39   | Batch Loss: 0.5909979939460754            | Epoch Loss: 0.5459850782003158            
Epoch: #36   | Batch: #40   | Batch Loss: 0.5344357490539551            | Epoch Loss: 0.5456963449716568            
Epoch: #36   | Batch: #41   | Batch Loss: 0.5718984603881836            | Epoch Loss: 0.5463354209574257            
Epoch: #36   | Batch: #42   | Batch Loss: 0.3716936409473419            | Epoch Loss: 0.5421772833381381            
Epoch: #36   | Batch: #43   | Batch Loss: 0.6145961880683899            | Epoch Loss: 0.5438614439132602            

Classifier Validation Epoch #36
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8318181818181818            
Batch: #2    | Top-1 Accuracy: 0.8386363636363636            
Batch: #3    | Top-1 Accuracy: 0.8348484848484848            
Batch: #4    | Top-1 Accuracy: 0.834090909090909             
Batch: #5    | Top-1 Accuracy: 0.8327272727272726            
Batch: #6    | Top-1 Accuracy: 0.8227272727272726            
Batch: #7    | Top-1 Accuracy: 0.824025974025974             
Batch: #8    | Top-1 Accuracy: 0.8284090909090909            
Batch: #9    | Top-1 Accuracy: 0.8282828282828283            
Batch: #10   | Top-1 Accuracy: 0.8227272727272726            
Batch: #11   | Top-1 Accuracy: 0.8252066115702479            
Batch: #12   | Top-1 Accuracy: 0.8272727272727272            
Batch: #13   | Top-1 Accuracy: 0.8293706293706293            
Batch: #14   | Top-1 Accuracy: 0.8314935064935065            
Batch: #15   | Top-1 Accuracy: 0.8321212121212122            
Batch: #16   | Top-1 Accuracy: 0.8323863636363636            
Batch: #17   | Top-1 Accuracy: 0.8352941176470587            

Classifier Training Epoch #37
------------------------------------------
Epoch: #37   | Batch: #1    | Batch Loss: 0.5547785758972168            | Epoch Loss: 0.5547785758972168            
Epoch: #37   | Batch: #2    | Batch Loss: 0.5361288785934448            | Epoch Loss: 0.5454537272453308            
Epoch: #37   | Batch: #3    | Batch Loss: 0.3874308168888092            | Epoch Loss: 0.4927794237931569            
Epoch: #37   | Batch: #4    | Batch Loss: 0.5211721658706665            | Epoch Loss: 0.49987760931253433           
Epoch: #37   | Batch: #5    | Batch Loss: 0.40355080366134644           | Epoch Loss: 0.48061224818229675           
Epoch: #37   | Batch: #6    | Batch Loss: 0.5785251259803772            | Epoch Loss: 0.4969310611486435            
Epoch: #37   | Batch: #7    | Batch Loss: 0.5869067907333374            | Epoch Loss: 0.5097847368035998            
Epoch: #37   | Batch: #8    | Batch Loss: 0.5606372356414795            | Epoch Loss: 0.5161412991583347            
Epoch: #37   | Batch: #9    | Batch Loss: 0.4533080756664276            | Epoch Loss: 0.5091598298814561            
Epoch: #37   | Batch: #10   | Batch Loss: 0.5157475471496582            | Epoch Loss: 0.5098186016082764            
Epoch: #37   | Batch: #11   | Batch Loss: 0.545972466468811             | Epoch Loss: 0.5131053165955977            
Epoch: #37   | Batch: #12   | Batch Loss: 0.4993364214897156            | Epoch Loss: 0.5119579086701075            
Epoch: #37   | Batch: #13   | Batch Loss: 0.41537249088287354           | Epoch Loss: 0.5045282611480126            
Epoch: #37   | Batch: #14   | Batch Loss: 0.5146116614341736            | Epoch Loss: 0.5052485040255955            
Epoch: #37   | Batch: #15   | Batch Loss: 0.5673640370368958            | Epoch Loss: 0.5093895395596822            
Epoch: #37   | Batch: #16   | Batch Loss: 0.5612741112709045            | Epoch Loss: 0.5126323252916336            
Epoch: #37   | Batch: #17   | Batch Loss: 0.4529232382774353            | Epoch Loss: 0.5091200260555043            
Epoch: #37   | Batch: #18   | Batch Loss: 0.5183621644973755            | Epoch Loss: 0.5096334781911638            
Epoch: #37   | Batch: #19   | Batch Loss: 0.603784441947937             | Epoch Loss: 0.5145887920730993            
Epoch: #37   | Batch: #20   | Batch Loss: 0.4653736352920532            | Epoch Loss: 0.512128034234047             
Epoch: #37   | Batch: #21   | Batch Loss: 0.6628155708312988            | Epoch Loss: 0.5193036312148684            
Epoch: #37   | Batch: #22   | Batch Loss: 0.45961013436317444           | Epoch Loss: 0.5165902904488824            
Epoch: #37   | Batch: #23   | Batch Loss: 0.37184879183769226           | Epoch Loss: 0.5102971818136133            
Epoch: #37   | Batch: #24   | Batch Loss: 0.5860675573348999            | Epoch Loss: 0.5134542807936668            
Epoch: #37   | Batch: #25   | Batch Loss: 0.5204736590385437            | Epoch Loss: 0.5137350559234619            
Epoch: #37   | Batch: #26   | Batch Loss: 0.41011154651641846           | Epoch Loss: 0.5097495363308833            
Epoch: #37   | Batch: #27   | Batch Loss: 0.6015490293502808            | Epoch Loss: 0.513149517553824             
Epoch: #37   | Batch: #28   | Batch Loss: 0.44301435351371765           | Epoch Loss: 0.5106446902666774            
Epoch: #37   | Batch: #29   | Batch Loss: 0.5408392548561096            | Epoch Loss: 0.5116858821490715            
Epoch: #37   | Batch: #30   | Batch Loss: 0.6400982737541199            | Epoch Loss: 0.5159662952025731            
Epoch: #37   | Batch: #31   | Batch Loss: 0.5928081274032593            | Epoch Loss: 0.5184450639832404            
Epoch: #37   | Batch: #32   | Batch Loss: 0.4533706307411194            | Epoch Loss: 0.5164114879444242            
Epoch: #37   | Batch: #33   | Batch Loss: 0.5700429677963257            | Epoch Loss: 0.5180366843035726            
Epoch: #37   | Batch: #34   | Batch Loss: 0.527894914150238             | Epoch Loss: 0.5183266322402393            
Epoch: #37   | Batch: #35   | Batch Loss: 0.5418255925178528            | Epoch Loss: 0.518998031105314             
Epoch: #37   | Batch: #36   | Batch Loss: 0.4969344139099121            | Epoch Loss: 0.5183851528498862            
Epoch: #37   | Batch: #37   | Batch Loss: 0.4293820559978485            | Epoch Loss: 0.515979663745777             
Epoch: #37   | Batch: #38   | Batch Loss: 0.560993492603302             | Epoch Loss: 0.517164238189396             
Epoch: #37   | Batch: #39   | Batch Loss: 0.5301624536514282            | Epoch Loss: 0.5174975257653457            
Epoch: #37   | Batch: #40   | Batch Loss: 0.517845094203949             | Epoch Loss: 0.5175062149763108            
Epoch: #37   | Batch: #41   | Batch Loss: 0.4446343183517456            | Epoch Loss: 0.5157288516440043            
Epoch: #37   | Batch: #42   | Batch Loss: 0.4309648275375366            | Epoch Loss: 0.5137106605938503            
Epoch: #37   | Batch: #43   | Batch Loss: 0.5681490898132324            | Epoch Loss: 0.5149766705756964            

Classifier Validation Epoch #37
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8227272727272728            
Batch: #2    | Top-1 Accuracy: 0.8500000000000001            
Batch: #3    | Top-1 Accuracy: 0.8333333333333334            
Batch: #4    | Top-1 Accuracy: 0.8363636363636364            
Batch: #5    | Top-1 Accuracy: 0.8436363636363637            
Batch: #6    | Top-1 Accuracy: 0.8371212121212123            
Batch: #7    | Top-1 Accuracy: 0.8350649350649351            
Batch: #8    | Top-1 Accuracy: 0.8306818181818182            
Batch: #9    | Top-1 Accuracy: 0.8277777777777778            
Batch: #10   | Top-1 Accuracy: 0.8304545454545454            
Batch: #11   | Top-1 Accuracy: 0.8305785123966942            
Batch: #12   | Top-1 Accuracy: 0.831439393939394             
Batch: #13   | Top-1 Accuracy: 0.834965034965035             
Batch: #14   | Top-1 Accuracy: 0.8340909090909091            
Batch: #15   | Top-1 Accuracy: 0.8351515151515152            
Batch: #16   | Top-1 Accuracy: 0.837784090909091             
Batch: #17   | Top-1 Accuracy: 0.8390374331550803            

Classifier Training Epoch #38
------------------------------------------
Epoch: #38   | Batch: #1    | Batch Loss: 0.47485262155532837           | Epoch Loss: 0.47485262155532837           
Epoch: #38   | Batch: #2    | Batch Loss: 0.48204711079597473           | Epoch Loss: 0.47844986617565155           
Epoch: #38   | Batch: #3    | Batch Loss: 0.48483216762542725           | Epoch Loss: 0.48057729999224347           
Epoch: #38   | Batch: #4    | Batch Loss: 0.42342984676361084           | Epoch Loss: 0.4662904366850853            
Epoch: #38   | Batch: #5    | Batch Loss: 0.4785628616809845            | Epoch Loss: 0.4687449216842651            
Epoch: #38   | Batch: #6    | Batch Loss: 0.4075520634651184            | Epoch Loss: 0.45854611198107403           
Epoch: #38   | Batch: #7    | Batch Loss: 0.41008612513542175           | Epoch Loss: 0.4516232567174094            
Epoch: #38   | Batch: #8    | Batch Loss: 0.48741766810417175           | Epoch Loss: 0.4560975581407547            
Epoch: #38   | Batch: #9    | Batch Loss: 0.5517805814743042            | Epoch Loss: 0.46672900517781574           
Epoch: #38   | Batch: #10   | Batch Loss: 0.5389155745506287            | Epoch Loss: 0.47394766211509703           
Epoch: #38   | Batch: #11   | Batch Loss: 0.5034652352333069            | Epoch Loss: 0.47663107785311615           
Epoch: #38   | Batch: #12   | Batch Loss: 0.5643373131752014            | Epoch Loss: 0.48393993079662323           
Epoch: #38   | Batch: #13   | Batch Loss: 0.4903302490711212            | Epoch Loss: 0.4844314937408154            
Epoch: #38   | Batch: #14   | Batch Loss: 0.5144538283348083            | Epoch Loss: 0.4865759462118149            
Epoch: #38   | Batch: #15   | Batch Loss: 0.44543737173080444           | Epoch Loss: 0.4838333745797475            
Epoch: #38   | Batch: #16   | Batch Loss: 0.4563266336917877            | Epoch Loss: 0.48211420327425003           
Epoch: #38   | Batch: #17   | Batch Loss: 0.547919511795044             | Epoch Loss: 0.4859851037754732            
Epoch: #38   | Batch: #18   | Batch Loss: 0.466426819562912             | Epoch Loss: 0.48489853243033093           
Epoch: #38   | Batch: #19   | Batch Loss: 0.6315506100654602            | Epoch Loss: 0.4926170628321798            
Epoch: #38   | Batch: #20   | Batch Loss: 0.6730300784111023            | Epoch Loss: 0.501637713611126             
Epoch: #38   | Batch: #21   | Batch Loss: 0.6180321574211121            | Epoch Loss: 0.5071803061735063            
Epoch: #38   | Batch: #22   | Batch Loss: 0.5183310508728027            | Epoch Loss: 0.5076871582052924            
Epoch: #38   | Batch: #23   | Batch Loss: 0.533257007598877             | Epoch Loss: 0.5087988907876222            
Epoch: #38   | Batch: #24   | Batch Loss: 0.4636073708534241            | Epoch Loss: 0.5069159107903639            
Epoch: #38   | Batch: #25   | Batch Loss: 0.5943778157234192            | Epoch Loss: 0.5104143869876862            
Epoch: #38   | Batch: #26   | Batch Loss: 0.48830080032348633           | Epoch Loss: 0.5095638644236785            
Epoch: #38   | Batch: #27   | Batch Loss: 0.5593488812446594            | Epoch Loss: 0.5114077539355667            
Epoch: #38   | Batch: #28   | Batch Loss: 0.5116270184516907            | Epoch Loss: 0.5114155848111425            
Epoch: #38   | Batch: #29   | Batch Loss: 0.49398282170295715           | Epoch Loss: 0.5108144550487913            
Epoch: #38   | Batch: #30   | Batch Loss: 0.5204725861549377            | Epoch Loss: 0.5111363927523295            
Epoch: #38   | Batch: #31   | Batch Loss: 0.535803496837616             | Epoch Loss: 0.5119321057873387            
Epoch: #38   | Batch: #32   | Batch Loss: 0.4985087215900421            | Epoch Loss: 0.5115126250311732            
Epoch: #38   | Batch: #33   | Batch Loss: 0.49896639585494995           | Epoch Loss: 0.5111324362682573            
Epoch: #38   | Batch: #34   | Batch Loss: 0.5291857719421387            | Epoch Loss: 0.5116634167292539            
Epoch: #38   | Batch: #35   | Batch Loss: 0.571346640586853             | Epoch Loss: 0.5133686516966138            
Epoch: #38   | Batch: #36   | Batch Loss: 0.5880624055862427            | Epoch Loss: 0.515443478193548             
Epoch: #38   | Batch: #37   | Batch Loss: 0.5976834893226624            | Epoch Loss: 0.5176661811970376            
Epoch: #38   | Batch: #38   | Batch Loss: 0.4030624032020569            | Epoch Loss: 0.5146502923024329            
Epoch: #38   | Batch: #39   | Batch Loss: 0.4711070954799652            | Epoch Loss: 0.5135338000762157            
Epoch: #38   | Batch: #40   | Batch Loss: 0.42279207706451416           | Epoch Loss: 0.5112652570009232            
Epoch: #38   | Batch: #41   | Batch Loss: 0.5961412191390991            | Epoch Loss: 0.5133354024189275            
Epoch: #38   | Batch: #42   | Batch Loss: 0.6712861657142639            | Epoch Loss: 0.5170961348783403            
Epoch: #38   | Batch: #43   | Batch Loss: 0.5846054553985596            | Epoch Loss: 0.5186661190764849            

Classifier Validation Epoch #38
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8681818181818182            
Batch: #2    | Top-1 Accuracy: 0.8340909090909091            
Batch: #3    | Top-1 Accuracy: 0.8439393939393939            
Batch: #4    | Top-1 Accuracy: 0.8443181818181817            
Batch: #5    | Top-1 Accuracy: 0.8409090909090908            
Batch: #6    | Top-1 Accuracy: 0.840151515151515             
Batch: #7    | Top-1 Accuracy: 0.8389610389610389            
Batch: #8    | Top-1 Accuracy: 0.8397727272727272            
Batch: #9    | Top-1 Accuracy: 0.8404040404040404            
Batch: #10   | Top-1 Accuracy: 0.8436363636363635            
Batch: #11   | Top-1 Accuracy: 0.8438016528925619            
Batch: #12   | Top-1 Accuracy: 0.8439393939393939            
Batch: #13   | Top-1 Accuracy: 0.8451048951048951            
Batch: #14   | Top-1 Accuracy: 0.8461038961038961            
Batch: #15   | Top-1 Accuracy: 0.8460606060606061            
Batch: #16   | Top-1 Accuracy: 0.8491477272727272            
Batch: #17   | Top-1 Accuracy: 0.848663101604278             

Classifier Training Epoch #39
------------------------------------------
Epoch: #39   | Batch: #1    | Batch Loss: 0.5335215926170349            | Epoch Loss: 0.5335215926170349            
Epoch: #39   | Batch: #2    | Batch Loss: 0.4628118574619293            | Epoch Loss: 0.4981667250394821            
Epoch: #39   | Batch: #3    | Batch Loss: 0.3403533101081848            | Epoch Loss: 0.44556225339571637           
Epoch: #39   | Batch: #4    | Batch Loss: 0.5755323171615601            | Epoch Loss: 0.4780547693371773            
Epoch: #39   | Batch: #5    | Batch Loss: 0.438839316368103             | Epoch Loss: 0.47021167874336245           
Epoch: #39   | Batch: #6    | Batch Loss: 0.5569466352462769            | Epoch Loss: 0.4846675048271815            
Epoch: #39   | Batch: #7    | Batch Loss: 0.5951690077781677            | Epoch Loss: 0.5004534338201795            
Epoch: #39   | Batch: #8    | Batch Loss: 0.45336127281188965           | Epoch Loss: 0.4945669136941433            
Epoch: #39   | Batch: #9    | Batch Loss: 0.563356339931488             | Epoch Loss: 0.5022101832760705            
Epoch: #39   | Batch: #10   | Batch Loss: 0.5486390590667725            | Epoch Loss: 0.5068530708551406            
Epoch: #39   | Batch: #11   | Batch Loss: 0.5978776812553406            | Epoch Loss: 0.515128035436977             
Epoch: #39   | Batch: #12   | Batch Loss: 0.6289559006690979            | Epoch Loss: 0.5246136908729871            
Epoch: #39   | Batch: #13   | Batch Loss: 0.5034174919128418            | Epoch Loss: 0.522983214029899             
Epoch: #39   | Batch: #14   | Batch Loss: 0.6434516310691833            | Epoch Loss: 0.5315881009612765            
Epoch: #39   | Batch: #15   | Batch Loss: 0.5157639980316162            | Epoch Loss: 0.5305331607659658            
Epoch: #39   | Batch: #16   | Batch Loss: 0.48876819014549255           | Epoch Loss: 0.5279228501021862            
Epoch: #39   | Batch: #17   | Batch Loss: 0.5620840191841125            | Epoch Loss: 0.5299323306364172            
Epoch: #39   | Batch: #18   | Batch Loss: 0.480668306350708             | Epoch Loss: 0.5271954403983222            
Epoch: #39   | Batch: #19   | Batch Loss: 0.6623796820640564            | Epoch Loss: 0.5343104004859924            
Epoch: #39   | Batch: #20   | Batch Loss: 0.5141656994819641            | Epoch Loss: 0.533303165435791             
Epoch: #39   | Batch: #21   | Batch Loss: 0.5406410098075867            | Epoch Loss: 0.5336525865963527            
Epoch: #39   | Batch: #22   | Batch Loss: 0.4427628517150879            | Epoch Loss: 0.5295212350108407            
Epoch: #39   | Batch: #23   | Batch Loss: 0.5142316818237305            | Epoch Loss: 0.5288564718287924            
Epoch: #39   | Batch: #24   | Batch Loss: 0.5762268304824829            | Epoch Loss: 0.5308302367726961            
Epoch: #39   | Batch: #25   | Batch Loss: 0.6691360473632812            | Epoch Loss: 0.5363624691963196            
Epoch: #39   | Batch: #26   | Batch Loss: 0.44137129187583923           | Epoch Loss: 0.532708962376301             
Epoch: #39   | Batch: #27   | Batch Loss: 0.5669503211975098            | Epoch Loss: 0.5339771608511606            
Epoch: #39   | Batch: #28   | Batch Loss: 0.6653929352760315            | Epoch Loss: 0.5386705813663346            
Epoch: #39   | Batch: #29   | Batch Loss: 0.5318294763565063            | Epoch Loss: 0.538434681193582             
Epoch: #39   | Batch: #30   | Batch Loss: 0.36778515577316284           | Epoch Loss: 0.532746363679568             
Epoch: #39   | Batch: #31   | Batch Loss: 0.5477020740509033            | Epoch Loss: 0.5332288059496111            
Epoch: #39   | Batch: #32   | Batch Loss: 0.5310695767402649            | Epoch Loss: 0.533161330036819             
Epoch: #39   | Batch: #33   | Batch Loss: 0.37280669808387756           | Epoch Loss: 0.5283020987655177            
Epoch: #39   | Batch: #34   | Batch Loss: 0.5734705924987793            | Epoch Loss: 0.5296305838753196            
Epoch: #39   | Batch: #35   | Batch Loss: 0.4856536388397217            | Epoch Loss: 0.5283740997314453            
Epoch: #39   | Batch: #36   | Batch Loss: 0.5082403421401978            | Epoch Loss: 0.5278148286872439            
Epoch: #39   | Batch: #37   | Batch Loss: 0.5969616174697876            | Epoch Loss: 0.529683660816502             
Epoch: #39   | Batch: #38   | Batch Loss: 0.446493536233902             | Epoch Loss: 0.5274944470116967            
Epoch: #39   | Batch: #39   | Batch Loss: 0.5787348747253418            | Epoch Loss: 0.5288083041325594            
Epoch: #39   | Batch: #40   | Batch Loss: 0.6253280639648438            | Epoch Loss: 0.5312212981283665            
Epoch: #39   | Batch: #41   | Batch Loss: 0.4047071933746338            | Epoch Loss: 0.5281355882563242            
Epoch: #39   | Batch: #42   | Batch Loss: 0.5092498660087585            | Epoch Loss: 0.5276859282028108            
Epoch: #39   | Batch: #43   | Batch Loss: 0.4509640336036682            | Epoch Loss: 0.525901698095854             

Classifier Validation Epoch #39
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8272727272727273            
Batch: #2    | Top-1 Accuracy: 0.8431818181818183            
Batch: #3    | Top-1 Accuracy: 0.8409090909090909            
Batch: #4    | Top-1 Accuracy: 0.8363636363636364            
Batch: #5    | Top-1 Accuracy: 0.8281818181818181            
Batch: #6    | Top-1 Accuracy: 0.828030303030303             
Batch: #7    | Top-1 Accuracy: 0.8285714285714285            
Batch: #8    | Top-1 Accuracy: 0.8295454545454546            
Batch: #9    | Top-1 Accuracy: 0.8308080808080809            
Batch: #10   | Top-1 Accuracy: 0.8336363636363636            
Batch: #11   | Top-1 Accuracy: 0.8363636363636363            
Batch: #12   | Top-1 Accuracy: 0.8356060606060606            
Batch: #13   | Top-1 Accuracy: 0.8377622377622378            
Batch: #14   | Top-1 Accuracy: 0.8399350649350649            
Batch: #15   | Top-1 Accuracy: 0.8406060606060605            
Batch: #16   | Top-1 Accuracy: 0.8409090909090908            
Batch: #17   | Top-1 Accuracy: 0.8430481283422459            

Classifier Training Epoch #40
------------------------------------------
Epoch: #40   | Batch: #1    | Batch Loss: 0.4781422019004822            | Epoch Loss: 0.4781422019004822            
Epoch: #40   | Batch: #2    | Batch Loss: 0.5419044494628906            | Epoch Loss: 0.5100233256816864            
Epoch: #40   | Batch: #3    | Batch Loss: 0.5590178966522217            | Epoch Loss: 0.5263548493385315            
Epoch: #40   | Batch: #4    | Batch Loss: 0.46266263723373413           | Epoch Loss: 0.5104317963123322            
Epoch: #40   | Batch: #5    | Batch Loss: 0.4902955889701843            | Epoch Loss: 0.5064045548439026            
Epoch: #40   | Batch: #6    | Batch Loss: 0.5033268332481384            | Epoch Loss: 0.5058916012446085            
Epoch: #40   | Batch: #7    | Batch Loss: 0.46132272481918335           | Epoch Loss: 0.4995246188981192            
Epoch: #40   | Batch: #8    | Batch Loss: 0.5201665759086609            | Epoch Loss: 0.502104863524437             
Epoch: #40   | Batch: #9    | Batch Loss: 0.5361774563789368            | Epoch Loss: 0.5058907071749369            
Epoch: #40   | Batch: #10   | Batch Loss: 0.5445684790611267            | Epoch Loss: 0.5097584843635559            
Epoch: #40   | Batch: #11   | Batch Loss: 0.43231919407844543           | Epoch Loss: 0.5027185488830913            
Epoch: #40   | Batch: #12   | Batch Loss: 0.6803882718086243            | Epoch Loss: 0.5175243591268858            
Epoch: #40   | Batch: #13   | Batch Loss: 0.4054167866706848            | Epoch Loss: 0.508900699707178             
Epoch: #40   | Batch: #14   | Batch Loss: 0.680770754814148             | Epoch Loss: 0.5211771322148187            
Epoch: #40   | Batch: #15   | Batch Loss: 0.533414900302887             | Epoch Loss: 0.5219929834206899            
Epoch: #40   | Batch: #16   | Batch Loss: 0.47088250517845154           | Epoch Loss: 0.51879857853055              
Epoch: #40   | Batch: #17   | Batch Loss: 0.5193448662757874            | Epoch Loss: 0.5188307131037992            
Epoch: #40   | Batch: #18   | Batch Loss: 0.5831027030944824            | Epoch Loss: 0.5224013792143928            
Epoch: #40   | Batch: #19   | Batch Loss: 0.5293537974357605            | Epoch Loss: 0.5227672959628858            
Epoch: #40   | Batch: #20   | Batch Loss: 0.5255098938941956            | Epoch Loss: 0.5229044258594513            
Epoch: #40   | Batch: #21   | Batch Loss: 0.4916597604751587            | Epoch Loss: 0.5214165846506754            
Epoch: #40   | Batch: #22   | Batch Loss: 0.558741569519043             | Epoch Loss: 0.5231131748719648            
Epoch: #40   | Batch: #23   | Batch Loss: 0.5025244355201721            | Epoch Loss: 0.5222180122914521            
Epoch: #40   | Batch: #24   | Batch Loss: 0.47996169328689575           | Epoch Loss: 0.520457332332929             
Epoch: #40   | Batch: #25   | Batch Loss: 0.4975455105304718            | Epoch Loss: 0.5195408594608307            
Epoch: #40   | Batch: #26   | Batch Loss: 0.4712052643299103            | Epoch Loss: 0.5176817981096414            
Epoch: #40   | Batch: #27   | Batch Loss: 0.49266940355300903           | Epoch Loss: 0.5167554131260624            
Epoch: #40   | Batch: #28   | Batch Loss: 0.3956699073314667            | Epoch Loss: 0.5124309307762555            
Epoch: #40   | Batch: #29   | Batch Loss: 0.4217480719089508            | Epoch Loss: 0.5093039356429001            
Epoch: #40   | Batch: #30   | Batch Loss: 0.4695234000682831            | Epoch Loss: 0.5079779177904129            
Epoch: #40   | Batch: #31   | Batch Loss: 0.5171633362770081            | Epoch Loss: 0.5082742216125611            
Epoch: #40   | Batch: #32   | Batch Loss: 0.42886000871658325           | Epoch Loss: 0.5057925274595618            
Epoch: #40   | Batch: #33   | Batch Loss: 0.567371666431427             | Epoch Loss: 0.5076585619738607            
Epoch: #40   | Batch: #34   | Batch Loss: 0.4665178954601288            | Epoch Loss: 0.5064485423705157            
Epoch: #40   | Batch: #35   | Batch Loss: 0.4548749327659607            | Epoch Loss: 0.5049750106675285            
Epoch: #40   | Batch: #36   | Batch Loss: 0.4840642511844635            | Epoch Loss: 0.5043941562374433            
Epoch: #40   | Batch: #37   | Batch Loss: 0.566596269607544             | Epoch Loss: 0.5060752944366352            
Epoch: #40   | Batch: #38   | Batch Loss: 0.6402453184127808            | Epoch Loss: 0.5096060845412707            
Epoch: #40   | Batch: #39   | Batch Loss: 0.5209417939186096            | Epoch Loss: 0.5098967437560742            
Epoch: #40   | Batch: #40   | Batch Loss: 0.5514107346534729            | Epoch Loss: 0.5109345935285091            
Epoch: #40   | Batch: #41   | Batch Loss: 0.459739625453949             | Epoch Loss: 0.5096859357705931            
Epoch: #40   | Batch: #42   | Batch Loss: 0.6434069275856018            | Epoch Loss: 0.5128697689090457            
Epoch: #40   | Batch: #43   | Batch Loss: 0.5257676243782043            | Epoch Loss: 0.5131697190362353            

Classifier Validation Epoch #40
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8272727272727273            
Batch: #2    | Top-1 Accuracy: 0.8295454545454546            
Batch: #3    | Top-1 Accuracy: 0.8272727272727273            
Batch: #4    | Top-1 Accuracy: 0.8420454545454545            
Batch: #5    | Top-1 Accuracy: 0.8400000000000001            
Batch: #6    | Top-1 Accuracy: 0.8416666666666667            
Batch: #7    | Top-1 Accuracy: 0.838961038961039             
Batch: #8    | Top-1 Accuracy: 0.8477272727272727            
Batch: #9    | Top-1 Accuracy: 0.8494949494949494            
Batch: #10   | Top-1 Accuracy: 0.849090909090909             
Batch: #11   | Top-1 Accuracy: 0.8520661157024793            
Batch: #12   | Top-1 Accuracy: 0.8511363636363636            
Batch: #13   | Top-1 Accuracy: 0.8499999999999999            
Batch: #14   | Top-1 Accuracy: 0.8483766233766233            
Batch: #15   | Top-1 Accuracy: 0.8481818181818181            
Batch: #16   | Top-1 Accuracy: 0.8465909090909092            
Batch: #17   | Top-1 Accuracy: 0.8462566844919788            

Classifier Training Epoch #41
------------------------------------------
Epoch: #41   | Batch: #1    | Batch Loss: 0.6157140731811523            | Epoch Loss: 0.6157140731811523            
Epoch: #41   | Batch: #2    | Batch Loss: 0.5485685467720032            | Epoch Loss: 0.5821413099765778            
Epoch: #41   | Batch: #3    | Batch Loss: 0.5620205402374268            | Epoch Loss: 0.5754343867301941            
Epoch: #41   | Batch: #4    | Batch Loss: 0.5232277512550354            | Epoch Loss: 0.5623827278614044            
Epoch: #41   | Batch: #5    | Batch Loss: 0.590887725353241             | Epoch Loss: 0.5680837273597718            
Epoch: #41   | Batch: #6    | Batch Loss: 0.5468744039535522            | Epoch Loss: 0.5645488401254019            
Epoch: #41   | Batch: #7    | Batch Loss: 0.6356045007705688            | Epoch Loss: 0.5746996487889972            
Epoch: #41   | Batch: #8    | Batch Loss: 0.5701776146888733            | Epoch Loss: 0.5741343945264816            
Epoch: #41   | Batch: #9    | Batch Loss: 0.44014790654182434           | Epoch Loss: 0.5592470069726309            
Epoch: #41   | Batch: #10   | Batch Loss: 0.5958593487739563            | Epoch Loss: 0.5629082411527634            
Epoch: #41   | Batch: #11   | Batch Loss: 0.46276015043258667           | Epoch Loss: 0.553803869269111             
Epoch: #41   | Batch: #12   | Batch Loss: 0.41127195954322815           | Epoch Loss: 0.5419262101252874            
Epoch: #41   | Batch: #13   | Batch Loss: 0.5599258542060852            | Epoch Loss: 0.5433107981315026            
Epoch: #41   | Batch: #14   | Batch Loss: 0.37072160840034485           | Epoch Loss: 0.5309829988649913            
Epoch: #41   | Batch: #15   | Batch Loss: 0.5784410238265991            | Epoch Loss: 0.5341468671957652            
Epoch: #41   | Batch: #16   | Batch Loss: 0.40730687975883484           | Epoch Loss: 0.526219367980957             
Epoch: #41   | Batch: #17   | Batch Loss: 0.4945101737976074            | Epoch Loss: 0.5243541212642894            
Epoch: #41   | Batch: #18   | Batch Loss: 0.5180246233940125            | Epoch Loss: 0.5240024824937185            
Epoch: #41   | Batch: #19   | Batch Loss: 0.578220009803772             | Epoch Loss: 0.5268560365626687            
Epoch: #41   | Batch: #20   | Batch Loss: 0.5776561498641968            | Epoch Loss: 0.5293960422277451            
Epoch: #41   | Batch: #21   | Batch Loss: 0.572641134262085             | Epoch Loss: 0.5314553323246184            
Epoch: #41   | Batch: #22   | Batch Loss: 0.5253099203109741            | Epoch Loss: 0.5311759954149072            
Epoch: #41   | Batch: #23   | Batch Loss: 0.45166438817977905           | Epoch Loss: 0.52771896901338              
Epoch: #41   | Batch: #24   | Batch Loss: 0.620415449142456             | Epoch Loss: 0.5315813223520914            
Epoch: #41   | Batch: #25   | Batch Loss: 0.5091996788978577            | Epoch Loss: 0.5306860566139221            
Epoch: #41   | Batch: #26   | Batch Loss: 0.517153263092041             | Epoch Loss: 0.5301655645553882            
Epoch: #41   | Batch: #27   | Batch Loss: 0.4835173189640045            | Epoch Loss: 0.5284378517557073            
Epoch: #41   | Batch: #28   | Batch Loss: 0.6150527000427246            | Epoch Loss: 0.5315312391945294            
Epoch: #41   | Batch: #29   | Batch Loss: 0.4866870045661926            | Epoch Loss: 0.5299848862763109            
Epoch: #41   | Batch: #30   | Batch Loss: 0.5161150693893433            | Epoch Loss: 0.5295225590467453            
Epoch: #41   | Batch: #31   | Batch Loss: 0.49743467569351196           | Epoch Loss: 0.5284874660353507            
Epoch: #41   | Batch: #32   | Batch Loss: 0.5446014404296875            | Epoch Loss: 0.5289910277351737            
Epoch: #41   | Batch: #33   | Batch Loss: 0.48553451895713806           | Epoch Loss: 0.527674163832809             
Epoch: #41   | Batch: #34   | Batch Loss: 0.6291638016700745            | Epoch Loss: 0.5306591531809639            
Epoch: #41   | Batch: #35   | Batch Loss: 0.4749510884284973            | Epoch Loss: 0.5290674941880362            
Epoch: #41   | Batch: #36   | Batch Loss: 0.5696738362312317            | Epoch Loss: 0.5301954481336806            
Epoch: #41   | Batch: #37   | Batch Loss: 0.5133088231086731            | Epoch Loss: 0.5297390528627344            
Epoch: #41   | Batch: #38   | Batch Loss: 0.5691956877708435            | Epoch Loss: 0.5307773853603163            
Epoch: #41   | Batch: #39   | Batch Loss: 0.4499724507331848            | Epoch Loss: 0.5287054639596206            
Epoch: #41   | Batch: #40   | Batch Loss: 0.562425971031189             | Epoch Loss: 0.5295484766364098            
Epoch: #41   | Batch: #41   | Batch Loss: 0.5209856033325195            | Epoch Loss: 0.5293396260680222            
Epoch: #41   | Batch: #42   | Batch Loss: 0.4569988250732422            | Epoch Loss: 0.5276172260443369            
Epoch: #41   | Batch: #43   | Batch Loss: 0.5412629842758179            | Epoch Loss: 0.5279345692590226            

Classifier Validation Epoch #41
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8272727272727273            
Batch: #2    | Top-1 Accuracy: 0.8227272727272728            
Batch: #3    | Top-1 Accuracy: 0.8393939393939394            
Batch: #4    | Top-1 Accuracy: 0.8534090909090909            
Batch: #5    | Top-1 Accuracy: 0.850909090909091             
Batch: #6    | Top-1 Accuracy: 0.8507575757575757            
Batch: #7    | Top-1 Accuracy: 0.8493506493506493            
Batch: #8    | Top-1 Accuracy: 0.8494318181818181            
Batch: #9    | Top-1 Accuracy: 0.85                          
Batch: #10   | Top-1 Accuracy: 0.8454545454545453            
Batch: #11   | Top-1 Accuracy: 0.8400826446280991            
Batch: #12   | Top-1 Accuracy: 0.8382575757575758            
Batch: #13   | Top-1 Accuracy: 0.8391608391608391            
Batch: #14   | Top-1 Accuracy: 0.8405844155844155            
Batch: #15   | Top-1 Accuracy: 0.842121212121212             
Batch: #16   | Top-1 Accuracy: 0.8411931818181818            
Batch: #17   | Top-1 Accuracy: 0.8409090909090908            

Classifier Training Epoch #42
------------------------------------------
Epoch: #42   | Batch: #1    | Batch Loss: 0.5698787569999695            | Epoch Loss: 0.5698787569999695            
Epoch: #42   | Batch: #2    | Batch Loss: 0.5302684307098389            | Epoch Loss: 0.5500735938549042            
Epoch: #42   | Batch: #3    | Batch Loss: 0.5817058682441711            | Epoch Loss: 0.5606176853179932            
Epoch: #42   | Batch: #4    | Batch Loss: 0.5221557021141052            | Epoch Loss: 0.5510021895170212            
Epoch: #42   | Batch: #5    | Batch Loss: 0.4315182566642761            | Epoch Loss: 0.5271054029464721            
Epoch: #42   | Batch: #6    | Batch Loss: 0.6765373349189758            | Epoch Loss: 0.5520107249418894            
Epoch: #42   | Batch: #7    | Batch Loss: 0.6686456799507141            | Epoch Loss: 0.5686728613717216            
Epoch: #42   | Batch: #8    | Batch Loss: 0.40367111563682556           | Epoch Loss: 0.5480476431548595            
Epoch: #42   | Batch: #9    | Batch Loss: 0.5029778480529785            | Epoch Loss: 0.5430398881435394            
Epoch: #42   | Batch: #10   | Batch Loss: 0.46110981702804565           | Epoch Loss: 0.5348468810319901            
Epoch: #42   | Batch: #11   | Batch Loss: 0.465074360370636             | Epoch Loss: 0.5285039246082306            
Epoch: #42   | Batch: #12   | Batch Loss: 0.5597169995307922            | Epoch Loss: 0.5311050141851107            
Epoch: #42   | Batch: #13   | Batch Loss: 0.5899433493614197            | Epoch Loss: 0.5356310399679037            
Epoch: #42   | Batch: #14   | Batch Loss: 0.5691553950309753            | Epoch Loss: 0.5380256367581231            
Epoch: #42   | Batch: #15   | Batch Loss: 0.6851325035095215            | Epoch Loss: 0.5478327612082163            
Epoch: #42   | Batch: #16   | Batch Loss: 0.38300472497940063           | Epoch Loss: 0.5375310089439154            
Epoch: #42   | Batch: #17   | Batch Loss: 0.5671474933624268            | Epoch Loss: 0.5392731550861808            
Epoch: #42   | Batch: #18   | Batch Loss: 0.44643473625183105           | Epoch Loss: 0.5341154651509391            
Epoch: #42   | Batch: #19   | Batch Loss: 0.4320867657661438            | Epoch Loss: 0.5287455336043709            
Epoch: #42   | Batch: #20   | Batch Loss: 0.5126978754997253            | Epoch Loss: 0.5279431506991387            
Epoch: #42   | Batch: #21   | Batch Loss: 0.5108679533004761            | Epoch Loss: 0.5271300460611071            
Epoch: #42   | Batch: #22   | Batch Loss: 0.5317040085792542            | Epoch Loss: 0.5273379534482956            
Epoch: #42   | Batch: #23   | Batch Loss: 0.4992486536502838            | Epoch Loss: 0.5261166795440342            
Epoch: #42   | Batch: #24   | Batch Loss: 0.43346941471099854           | Epoch Loss: 0.5222563768426577            
Epoch: #42   | Batch: #25   | Batch Loss: 0.45068758726119995           | Epoch Loss: 0.5193936252593994            
Epoch: #42   | Batch: #26   | Batch Loss: 0.5777966976165771            | Epoch Loss: 0.521639897273137             
Epoch: #42   | Batch: #27   | Batch Loss: 0.47285765409469604           | Epoch Loss: 0.5198331475257874            
Epoch: #42   | Batch: #28   | Batch Loss: 0.6117453575134277            | Epoch Loss: 0.5231157264539174            
Epoch: #42   | Batch: #29   | Batch Loss: 0.6026806235313416            | Epoch Loss: 0.5258593435945182            
Epoch: #42   | Batch: #30   | Batch Loss: 0.46814605593681335           | Epoch Loss: 0.5239355673392614            
Epoch: #42   | Batch: #31   | Batch Loss: 0.4410533905029297            | Epoch Loss: 0.5212619487316378            
Epoch: #42   | Batch: #32   | Batch Loss: 0.5706745386123657            | Epoch Loss: 0.5228060921654105            
Epoch: #42   | Batch: #33   | Batch Loss: 0.457562118768692             | Epoch Loss: 0.5208290020624796            
Epoch: #42   | Batch: #34   | Batch Loss: 0.47126996517181396           | Epoch Loss: 0.5193713833304012            
Epoch: #42   | Batch: #35   | Batch Loss: 0.5408920049667358            | Epoch Loss: 0.5199862582342966            
Epoch: #42   | Batch: #36   | Batch Loss: 0.6951208710670471            | Epoch Loss: 0.5248511085907618            
Epoch: #42   | Batch: #37   | Batch Loss: 0.4854409396648407            | Epoch Loss: 0.5237859688900612            
Epoch: #42   | Batch: #38   | Batch Loss: 0.5426271557807922            | Epoch Loss: 0.5242817895977121            
Epoch: #42   | Batch: #39   | Batch Loss: 0.3543328642845154            | Epoch Loss: 0.5199241248460916            
Epoch: #42   | Batch: #40   | Batch Loss: 0.4923142194747925            | Epoch Loss: 0.5192338772118091            
Epoch: #42   | Batch: #41   | Batch Loss: 0.5632936358451843            | Epoch Loss: 0.5203085054711598            
Epoch: #42   | Batch: #42   | Batch Loss: 0.5516985654830933            | Epoch Loss: 0.5210558878523963            
Epoch: #42   | Batch: #43   | Batch Loss: 0.5426139235496521            | Epoch Loss: 0.5215572375197743            

Classifier Validation Epoch #42
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8409090909090909            
Batch: #2    | Top-1 Accuracy: 0.8431818181818183            
Batch: #3    | Top-1 Accuracy: 0.8500000000000001            
Batch: #4    | Top-1 Accuracy: 0.8420454545454547            
Batch: #5    | Top-1 Accuracy: 0.840909090909091             
Batch: #6    | Top-1 Accuracy: 0.8333333333333334            
Batch: #7    | Top-1 Accuracy: 0.8357142857142856            
Batch: #8    | Top-1 Accuracy: 0.8323863636363636            
Batch: #9    | Top-1 Accuracy: 0.8383838383838385            
Batch: #10   | Top-1 Accuracy: 0.8431818181818181            
Batch: #11   | Top-1 Accuracy: 0.8442148760330578            
Batch: #12   | Top-1 Accuracy: 0.8420454545454544            
Batch: #13   | Top-1 Accuracy: 0.8416083916083915            
Batch: #14   | Top-1 Accuracy: 0.843181818181818             
Batch: #15   | Top-1 Accuracy: 0.8445454545454544            
Batch: #16   | Top-1 Accuracy: 0.84375                       
Batch: #17   | Top-1 Accuracy: 0.8435828877005348            

Classifier Training Epoch #43
------------------------------------------
Epoch: #43   | Batch: #1    | Batch Loss: 0.4869796633720398            | Epoch Loss: 0.4869796633720398            
Epoch: #43   | Batch: #2    | Batch Loss: 0.4439714550971985            | Epoch Loss: 0.46547555923461914           
Epoch: #43   | Batch: #3    | Batch Loss: 0.5219789743423462            | Epoch Loss: 0.4843100309371948            
Epoch: #43   | Batch: #4    | Batch Loss: 0.62371826171875              | Epoch Loss: 0.5191620886325836            
Epoch: #43   | Batch: #5    | Batch Loss: 0.525434672832489             | Epoch Loss: 0.5204166054725647            
Epoch: #43   | Batch: #6    | Batch Loss: 0.466813325881958             | Epoch Loss: 0.5114827255407969            
Epoch: #43   | Batch: #7    | Batch Loss: 0.4487791955471039            | Epoch Loss: 0.5025250783988408            
Epoch: #43   | Batch: #8    | Batch Loss: 0.4814966022968292            | Epoch Loss: 0.4998965188860893            
Epoch: #43   | Batch: #9    | Batch Loss: 0.5612586140632629            | Epoch Loss: 0.5067145294613309            
Epoch: #43   | Batch: #10   | Batch Loss: 0.40914931893348694           | Epoch Loss: 0.4969580084085464            
Epoch: #43   | Batch: #11   | Batch Loss: 0.5459955930709839            | Epoch Loss: 0.5014159706505862            
Epoch: #43   | Batch: #12   | Batch Loss: 0.522679328918457             | Epoch Loss: 0.5031879171729088            
Epoch: #43   | Batch: #13   | Batch Loss: 0.5574575662612915            | Epoch Loss: 0.5073625055643228            
Epoch: #43   | Batch: #14   | Batch Loss: 0.5456196069717407            | Epoch Loss: 0.5100951556648526            
Epoch: #43   | Batch: #15   | Batch Loss: 0.4750487804412842            | Epoch Loss: 0.5077587306499481            
Epoch: #43   | Batch: #16   | Batch Loss: 0.4982454776763916            | Epoch Loss: 0.5071641523391008            
Epoch: #43   | Batch: #17   | Batch Loss: 0.44235900044441223           | Epoch Loss: 0.5033520845805898            
Epoch: #43   | Batch: #18   | Batch Loss: 0.6310195922851562            | Epoch Loss: 0.51044472389751              
Epoch: #43   | Batch: #19   | Batch Loss: 0.5502843260765076            | Epoch Loss: 0.5125415450648257            
Epoch: #43   | Batch: #20   | Batch Loss: 0.47887691855430603           | Epoch Loss: 0.5108583137392998            
Epoch: #43   | Batch: #21   | Batch Loss: 0.46847739815711975           | Epoch Loss: 0.5088401749020531            
Epoch: #43   | Batch: #22   | Batch Loss: 0.5120657682418823            | Epoch Loss: 0.5089867927811362            
Epoch: #43   | Batch: #23   | Batch Loss: 0.5530403256416321            | Epoch Loss: 0.5109021637750708            
Epoch: #43   | Batch: #24   | Batch Loss: 0.5345417261123657            | Epoch Loss: 0.5118871455391248            
Epoch: #43   | Batch: #25   | Batch Loss: 0.5254243612289429            | Epoch Loss: 0.5124286341667176            
Epoch: #43   | Batch: #26   | Batch Loss: 0.5129714012145996            | Epoch Loss: 0.5124495098224053            
Epoch: #43   | Batch: #27   | Batch Loss: 0.4437108337879181            | Epoch Loss: 0.5099036329322391            
Epoch: #43   | Batch: #28   | Batch Loss: 0.5475155711174011            | Epoch Loss: 0.511246916438852             
Epoch: #43   | Batch: #29   | Batch Loss: 0.4435323178768158            | Epoch Loss: 0.5089119302815405            
Epoch: #43   | Batch: #30   | Batch Loss: 0.36944833397865295           | Epoch Loss: 0.5042631437381109            
Epoch: #43   | Batch: #31   | Batch Loss: 0.7499197721481323            | Epoch Loss: 0.5121875511061761            
Epoch: #43   | Batch: #32   | Batch Loss: 0.5458640456199646            | Epoch Loss: 0.513239941559732             
Epoch: #43   | Batch: #33   | Batch Loss: 0.47057369351387024           | Epoch Loss: 0.5119470249522816            
Epoch: #43   | Batch: #34   | Batch Loss: 0.49592235684394836           | Epoch Loss: 0.5114757111843895            
Epoch: #43   | Batch: #35   | Batch Loss: 0.47047895193099976           | Epoch Loss: 0.5103043752057211            
Epoch: #43   | Batch: #36   | Batch Loss: 0.574479877948761             | Epoch Loss: 0.5120870280596945            
Epoch: #43   | Batch: #37   | Batch Loss: 0.5525867938995361            | Epoch Loss: 0.5131816163256362            
Epoch: #43   | Batch: #38   | Batch Loss: 0.5392330884933472            | Epoch Loss: 0.5138671813826812            
Epoch: #43   | Batch: #39   | Batch Loss: 0.546057939529419             | Epoch Loss: 0.5146925854377258            
Epoch: #43   | Batch: #40   | Batch Loss: 0.43100693821907043           | Epoch Loss: 0.5126004442572594            
Epoch: #43   | Batch: #41   | Batch Loss: 0.5159559845924377            | Epoch Loss: 0.5126822867044588            
Epoch: #43   | Batch: #42   | Batch Loss: 0.45346298813819885           | Epoch Loss: 0.5112723034052622            
Epoch: #43   | Batch: #43   | Batch Loss: 0.49775660037994385           | Epoch Loss: 0.5109579847302548            

Classifier Validation Epoch #43
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8409090909090909            
Batch: #2    | Top-1 Accuracy: 0.865909090909091             
Batch: #3    | Top-1 Accuracy: 0.8439393939393941            
Batch: #4    | Top-1 Accuracy: 0.8431818181818183            
Batch: #5    | Top-1 Accuracy: 0.8400000000000001            
Batch: #6    | Top-1 Accuracy: 0.8401515151515152            
Batch: #7    | Top-1 Accuracy: 0.8318181818181819            
Batch: #8    | Top-1 Accuracy: 0.8318181818181819            
Batch: #9    | Top-1 Accuracy: 0.8404040404040405            
Batch: #10   | Top-1 Accuracy: 0.8413636363636364            
Batch: #11   | Top-1 Accuracy: 0.8417355371900826            
Batch: #12   | Top-1 Accuracy: 0.8420454545454547            
Batch: #13   | Top-1 Accuracy: 0.8444055944055945            
Batch: #14   | Top-1 Accuracy: 0.8451298701298702            
Batch: #15   | Top-1 Accuracy: 0.8454545454545455            
Batch: #16   | Top-1 Accuracy: 0.8482954545454545            
Batch: #17   | Top-1 Accuracy: 0.8470588235294118            

Classifier Training Epoch #44
------------------------------------------
Epoch: #44   | Batch: #1    | Batch Loss: 0.4477875530719757            | Epoch Loss: 0.4477875530719757            
Epoch: #44   | Batch: #2    | Batch Loss: 0.628445565700531             | Epoch Loss: 0.5381165593862534            
Epoch: #44   | Batch: #3    | Batch Loss: 0.42371195554733276           | Epoch Loss: 0.4999816914399465            
Epoch: #44   | Batch: #4    | Batch Loss: 0.41779398918151855           | Epoch Loss: 0.4794347658753395            
Epoch: #44   | Batch: #5    | Batch Loss: 0.45711663365364075           | Epoch Loss: 0.4749711394309998            
Epoch: #44   | Batch: #6    | Batch Loss: 0.44511058926582336           | Epoch Loss: 0.469994381070137             
Epoch: #44   | Batch: #7    | Batch Loss: 0.42466190457344055           | Epoch Loss: 0.4635183129991804            
Epoch: #44   | Batch: #8    | Batch Loss: 0.43701618909835815           | Epoch Loss: 0.4602055475115776            
Epoch: #44   | Batch: #9    | Batch Loss: 0.43209779262542725           | Epoch Loss: 0.45708246363533866           
Epoch: #44   | Batch: #10   | Batch Loss: 0.5014153718948364            | Epoch Loss: 0.46151575446128845           
Epoch: #44   | Batch: #11   | Batch Loss: 0.4441661834716797            | Epoch Loss: 0.4599385207349604            
Epoch: #44   | Batch: #12   | Batch Loss: 0.5771247148513794            | Epoch Loss: 0.4697040369113286            
Epoch: #44   | Batch: #13   | Batch Loss: 0.5710323452949524            | Epoch Loss: 0.4774985221716074            
Epoch: #44   | Batch: #14   | Batch Loss: 0.4055696725845337            | Epoch Loss: 0.4723607472011021            
Epoch: #44   | Batch: #15   | Batch Loss: 0.5186781883239746            | Epoch Loss: 0.4754485766092936            
Epoch: #44   | Batch: #16   | Batch Loss: 0.40285563468933105           | Epoch Loss: 0.47091151773929596           
Epoch: #44   | Batch: #17   | Batch Loss: 0.423854798078537             | Epoch Loss: 0.46814347540631013           
Epoch: #44   | Batch: #18   | Batch Loss: 0.46647053956985474           | Epoch Loss: 0.4680505345265071            
Epoch: #44   | Batch: #19   | Batch Loss: 0.5105782747268677            | Epoch Loss: 0.4702888366423155            
Epoch: #44   | Batch: #20   | Batch Loss: 0.44846096634864807           | Epoch Loss: 0.4691974431276321            
Epoch: #44   | Batch: #21   | Batch Loss: 0.5956386923789978            | Epoch Loss: 0.4752184549967448            
Epoch: #44   | Batch: #22   | Batch Loss: 0.5317783951759338            | Epoch Loss: 0.4777893613685261            
Epoch: #44   | Batch: #23   | Batch Loss: 0.5784092545509338            | Epoch Loss: 0.48216413933297864           
Epoch: #44   | Batch: #24   | Batch Loss: 0.5689772963523865            | Epoch Loss: 0.48578135420878726           
Epoch: #44   | Batch: #25   | Batch Loss: 0.5297228097915649            | Epoch Loss: 0.4875390124320984            
Epoch: #44   | Batch: #26   | Batch Loss: 0.5832123756408691            | Epoch Loss: 0.49121875717089725           
Epoch: #44   | Batch: #27   | Batch Loss: 0.6500645875930786            | Epoch Loss: 0.4971019360754225            
Epoch: #44   | Batch: #28   | Batch Loss: 0.459139883518219             | Epoch Loss: 0.4957461484840938            
Epoch: #44   | Batch: #29   | Batch Loss: 0.45465022325515747           | Epoch Loss: 0.4943290476141305            
Epoch: #44   | Batch: #30   | Batch Loss: 0.565558910369873             | Epoch Loss: 0.4967033763726552            
Epoch: #44   | Batch: #31   | Batch Loss: 0.640533983707428             | Epoch Loss: 0.5013430733834544            
Epoch: #44   | Batch: #32   | Batch Loss: 0.5114189386367798            | Epoch Loss: 0.5016579441726208            
Epoch: #44   | Batch: #33   | Batch Loss: 0.5072605013847351            | Epoch Loss: 0.501827718633594             
Epoch: #44   | Batch: #34   | Batch Loss: 0.4627126157283783            | Epoch Loss: 0.5006772744304994            
Epoch: #44   | Batch: #35   | Batch Loss: 0.5816552042961121            | Epoch Loss: 0.5029909295695169            
Epoch: #44   | Batch: #36   | Batch Loss: 0.5350255370140076            | Epoch Loss: 0.5038807797763083            
Epoch: #44   | Batch: #37   | Batch Loss: 0.6203645467758179            | Epoch Loss: 0.5070289896952139            
Epoch: #44   | Batch: #38   | Batch Loss: 0.4990275502204895            | Epoch Loss: 0.5068184254985106            
Epoch: #44   | Batch: #39   | Batch Loss: 0.5125963091850281            | Epoch Loss: 0.5069665763622675            
Epoch: #44   | Batch: #40   | Batch Loss: 0.44522538781166077           | Epoch Loss: 0.5054230466485023            
Epoch: #44   | Batch: #41   | Batch Loss: 0.5271188616752625            | Epoch Loss: 0.5059522128686672            
Epoch: #44   | Batch: #42   | Batch Loss: 0.5779866576194763            | Epoch Loss: 0.5076673186960674            
Epoch: #44   | Batch: #43   | Batch Loss: 0.4031992554664612            | Epoch Loss: 0.5052378288535184            

Classifier Validation Epoch #44
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8818181818181818            
Batch: #2    | Top-1 Accuracy: 0.8545454545454545            
Batch: #3    | Top-1 Accuracy: 0.8484848484848485            
Batch: #4    | Top-1 Accuracy: 0.8431818181818181            
Batch: #5    | Top-1 Accuracy: 0.8509090909090908            
Batch: #6    | Top-1 Accuracy: 0.85                          
Batch: #7    | Top-1 Accuracy: 0.8493506493506493            
Batch: #8    | Top-1 Accuracy: 0.85625                       
Batch: #9    | Top-1 Accuracy: 0.8535353535353535            
Batch: #10   | Top-1 Accuracy: 0.8550000000000001            
Batch: #11   | Top-1 Accuracy: 0.8516528925619835            
Batch: #12   | Top-1 Accuracy: 0.8488636363636365            
Batch: #13   | Top-1 Accuracy: 0.8444055944055945            
Batch: #14   | Top-1 Accuracy: 0.8425324675324676            
Batch: #15   | Top-1 Accuracy: 0.8445454545454546            
Batch: #16   | Top-1 Accuracy: 0.8420454545454545            
Batch: #17   | Top-1 Accuracy: 0.8398395721925134            

Classifier Training Epoch #45
------------------------------------------
Epoch: #45   | Batch: #1    | Batch Loss: 0.5770025253295898            | Epoch Loss: 0.5770025253295898            
Epoch: #45   | Batch: #2    | Batch Loss: 0.49488702416419983           | Epoch Loss: 0.5359447747468948            
Epoch: #45   | Batch: #3    | Batch Loss: 0.7285466194152832            | Epoch Loss: 0.6001453896363577            
Epoch: #45   | Batch: #4    | Batch Loss: 0.4787464737892151            | Epoch Loss: 0.569795660674572             
Epoch: #45   | Batch: #5    | Batch Loss: 0.48472005128860474           | Epoch Loss: 0.5527805387973785            
Epoch: #45   | Batch: #6    | Batch Loss: 0.4926649332046509            | Epoch Loss: 0.5427612711985906            
Epoch: #45   | Batch: #7    | Batch Loss: 0.5784024596214294            | Epoch Loss: 0.5478528695447105            
Epoch: #45   | Batch: #8    | Batch Loss: 0.5649315118789673            | Epoch Loss: 0.5499876998364925            
Epoch: #45   | Batch: #9    | Batch Loss: 0.4059443175792694            | Epoch Loss: 0.53398287958569              
Epoch: #45   | Batch: #10   | Batch Loss: 0.45885929465293884           | Epoch Loss: 0.5264705210924149            
Epoch: #45   | Batch: #11   | Batch Loss: 0.47051921486854553           | Epoch Loss: 0.5213840387084268            
Epoch: #45   | Batch: #12   | Batch Loss: 0.613090991973877             | Epoch Loss: 0.5290262848138809            
Epoch: #45   | Batch: #13   | Batch Loss: 0.4712730050086975            | Epoch Loss: 0.5245837248288668            
Epoch: #45   | Batch: #14   | Batch Loss: 0.4629017114639282            | Epoch Loss: 0.5201778667313712            
Epoch: #45   | Batch: #15   | Batch Loss: 0.493693083524704             | Epoch Loss: 0.5184122145175933            
Epoch: #45   | Batch: #16   | Batch Loss: 0.5301469564437866            | Epoch Loss: 0.5191456358879805            
Epoch: #45   | Batch: #17   | Batch Loss: 0.5309926271438599            | Epoch Loss: 0.5198425177265616            
Epoch: #45   | Batch: #18   | Batch Loss: 0.48976564407348633           | Epoch Loss: 0.5181715803013908            
Epoch: #45   | Batch: #19   | Batch Loss: 0.4632197618484497            | Epoch Loss: 0.5152793793301833            
Epoch: #45   | Batch: #20   | Batch Loss: 0.4425380825996399            | Epoch Loss: 0.5116423144936562            
Epoch: #45   | Batch: #21   | Batch Loss: 0.6858462691307068            | Epoch Loss: 0.5199377409049443            
Epoch: #45   | Batch: #22   | Batch Loss: 0.5037700533866882            | Epoch Loss: 0.5192028460177508            
Epoch: #45   | Batch: #23   | Batch Loss: 0.4248769283294678            | Epoch Loss: 0.5151017191617385            
Epoch: #45   | Batch: #24   | Batch Loss: 0.4649391770362854            | Epoch Loss: 0.5130116132398447            
Epoch: #45   | Batch: #25   | Batch Loss: 0.5007362365722656            | Epoch Loss: 0.5125205981731414            
Epoch: #45   | Batch: #26   | Batch Loss: 0.5429149866104126            | Epoch Loss: 0.5136896131130365            
Epoch: #45   | Batch: #27   | Batch Loss: 0.45573699474334717           | Epoch Loss: 0.5115432198400851            
Epoch: #45   | Batch: #28   | Batch Loss: 0.5589137673377991            | Epoch Loss: 0.5132350251078606            
Epoch: #45   | Batch: #29   | Batch Loss: 0.46590563654899597           | Epoch Loss: 0.5116029772265204            
Epoch: #45   | Batch: #30   | Batch Loss: 0.5161357522010803            | Epoch Loss: 0.5117540697256724            
Epoch: #45   | Batch: #31   | Batch Loss: 0.6186076998710632            | Epoch Loss: 0.515200961020685             
Epoch: #45   | Batch: #32   | Batch Loss: 0.4244992733001709            | Epoch Loss: 0.512366533279419             
Epoch: #45   | Batch: #33   | Batch Loss: 0.4907603859901428            | Epoch Loss: 0.5117118015433803            
Epoch: #45   | Batch: #34   | Batch Loss: 0.5124738812446594            | Epoch Loss: 0.5117342156522414            
Epoch: #45   | Batch: #35   | Batch Loss: 0.4526984393596649            | Epoch Loss: 0.5100474791867392            
Epoch: #45   | Batch: #36   | Batch Loss: 0.5679992437362671            | Epoch Loss: 0.5116572504242262            
Epoch: #45   | Batch: #37   | Batch Loss: 0.41316771507263184           | Epoch Loss: 0.5089953710903993            
Epoch: #45   | Batch: #38   | Batch Loss: 0.5209941267967224            | Epoch Loss: 0.509311127819513             
Epoch: #45   | Batch: #39   | Batch Loss: 0.5039323568344116            | Epoch Loss: 0.5091732106147668            
Epoch: #45   | Batch: #40   | Batch Loss: 0.6198030114173889            | Epoch Loss: 0.5119389556348324            
Epoch: #45   | Batch: #41   | Batch Loss: 0.4726124703884125            | Epoch Loss: 0.5109797730678465            
Epoch: #45   | Batch: #42   | Batch Loss: 0.4787871837615967            | Epoch Loss: 0.5102132828462691            
Epoch: #45   | Batch: #43   | Batch Loss: 0.6457882523536682            | Epoch Loss: 0.5133661891138831            

Classifier Validation Epoch #45
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8363636363636363            
Batch: #2    | Top-1 Accuracy: 0.8454545454545455            
Batch: #3    | Top-1 Accuracy: 0.8318181818181819            
Batch: #4    | Top-1 Accuracy: 0.8375                        
Batch: #5    | Top-1 Accuracy: 0.8327272727272728            
Batch: #6    | Top-1 Accuracy: 0.8272727272727273            
Batch: #7    | Top-1 Accuracy: 0.8344155844155844            
Batch: #8    | Top-1 Accuracy: 0.8403409090909091            
Batch: #9    | Top-1 Accuracy: 0.8318181818181818            
Batch: #10   | Top-1 Accuracy: 0.8318181818181818            
Batch: #11   | Top-1 Accuracy: 0.8338842975206611            
Batch: #12   | Top-1 Accuracy: 0.8318181818181817            
Batch: #13   | Top-1 Accuracy: 0.8321678321678321            
Batch: #14   | Top-1 Accuracy: 0.8347402597402596            
Batch: #15   | Top-1 Accuracy: 0.8327272727272725            
Batch: #16   | Top-1 Accuracy: 0.8318181818181818            
Batch: #17   | Top-1 Accuracy: 0.8331550802139037            

Classifier Training Epoch #46
------------------------------------------
Epoch: #46   | Batch: #1    | Batch Loss: 0.589545726776123             | Epoch Loss: 0.589545726776123             
Epoch: #46   | Batch: #2    | Batch Loss: 0.5926564931869507            | Epoch Loss: 0.5911011099815369            
Epoch: #46   | Batch: #3    | Batch Loss: 0.5821481943130493            | Epoch Loss: 0.5881168047587076            
Epoch: #46   | Batch: #4    | Batch Loss: 0.48811087012290955           | Epoch Loss: 0.5631153210997581            
Epoch: #46   | Batch: #5    | Batch Loss: 0.49909743666648865           | Epoch Loss: 0.5503117442131042            
Epoch: #46   | Batch: #6    | Batch Loss: 0.5297604203224182            | Epoch Loss: 0.5468865235646566            
Epoch: #46   | Batch: #7    | Batch Loss: 0.4329191744327545            | Epoch Loss: 0.5306054736886706            
Epoch: #46   | Batch: #8    | Batch Loss: 0.5326856374740601            | Epoch Loss: 0.5308654941618443            
Epoch: #46   | Batch: #9    | Batch Loss: 0.31481778621673584           | Epoch Loss: 0.5068601932790544            
Epoch: #46   | Batch: #10   | Batch Loss: 0.5993066430091858            | Epoch Loss: 0.5161048382520675            
Epoch: #46   | Batch: #11   | Batch Loss: 0.5406103730201721            | Epoch Loss: 0.518332614140077             
Epoch: #46   | Batch: #12   | Batch Loss: 0.428578644990921             | Epoch Loss: 0.5108531167109808            
Epoch: #46   | Batch: #13   | Batch Loss: 0.6092135310173035            | Epoch Loss: 0.5184193024268517            
Epoch: #46   | Batch: #14   | Batch Loss: 0.5763680338859558            | Epoch Loss: 0.5225584975310734            
Epoch: #46   | Batch: #15   | Batch Loss: 0.49356117844581604           | Epoch Loss: 0.5206253429253896            
Epoch: #46   | Batch: #16   | Batch Loss: 0.5500928163528442            | Epoch Loss: 0.5224670600146055            
Epoch: #46   | Batch: #17   | Batch Loss: 0.47760987281799316           | Epoch Loss: 0.5198284019442165            
Epoch: #46   | Batch: #18   | Batch Loss: 0.6046422123908997            | Epoch Loss: 0.5245402803023657            
Epoch: #46   | Batch: #19   | Batch Loss: 0.5257518887519836            | Epoch Loss: 0.524604049168135             
Epoch: #46   | Batch: #20   | Batch Loss: 0.41115322709083557           | Epoch Loss: 0.51893150806427              
Epoch: #46   | Batch: #21   | Batch Loss: 0.5270311832427979            | Epoch Loss: 0.5193172068822951            
Epoch: #46   | Batch: #22   | Batch Loss: 0.3925383388996124            | Epoch Loss: 0.5135545310649005            
Epoch: #46   | Batch: #23   | Batch Loss: 0.3518223166465759            | Epoch Loss: 0.5065226956554081            
Epoch: #46   | Batch: #24   | Batch Loss: 0.4673563539981842            | Epoch Loss: 0.5048907647530237            
Epoch: #46   | Batch: #25   | Batch Loss: 0.512047290802002             | Epoch Loss: 0.5051770257949829            
Epoch: #46   | Batch: #26   | Batch Loss: 0.42146557569503784           | Epoch Loss: 0.5019573546372927            
Epoch: #46   | Batch: #27   | Batch Loss: 0.412052184343338             | Epoch Loss: 0.4986275335152944            
Epoch: #46   | Batch: #28   | Batch Loss: 0.5561174750328064            | Epoch Loss: 0.5006807457123484            
Epoch: #46   | Batch: #29   | Batch Loss: 0.5322374105453491            | Epoch Loss: 0.5017689065686588            
Epoch: #46   | Batch: #30   | Batch Loss: 0.505540668964386             | Epoch Loss: 0.5018946319818497            
Epoch: #46   | Batch: #31   | Batch Loss: 0.5991095900535583            | Epoch Loss: 0.5050305983712596            
Epoch: #46   | Batch: #32   | Batch Loss: 0.5668825507164001            | Epoch Loss: 0.5069634718820453            
Epoch: #46   | Batch: #33   | Batch Loss: 0.34557726979255676           | Epoch Loss: 0.5020729809096365            
Epoch: #46   | Batch: #34   | Batch Loss: 0.46509331464767456           | Epoch Loss: 0.5009853436666376            
Epoch: #46   | Batch: #35   | Batch Loss: 0.6230567097663879            | Epoch Loss: 0.5044730969837734            
Epoch: #46   | Batch: #36   | Batch Loss: 0.5080629587173462            | Epoch Loss: 0.5045728153652616            
Epoch: #46   | Batch: #37   | Batch Loss: 0.5515641570091248            | Epoch Loss: 0.5058428516259065            
Epoch: #46   | Batch: #38   | Batch Loss: 0.5624170899391174            | Epoch Loss: 0.5073316473709909            
Epoch: #46   | Batch: #39   | Batch Loss: 0.4422788918018341            | Epoch Loss: 0.5056636279974228            
Epoch: #46   | Batch: #40   | Batch Loss: 0.5409014225006104            | Epoch Loss: 0.5065445728600025            
Epoch: #46   | Batch: #41   | Batch Loss: 0.6309079527854919            | Epoch Loss: 0.5095778260289169            
Epoch: #46   | Batch: #42   | Batch Loss: 0.5302718877792358            | Epoch Loss: 0.5100705417848769            
Epoch: #46   | Batch: #43   | Batch Loss: 0.4864210784435272            | Epoch Loss: 0.5095205542653106            

Classifier Validation Epoch #46
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8318181818181818            
Batch: #2    | Top-1 Accuracy: 0.825                         
Batch: #3    | Top-1 Accuracy: 0.8257575757575757            
Batch: #4    | Top-1 Accuracy: 0.8352272727272727            
Batch: #5    | Top-1 Accuracy: 0.8336363636363636            
Batch: #6    | Top-1 Accuracy: 0.831060606060606             
Batch: #7    | Top-1 Accuracy: 0.8285714285714285            
Batch: #8    | Top-1 Accuracy: 0.83125                       
Batch: #9    | Top-1 Accuracy: 0.8323232323232324            
Batch: #10   | Top-1 Accuracy: 0.8309090909090908            
Batch: #11   | Top-1 Accuracy: 0.8338842975206611            
Batch: #12   | Top-1 Accuracy: 0.8329545454545454            
Batch: #13   | Top-1 Accuracy: 0.8318181818181818            
Batch: #14   | Top-1 Accuracy: 0.8318181818181818            
Batch: #15   | Top-1 Accuracy: 0.8339393939393939            
Batch: #16   | Top-1 Accuracy: 0.8338068181818182            
Batch: #17   | Top-1 Accuracy: 0.8331550802139037            

Classifier Training Epoch #47
------------------------------------------
Epoch: #47   | Batch: #1    | Batch Loss: 0.5863888263702393            | Epoch Loss: 0.5863888263702393            
Epoch: #47   | Batch: #2    | Batch Loss: 0.5617223978042603            | Epoch Loss: 0.5740556120872498            
Epoch: #47   | Batch: #3    | Batch Loss: 0.47436094284057617           | Epoch Loss: 0.5408240556716919            
Epoch: #47   | Batch: #4    | Batch Loss: 0.45166122913360596           | Epoch Loss: 0.5185333490371704            
Epoch: #47   | Batch: #5    | Batch Loss: 0.4737411439418793            | Epoch Loss: 0.5095749080181122            
Epoch: #47   | Batch: #6    | Batch Loss: 0.5906338095664978            | Epoch Loss: 0.5230847249428431            
Epoch: #47   | Batch: #7    | Batch Loss: 0.5471129417419434            | Epoch Loss: 0.5265173273427146            
Epoch: #47   | Batch: #8    | Batch Loss: 0.45251768827438354           | Epoch Loss: 0.5172673724591732            
Epoch: #47   | Batch: #9    | Batch Loss: 0.4741280674934387            | Epoch Loss: 0.5124741163518693            
Epoch: #47   | Batch: #10   | Batch Loss: 0.5084060430526733            | Epoch Loss: 0.5120673090219497            
Epoch: #47   | Batch: #11   | Batch Loss: 0.5215686559677124            | Epoch Loss: 0.5129310678352009            
Epoch: #47   | Batch: #12   | Batch Loss: 0.45222362875938416           | Epoch Loss: 0.5078721145788828            
Epoch: #47   | Batch: #13   | Batch Loss: 0.5141720771789551            | Epoch Loss: 0.5083567270865808            
Epoch: #47   | Batch: #14   | Batch Loss: 0.4669797122478485            | Epoch Loss: 0.5054012260266713            
Epoch: #47   | Batch: #15   | Batch Loss: 0.5091254711151123            | Epoch Loss: 0.5056495090325673            
Epoch: #47   | Batch: #16   | Batch Loss: 0.5248291492462158            | Epoch Loss: 0.5068482365459204            
Epoch: #47   | Batch: #17   | Batch Loss: 0.496919184923172             | Epoch Loss: 0.5062641746857587            
Epoch: #47   | Batch: #18   | Batch Loss: 0.6008741855621338            | Epoch Loss: 0.5115202864011129            
Epoch: #47   | Batch: #19   | Batch Loss: 0.4987183213233948            | Epoch Loss: 0.5108464987654435            
Epoch: #47   | Batch: #20   | Batch Loss: 0.4959442913532257            | Epoch Loss: 0.5101013883948327            
Epoch: #47   | Batch: #21   | Batch Loss: 0.493635356426239             | Epoch Loss: 0.5093172916344234            
Epoch: #47   | Batch: #22   | Batch Loss: 0.4830056130886078            | Epoch Loss: 0.5081213062459772            
Epoch: #47   | Batch: #23   | Batch Loss: 0.41641324758529663           | Epoch Loss: 0.5041339993476868            
Epoch: #47   | Batch: #24   | Batch Loss: 0.5321251153945923            | Epoch Loss: 0.5053002958496412            
Epoch: #47   | Batch: #25   | Batch Loss: 0.607666552066803             | Epoch Loss: 0.5093949460983276            
Epoch: #47   | Batch: #26   | Batch Loss: 0.4464836120605469            | Epoch Loss: 0.5069752794045669            
Epoch: #47   | Batch: #27   | Batch Loss: 0.481250137090683             | Epoch Loss: 0.5060224963559045            
Epoch: #47   | Batch: #28   | Batch Loss: 0.48078417778015137           | Epoch Loss: 0.5051211278353419            
Epoch: #47   | Batch: #29   | Batch Loss: 0.5127878189086914            | Epoch Loss: 0.5053854964930435            
Epoch: #47   | Batch: #30   | Batch Loss: 0.46925973892211914           | Epoch Loss: 0.5041813045740128            
Epoch: #47   | Batch: #31   | Batch Loss: 0.5604945421218872            | Epoch Loss: 0.5059978606239441            
Epoch: #47   | Batch: #32   | Batch Loss: 0.7436753511428833            | Epoch Loss: 0.513425282202661             
Epoch: #47   | Batch: #33   | Batch Loss: 0.6010931134223938            | Epoch Loss: 0.5160818831487135            
Epoch: #47   | Batch: #34   | Batch Loss: 0.442380428314209             | Epoch Loss: 0.5139141933006399            
Epoch: #47   | Batch: #35   | Batch Loss: 0.595496416091919             | Epoch Loss: 0.5162451139518193            
Epoch: #47   | Batch: #36   | Batch Loss: 0.4486064910888672            | Epoch Loss: 0.5143662633167373            
Epoch: #47   | Batch: #37   | Batch Loss: 0.5551055073738098            | Epoch Loss: 0.5154673239669284            
Epoch: #47   | Batch: #38   | Batch Loss: 0.5607118606567383            | Epoch Loss: 0.5166579696692919            
Epoch: #47   | Batch: #39   | Batch Loss: 0.5051627159118652            | Epoch Loss: 0.5163632195729476            
Epoch: #47   | Batch: #40   | Batch Loss: 0.44227302074432373           | Epoch Loss: 0.514510964602232             
Epoch: #47   | Batch: #41   | Batch Loss: 0.5922167897224426            | Epoch Loss: 0.5164062286295542            
Epoch: #47   | Batch: #42   | Batch Loss: 0.5802554488182068            | Epoch Loss: 0.5179264481578555            
Epoch: #47   | Batch: #43   | Batch Loss: 0.5422585010528564            | Epoch Loss: 0.518492309853088             

Classifier Validation Epoch #47
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.85                          
Batch: #2    | Top-1 Accuracy: 0.865909090909091             
Batch: #3    | Top-1 Accuracy: 0.8666666666666667            
Batch: #4    | Top-1 Accuracy: 0.8681818181818182            
Batch: #5    | Top-1 Accuracy: 0.8563636363636362            
Batch: #6    | Top-1 Accuracy: 0.8553030303030301            
Batch: #7    | Top-1 Accuracy: 0.8538961038961038            
Batch: #8    | Top-1 Accuracy: 0.85625                       
Batch: #9    | Top-1 Accuracy: 0.8505050505050504            
Batch: #10   | Top-1 Accuracy: 0.8495454545454546            
Batch: #11   | Top-1 Accuracy: 0.8508264462809918            
Batch: #12   | Top-1 Accuracy: 0.8492424242424242            
Batch: #13   | Top-1 Accuracy: 0.8500000000000001            
Batch: #14   | Top-1 Accuracy: 0.8503246753246755            
Batch: #15   | Top-1 Accuracy: 0.8493939393939395            
Batch: #16   | Top-1 Accuracy: 0.8488636363636364            
Batch: #17   | Top-1 Accuracy: 0.8473262032085561            

Classifier Training Epoch #48
------------------------------------------
Epoch: #48   | Batch: #1    | Batch Loss: 0.5380430817604065            | Epoch Loss: 0.5380430817604065            
Epoch: #48   | Batch: #2    | Batch Loss: 0.5335489511489868            | Epoch Loss: 0.5357960164546967            
Epoch: #48   | Batch: #3    | Batch Loss: 0.46237483620643616           | Epoch Loss: 0.5113222897052765            
Epoch: #48   | Batch: #4    | Batch Loss: 0.6268157362937927            | Epoch Loss: 0.5401956513524055            
Epoch: #48   | Batch: #5    | Batch Loss: 0.4459008574485779            | Epoch Loss: 0.5213366925716401            
Epoch: #48   | Batch: #6    | Batch Loss: 0.539364218711853             | Epoch Loss: 0.5243412802616755            
Epoch: #48   | Batch: #7    | Batch Loss: 0.4945531189441681            | Epoch Loss: 0.5200858286448887            
Epoch: #48   | Batch: #8    | Batch Loss: 0.47455430030822754           | Epoch Loss: 0.5143943876028061            
Epoch: #48   | Batch: #9    | Batch Loss: 0.508106529712677             | Epoch Loss: 0.5136957367261251            
Epoch: #48   | Batch: #10   | Batch Loss: 0.3746304512023926            | Epoch Loss: 0.49978920817375183           
Epoch: #48   | Batch: #11   | Batch Loss: 0.5561705231666565            | Epoch Loss: 0.5049147822640159            
Epoch: #48   | Batch: #12   | Batch Loss: 0.5343578457832336            | Epoch Loss: 0.5073683708906174            
Epoch: #48   | Batch: #13   | Batch Loss: 0.4954386055469513            | Epoch Loss: 0.5064506966334122            
Epoch: #48   | Batch: #14   | Batch Loss: 0.5957462787628174            | Epoch Loss: 0.5128289524997983            
Epoch: #48   | Batch: #15   | Batch Loss: 0.5700892806053162            | Epoch Loss: 0.5166463077068328            
Epoch: #48   | Batch: #16   | Batch Loss: 0.6238874793052673            | Epoch Loss: 0.523348880931735             
Epoch: #48   | Batch: #17   | Batch Loss: 0.4823845624923706            | Epoch Loss: 0.5209392151411842            
Epoch: #48   | Batch: #18   | Batch Loss: 0.5453780889511108            | Epoch Loss: 0.5222969303528467            
Epoch: #48   | Batch: #19   | Batch Loss: 0.5660425424575806            | Epoch Loss: 0.524599330989938             
Epoch: #48   | Batch: #20   | Batch Loss: 0.4662720859050751            | Epoch Loss: 0.5216829687356949            
Epoch: #48   | Batch: #21   | Batch Loss: 0.451455295085907             | Epoch Loss: 0.5183387937999907            
Epoch: #48   | Batch: #22   | Batch Loss: 0.4472171664237976            | Epoch Loss: 0.5151059925556183            
Epoch: #48   | Batch: #23   | Batch Loss: 0.4333705008029938            | Epoch Loss: 0.5115522755228955            
Epoch: #48   | Batch: #24   | Batch Loss: 0.36769741773605347           | Epoch Loss: 0.5055583231151104            
Epoch: #48   | Batch: #25   | Batch Loss: 0.5617508292198181            | Epoch Loss: 0.5078060233592987            
Epoch: #48   | Batch: #26   | Batch Loss: 0.5937589406967163            | Epoch Loss: 0.5111119047953532            
Epoch: #48   | Batch: #27   | Batch Loss: 0.4070710241794586            | Epoch Loss: 0.5072585388466164            
Epoch: #48   | Batch: #28   | Batch Loss: 0.371773362159729             | Epoch Loss: 0.5024197825363704            
Epoch: #48   | Batch: #29   | Batch Loss: 0.4395799934864044            | Epoch Loss: 0.5002528932587854            
Epoch: #48   | Batch: #30   | Batch Loss: 0.5655376315116882            | Epoch Loss: 0.5024290512005488            
Epoch: #48   | Batch: #31   | Batch Loss: 0.44038280844688416           | Epoch Loss: 0.5004275594988177            
Epoch: #48   | Batch: #32   | Batch Loss: 0.5182914137840271            | Epoch Loss: 0.5009858049452305            
Epoch: #48   | Batch: #33   | Batch Loss: 0.5201358199119568            | Epoch Loss: 0.5015661084290707            
Epoch: #48   | Batch: #34   | Batch Loss: 0.4270830750465393            | Epoch Loss: 0.4993754309766433            
Epoch: #48   | Batch: #35   | Batch Loss: 0.4321367144584656            | Epoch Loss: 0.49745432479040963           
Epoch: #48   | Batch: #36   | Batch Loss: 0.5026866793632507            | Epoch Loss: 0.49759966797298855           
Epoch: #48   | Batch: #37   | Batch Loss: 0.52239590883255              | Epoch Loss: 0.49826983664486857           
Epoch: #48   | Batch: #38   | Batch Loss: 0.49386435747146606           | Epoch Loss: 0.49815390298241063           
Epoch: #48   | Batch: #39   | Batch Loss: 0.5361512899398804            | Epoch Loss: 0.49912819495567906           
Epoch: #48   | Batch: #40   | Batch Loss: 0.4963974058628082            | Epoch Loss: 0.4990599252283573            
Epoch: #48   | Batch: #41   | Batch Loss: 0.49552154541015625           | Epoch Loss: 0.4989736232815719            
Epoch: #48   | Batch: #42   | Batch Loss: 0.6191642880439758            | Epoch Loss: 0.5018353057759148            
Epoch: #48   | Batch: #43   | Batch Loss: 0.5159434080123901            | Epoch Loss: 0.5021634011767632            

Classifier Validation Epoch #48
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8409090909090909            
Batch: #2    | Top-1 Accuracy: 0.8272727272727273            
Batch: #3    | Top-1 Accuracy: 0.8439393939393939            
Batch: #4    | Top-1 Accuracy: 0.8352272727272727            
Batch: #5    | Top-1 Accuracy: 0.8481818181818183            
Batch: #6    | Top-1 Accuracy: 0.8446969696969697            
Batch: #7    | Top-1 Accuracy: 0.8454545454545455            
Batch: #8    | Top-1 Accuracy: 0.8448863636363636            
Batch: #9    | Top-1 Accuracy: 0.8383838383838383            
Batch: #10   | Top-1 Accuracy: 0.8381818181818181            
Batch: #11   | Top-1 Accuracy: 0.8384297520661157            
Batch: #12   | Top-1 Accuracy: 0.8401515151515152            
Batch: #13   | Top-1 Accuracy: 0.8409090909090909            
Batch: #14   | Top-1 Accuracy: 0.8402597402597403            
Batch: #15   | Top-1 Accuracy: 0.8418181818181818            
Batch: #16   | Top-1 Accuracy: 0.8426136363636364            
Batch: #17   | Top-1 Accuracy: 0.8427807486631016            

Classifier Training Epoch #49
------------------------------------------
Epoch: #49   | Batch: #1    | Batch Loss: 0.5557220578193665            | Epoch Loss: 0.5557220578193665            
Epoch: #49   | Batch: #2    | Batch Loss: 0.5799352526664734            | Epoch Loss: 0.5678286552429199            
Epoch: #49   | Batch: #3    | Batch Loss: 0.4559757709503174            | Epoch Loss: 0.5305443604787191            
Epoch: #49   | Batch: #4    | Batch Loss: 0.4683733880519867            | Epoch Loss: 0.515001617372036             
Epoch: #49   | Batch: #5    | Batch Loss: 0.499679833650589             | Epoch Loss: 0.5119372606277466            
Epoch: #49   | Batch: #6    | Batch Loss: 0.4101310670375824            | Epoch Loss: 0.49496956169605255           
Epoch: #49   | Batch: #7    | Batch Loss: 0.6423725485801697            | Epoch Loss: 0.5160271312509265            
Epoch: #49   | Batch: #8    | Batch Loss: 0.5370677709579468            | Epoch Loss: 0.518657211214304             
Epoch: #49   | Batch: #9    | Batch Loss: 0.526592493057251             | Epoch Loss: 0.5195389091968536            
Epoch: #49   | Batch: #10   | Batch Loss: 0.4661169648170471            | Epoch Loss: 0.514196714758873             
Epoch: #49   | Batch: #11   | Batch Loss: 0.5241919159889221            | Epoch Loss: 0.5151053694161501            
Epoch: #49   | Batch: #12   | Batch Loss: 0.5273640751838684            | Epoch Loss: 0.5161269282301267            
Epoch: #49   | Batch: #13   | Batch Loss: 0.5067351460456848            | Epoch Loss: 0.5154044834467081            
Epoch: #49   | Batch: #14   | Batch Loss: 0.4777826964855194            | Epoch Loss: 0.5127172129494804            
Epoch: #49   | Batch: #15   | Batch Loss: 0.4836377501487732            | Epoch Loss: 0.5107785820960998            
Epoch: #49   | Batch: #16   | Batch Loss: 0.39418748021125793           | Epoch Loss: 0.5034916382282972            
Epoch: #49   | Batch: #17   | Batch Loss: 0.5054948925971985            | Epoch Loss: 0.5036094767205855            
Epoch: #49   | Batch: #18   | Batch Loss: 0.5570636987686157            | Epoch Loss: 0.5065791557232538            
Epoch: #49   | Batch: #19   | Batch Loss: 0.45440611243247986           | Epoch Loss: 0.503833206076371             
Epoch: #49   | Batch: #20   | Batch Loss: 0.4647153615951538            | Epoch Loss: 0.5018773138523102            
Epoch: #49   | Batch: #21   | Batch Loss: 0.455396831035614             | Epoch Loss: 0.4996639575277056            
Epoch: #49   | Batch: #22   | Batch Loss: 0.5140576958656311            | Epoch Loss: 0.5003182183612477            
Epoch: #49   | Batch: #23   | Batch Loss: 0.43301114439964294           | Epoch Loss: 0.4973918238411779            
Epoch: #49   | Batch: #24   | Batch Loss: 0.5448009371757507            | Epoch Loss: 0.49936720356345177           
Epoch: #49   | Batch: #25   | Batch Loss: 0.5005178451538086            | Epoch Loss: 0.49941322922706605           
Epoch: #49   | Batch: #26   | Batch Loss: 0.624120831489563             | Epoch Loss: 0.5042096754679313            
Epoch: #49   | Batch: #27   | Batch Loss: 0.44411879777908325           | Epoch Loss: 0.5019840874053814            
Epoch: #49   | Batch: #28   | Batch Loss: 0.5142748951911926            | Epoch Loss: 0.5024230448263032            
Epoch: #49   | Batch: #29   | Batch Loss: 0.5748132467269897            | Epoch Loss: 0.5049192586849476            
Epoch: #49   | Batch: #30   | Batch Loss: 0.5032161474227905            | Epoch Loss: 0.5048624883095424            
Epoch: #49   | Batch: #31   | Batch Loss: 0.5579063296318054            | Epoch Loss: 0.5065735799650992            
Epoch: #49   | Batch: #32   | Batch Loss: 0.510330319404602             | Epoch Loss: 0.5066909780725837            
Epoch: #49   | Batch: #33   | Batch Loss: 0.5819065570831299            | Epoch Loss: 0.5089702380426002            
Epoch: #49   | Batch: #34   | Batch Loss: 0.6607620716094971            | Epoch Loss: 0.5134347037357443            
Epoch: #49   | Batch: #35   | Batch Loss: 0.6063544750213623            | Epoch Loss: 0.5160895543439048            
Epoch: #49   | Batch: #36   | Batch Loss: 0.5218095183372498            | Epoch Loss: 0.5162484422326088            
Epoch: #49   | Batch: #37   | Batch Loss: 0.383436918258667             | Epoch Loss: 0.5126589415846644            
Epoch: #49   | Batch: #38   | Batch Loss: 0.5213977694511414            | Epoch Loss: 0.5128889107390454            
Epoch: #49   | Batch: #39   | Batch Loss: 0.5036064386367798            | Epoch Loss: 0.512650898633859             
Epoch: #49   | Batch: #40   | Batch Loss: 0.5122492909431458            | Epoch Loss: 0.5126408584415912            
Epoch: #49   | Batch: #41   | Batch Loss: 0.3870813250541687            | Epoch Loss: 0.5095784307979956            
Epoch: #49   | Batch: #42   | Batch Loss: 0.5457760095596313            | Epoch Loss: 0.5104402779113679            
Epoch: #49   | Batch: #43   | Batch Loss: 0.6153539419174194            | Epoch Loss: 0.5128801305626713            

Classifier Validation Epoch #49
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8363636363636363            
Batch: #2    | Top-1 Accuracy: 0.825                         
Batch: #3    | Top-1 Accuracy: 0.8196969696969697            
Batch: #4    | Top-1 Accuracy: 0.8386363636363636            
Batch: #5    | Top-1 Accuracy: 0.8445454545454545            
Batch: #6    | Top-1 Accuracy: 0.8439393939393939            
Batch: #7    | Top-1 Accuracy: 0.8428571428571427            
Batch: #8    | Top-1 Accuracy: 0.84375                       
Batch: #9    | Top-1 Accuracy: 0.843939393939394             
Batch: #10   | Top-1 Accuracy: 0.8427272727272728            
Batch: #11   | Top-1 Accuracy: 0.8429752066115703            
Batch: #12   | Top-1 Accuracy: 0.846969696969697             
Batch: #13   | Top-1 Accuracy: 0.8475524475524476            
Batch: #14   | Top-1 Accuracy: 0.8461038961038962            
Batch: #15   | Top-1 Accuracy: 0.8439393939393941            
Batch: #16   | Top-1 Accuracy: 0.8423295454545454            
Batch: #17   | Top-1 Accuracy: 0.8398395721925134            

Classifier Training Epoch #50
------------------------------------------
Epoch: #50   | Batch: #1    | Batch Loss: 0.46644461154937744           | Epoch Loss: 0.46644461154937744           
Epoch: #50   | Batch: #2    | Batch Loss: 0.7404565811157227            | Epoch Loss: 0.60345059633255              
Epoch: #50   | Batch: #3    | Batch Loss: 0.4614824950695038            | Epoch Loss: 0.5561278959115347            
Epoch: #50   | Batch: #4    | Batch Loss: 0.4834098815917969            | Epoch Loss: 0.5379483923316002            
Epoch: #50   | Batch: #5    | Batch Loss: 0.459978312253952             | Epoch Loss: 0.5223543763160705            
Epoch: #50   | Batch: #6    | Batch Loss: 0.5178715586662292            | Epoch Loss: 0.521607240041097             
Epoch: #50   | Batch: #7    | Batch Loss: 0.49965476989746094           | Epoch Loss: 0.5184711728777204            
Epoch: #50   | Batch: #8    | Batch Loss: 0.5942704677581787            | Epoch Loss: 0.5279460847377777            
Epoch: #50   | Batch: #9    | Batch Loss: 0.5639699697494507            | Epoch Loss: 0.5319487386279635            
Epoch: #50   | Batch: #10   | Batch Loss: 0.38363322615623474           | Epoch Loss: 0.5171171873807907            
Epoch: #50   | Batch: #11   | Batch Loss: 0.43137866258621216           | Epoch Loss: 0.509322776035829             
Epoch: #50   | Batch: #12   | Batch Loss: 0.5324880480766296            | Epoch Loss: 0.5112532153725624            
Epoch: #50   | Batch: #13   | Batch Loss: 0.5222018957138062            | Epoch Loss: 0.5120954215526581            
Epoch: #50   | Batch: #14   | Batch Loss: 0.44340401887893677           | Epoch Loss: 0.5071888927902494            
Epoch: #50   | Batch: #15   | Batch Loss: 0.44360455870628357           | Epoch Loss: 0.5029499371846516            
Epoch: #50   | Batch: #16   | Batch Loss: 0.5698180198669434            | Epoch Loss: 0.5071291923522949            
Epoch: #50   | Batch: #17   | Batch Loss: 0.5647782683372498            | Epoch Loss: 0.510520314469057             
Epoch: #50   | Batch: #18   | Batch Loss: 0.5416613817214966            | Epoch Loss: 0.5122503737608591            
Epoch: #50   | Batch: #19   | Batch Loss: 0.5175020098686218            | Epoch Loss: 0.5125267756612677            
Epoch: #50   | Batch: #20   | Batch Loss: 0.4433651566505432            | Epoch Loss: 0.5090686947107315            
Epoch: #50   | Batch: #21   | Batch Loss: 0.5482437610626221            | Epoch Loss: 0.5109341740608215            
Epoch: #50   | Batch: #22   | Batch Loss: 0.4613990783691406            | Epoch Loss: 0.5086825788021088            
Epoch: #50   | Batch: #23   | Batch Loss: 0.48117533326148987           | Epoch Loss: 0.5074866116046906            
Epoch: #50   | Batch: #24   | Batch Loss: 0.41534802317619324           | Epoch Loss: 0.5036475037535032            
Epoch: #50   | Batch: #25   | Batch Loss: 0.48087823390960693           | Epoch Loss: 0.5027367329597473            
Epoch: #50   | Batch: #26   | Batch Loss: 0.46835875511169434           | Epoch Loss: 0.5014145030425146            
Epoch: #50   | Batch: #27   | Batch Loss: 0.4481864273548126            | Epoch Loss: 0.4994430928318589            
Epoch: #50   | Batch: #28   | Batch Loss: 0.4605911672115326            | Epoch Loss: 0.4980555240597044            
Epoch: #50   | Batch: #29   | Batch Loss: 0.5559283494949341            | Epoch Loss: 0.5000511387298847            
Epoch: #50   | Batch: #30   | Batch Loss: 0.34609454870224              | Epoch Loss: 0.4949192523956299            
Epoch: #50   | Batch: #31   | Batch Loss: 0.5951854586601257            | Epoch Loss: 0.4981536461460975            
Epoch: #50   | Batch: #32   | Batch Loss: 0.545109212398529             | Epoch Loss: 0.499621007591486             
Epoch: #50   | Batch: #33   | Batch Loss: 0.4918711185455322            | Epoch Loss: 0.49938616246888134           
Epoch: #50   | Batch: #34   | Batch Loss: 0.49948641657829285           | Epoch Loss: 0.4993891111191581            
Epoch: #50   | Batch: #35   | Batch Loss: 0.49274304509162903           | Epoch Loss: 0.4991992235183716            
Epoch: #50   | Batch: #36   | Batch Loss: 0.5032408833503723            | Epoch Loss: 0.49931149184703827           
Epoch: #50   | Batch: #37   | Batch Loss: 0.4620371460914612            | Epoch Loss: 0.4983040770968875            
Epoch: #50   | Batch: #38   | Batch Loss: 0.6120677590370178            | Epoch Loss: 0.5012978582005752            
Epoch: #50   | Batch: #39   | Batch Loss: 0.5238688588142395            | Epoch Loss: 0.5018766018060538            
Epoch: #50   | Batch: #40   | Batch Loss: 0.5630727410316467            | Epoch Loss: 0.5034065052866936            
Epoch: #50   | Batch: #41   | Batch Loss: 0.518367350101471             | Epoch Loss: 0.5037714039407125            
Epoch: #50   | Batch: #42   | Batch Loss: 0.45270800590515137           | Epoch Loss: 0.5025556087493896            
Epoch: #50   | Batch: #43   | Batch Loss: 0.5710040330886841            | Epoch Loss: 0.5041474325712337            

Classifier Validation Epoch #50
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8227272727272728            
Batch: #2    | Top-1 Accuracy: 0.8159090909090909            
Batch: #3    | Top-1 Accuracy: 0.8136363636363636            
Batch: #4    | Top-1 Accuracy: 0.8272727272727273            
Batch: #5    | Top-1 Accuracy: 0.8290909090909091            
Batch: #6    | Top-1 Accuracy: 0.8348484848484848            
Batch: #7    | Top-1 Accuracy: 0.8474025974025974            
Batch: #8    | Top-1 Accuracy: 0.8471590909090909            
Batch: #9    | Top-1 Accuracy: 0.8454545454545455            
Batch: #10   | Top-1 Accuracy: 0.8450000000000001            
Batch: #11   | Top-1 Accuracy: 0.8466942148760331            
Batch: #12   | Top-1 Accuracy: 0.8484848484848486            
Batch: #13   | Top-1 Accuracy: 0.8486013986013987            
Batch: #14   | Top-1 Accuracy: 0.8500000000000002            
Batch: #15   | Top-1 Accuracy: 0.8472727272727274            
Batch: #16   | Top-1 Accuracy: 0.8491477272727272            
Batch: #17   | Top-1 Accuracy: 0.8483957219251337            

Classifier Training Epoch #51
------------------------------------------
Epoch: #51   | Batch: #1    | Batch Loss: 0.3650383949279785            | Epoch Loss: 0.3650383949279785            
Epoch: #51   | Batch: #2    | Batch Loss: 0.4330972731113434            | Epoch Loss: 0.39906783401966095           
Epoch: #51   | Batch: #3    | Batch Loss: 0.5894850492477417            | Epoch Loss: 0.46254023909568787           
Epoch: #51   | Batch: #4    | Batch Loss: 0.5500707030296326            | Epoch Loss: 0.48442285507917404           
Epoch: #51   | Batch: #5    | Batch Loss: 0.4375895857810974            | Epoch Loss: 0.4750562012195587            
Epoch: #51   | Batch: #6    | Batch Loss: 0.526494562625885             | Epoch Loss: 0.4836292614539464            
Epoch: #51   | Batch: #7    | Batch Loss: 0.5505099296569824            | Epoch Loss: 0.4931836426258087            
Epoch: #51   | Batch: #8    | Batch Loss: 0.5350823402404785            | Epoch Loss: 0.49842097982764244           
Epoch: #51   | Batch: #9    | Batch Loss: 0.587044358253479             | Epoch Loss: 0.5082680218749576            
Epoch: #51   | Batch: #10   | Batch Loss: 0.477122038602829             | Epoch Loss: 0.5051534235477447            
Epoch: #51   | Batch: #11   | Batch Loss: 0.6602367162704468            | Epoch Loss: 0.519251904704354             
Epoch: #51   | Batch: #12   | Batch Loss: 0.4892579913139343            | Epoch Loss: 0.5167524119218191            
Epoch: #51   | Batch: #13   | Batch Loss: 0.5011422038078308            | Epoch Loss: 0.5155516266822815            
Epoch: #51   | Batch: #14   | Batch Loss: 0.5010347962379456            | Epoch Loss: 0.5145147102219718            
Epoch: #51   | Batch: #15   | Batch Loss: 0.6589889526367188            | Epoch Loss: 0.5241463263829549            
Epoch: #51   | Batch: #16   | Batch Loss: 0.4208104908466339            | Epoch Loss: 0.5176878366619349            
Epoch: #51   | Batch: #17   | Batch Loss: 0.4058988690376282            | Epoch Loss: 0.5111120150369757            
Epoch: #51   | Batch: #18   | Batch Loss: 0.43322303891181946           | Epoch Loss: 0.5067848496966891            
Epoch: #51   | Batch: #19   | Batch Loss: 0.5303909778594971            | Epoch Loss: 0.5080272774947318            
Epoch: #51   | Batch: #20   | Batch Loss: 0.5244402289390564            | Epoch Loss: 0.508847925066948             
Epoch: #51   | Batch: #21   | Batch Loss: 0.5087681412696838            | Epoch Loss: 0.5088441258385068            
Epoch: #51   | Batch: #22   | Batch Loss: 0.5175731778144836            | Epoch Loss: 0.509240900928324             
Epoch: #51   | Batch: #23   | Batch Loss: 0.5808423161506653            | Epoch Loss: 0.512354005937991             
Epoch: #51   | Batch: #24   | Batch Loss: 0.4680878520011902            | Epoch Loss: 0.5105095828572909            
Epoch: #51   | Batch: #25   | Batch Loss: 0.5175575017929077            | Epoch Loss: 0.5107914996147156            
Epoch: #51   | Batch: #26   | Batch Loss: 0.47472405433654785           | Epoch Loss: 0.5094042901809399            
Epoch: #51   | Batch: #27   | Batch Loss: 0.47279879450798035           | Epoch Loss: 0.5080485310819414            
Epoch: #51   | Batch: #28   | Batch Loss: 0.4436511993408203            | Epoch Loss: 0.5057486263769013            
Epoch: #51   | Batch: #29   | Batch Loss: 0.5234026312828064            | Epoch Loss: 0.5063573851667601            
Epoch: #51   | Batch: #30   | Batch Loss: 0.5983006358146667            | Epoch Loss: 0.5094221601883571            
Epoch: #51   | Batch: #31   | Batch Loss: 0.41717851161956787           | Epoch Loss: 0.5064465586216219            
Epoch: #51   | Batch: #32   | Batch Loss: 0.5047522187232971            | Epoch Loss: 0.5063936104997993            
Epoch: #51   | Batch: #33   | Batch Loss: 0.6193851232528687            | Epoch Loss: 0.5098175957347407            
Epoch: #51   | Batch: #34   | Batch Loss: 0.4211291968822479            | Epoch Loss: 0.5072091134155498            
Epoch: #51   | Batch: #35   | Batch Loss: 0.5892418026924133            | Epoch Loss: 0.5095529045377459            
Epoch: #51   | Batch: #36   | Batch Loss: 0.43237075209617615           | Epoch Loss: 0.5074089558588134            
Epoch: #51   | Batch: #37   | Batch Loss: 0.46118229627609253           | Epoch Loss: 0.506159586680902             
Epoch: #51   | Batch: #38   | Batch Loss: 0.5343576669692993            | Epoch Loss: 0.5069016414253336            
Epoch: #51   | Batch: #39   | Batch Loss: 0.44139793515205383           | Epoch Loss: 0.5052220592131982            
Epoch: #51   | Batch: #40   | Batch Loss: 0.5846027731895447            | Epoch Loss: 0.5072065770626069            
Epoch: #51   | Batch: #41   | Batch Loss: 0.4203760027885437            | Epoch Loss: 0.5050887581778736            
Epoch: #51   | Batch: #42   | Batch Loss: 0.5309343934059143            | Epoch Loss: 0.5057041304452079            
Epoch: #51   | Batch: #43   | Batch Loss: 0.4491807818412781            | Epoch Loss: 0.5043896339660467            

Classifier Validation Epoch #51
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8454545454545455            
Batch: #2    | Top-1 Accuracy: 0.8295454545454546            
Batch: #3    | Top-1 Accuracy: 0.8303030303030304            
Batch: #4    | Top-1 Accuracy: 0.8397727272727273            
Batch: #5    | Top-1 Accuracy: 0.8427272727272728            
Batch: #6    | Top-1 Accuracy: 0.8431818181818183            
Batch: #7    | Top-1 Accuracy: 0.8454545454545456            
Batch: #8    | Top-1 Accuracy: 0.8460227272727273            
Batch: #9    | Top-1 Accuracy: 0.8489898989898991            
Batch: #10   | Top-1 Accuracy: 0.8504545454545456            
Batch: #11   | Top-1 Accuracy: 0.8471074380165291            
Batch: #12   | Top-1 Accuracy: 0.847727272727273             
Batch: #13   | Top-1 Accuracy: 0.8465034965034968            
Batch: #14   | Top-1 Accuracy: 0.8470779220779223            
Batch: #15   | Top-1 Accuracy: 0.8475757575757579            
Batch: #16   | Top-1 Accuracy: 0.8494318181818181            
Batch: #17   | Top-1 Accuracy: 0.8473262032085561            

Classifier Training Epoch #52
------------------------------------------
Epoch: #52   | Batch: #1    | Batch Loss: 0.640598714351654             | Epoch Loss: 0.640598714351654             
Epoch: #52   | Batch: #2    | Batch Loss: 0.5002135038375854            | Epoch Loss: 0.5704061090946198            
Epoch: #52   | Batch: #3    | Batch Loss: 0.4070226550102234            | Epoch Loss: 0.5159449577331543            
Epoch: #52   | Batch: #4    | Batch Loss: 0.5358859896659851            | Epoch Loss: 0.520930215716362             
Epoch: #52   | Batch: #5    | Batch Loss: 0.5036685466766357            | Epoch Loss: 0.5174778819084167            
Epoch: #52   | Batch: #6    | Batch Loss: 0.5748578906059265            | Epoch Loss: 0.5270412166913351            
Epoch: #52   | Batch: #7    | Batch Loss: 0.5567605495452881            | Epoch Loss: 0.5312868356704712            
Epoch: #52   | Batch: #8    | Batch Loss: 0.5281636714935303            | Epoch Loss: 0.5308964401483536            
Epoch: #52   | Batch: #9    | Batch Loss: 0.44645175337791443           | Epoch Loss: 0.5215136971738603            
Epoch: #52   | Batch: #10   | Batch Loss: 0.36452150344848633           | Epoch Loss: 0.5058144778013229            
Epoch: #52   | Batch: #11   | Batch Loss: 0.4823046326637268            | Epoch Loss: 0.5036772191524506            
Epoch: #52   | Batch: #12   | Batch Loss: 0.5525493025779724            | Epoch Loss: 0.507749892771244             
Epoch: #52   | Batch: #13   | Batch Loss: 0.48359188437461853           | Epoch Loss: 0.505891584433042             
Epoch: #52   | Batch: #14   | Batch Loss: 0.5221118330955505            | Epoch Loss: 0.5070501736232212            
Epoch: #52   | Batch: #15   | Batch Loss: 0.6033607721328735            | Epoch Loss: 0.5134708801905314            
Epoch: #52   | Batch: #16   | Batch Loss: 0.48805150389671326           | Epoch Loss: 0.5118821691721678            
Epoch: #52   | Batch: #17   | Batch Loss: 0.5557330250740051            | Epoch Loss: 0.5144616312840405            
Epoch: #52   | Batch: #18   | Batch Loss: 0.4651950001716614            | Epoch Loss: 0.5117245962222418            
Epoch: #52   | Batch: #19   | Batch Loss: 0.4986761212348938            | Epoch Loss: 0.5110378343808023            
Epoch: #52   | Batch: #20   | Batch Loss: 0.43823665380477905           | Epoch Loss: 0.5073977753520011            
Epoch: #52   | Batch: #21   | Batch Loss: 0.39954236149787903           | Epoch Loss: 0.5022618032637096            
Epoch: #52   | Batch: #22   | Batch Loss: 0.4864558279514313            | Epoch Loss: 0.5015433498404243            
Epoch: #52   | Batch: #23   | Batch Loss: 0.4375707507133484            | Epoch Loss: 0.4987619324870732            
Epoch: #52   | Batch: #24   | Batch Loss: 0.40473607182502747           | Epoch Loss: 0.49484418829282123           
Epoch: #52   | Batch: #25   | Batch Loss: 0.3774227499961853            | Epoch Loss: 0.4901473307609558            
Epoch: #52   | Batch: #26   | Batch Loss: 0.5752266049385071            | Epoch Loss: 0.4934196105370155            
Epoch: #52   | Batch: #27   | Batch Loss: 0.5130996108055115            | Epoch Loss: 0.49414849943584865           
Epoch: #52   | Batch: #28   | Batch Loss: 0.42691513895988464           | Epoch Loss: 0.4917473079902785            
Epoch: #52   | Batch: #29   | Batch Loss: 0.5075715780258179            | Epoch Loss: 0.4922929724742626            
Epoch: #52   | Batch: #30   | Batch Loss: 0.502621054649353             | Epoch Loss: 0.492637241880099             
Epoch: #52   | Batch: #31   | Batch Loss: 0.48238351941108704           | Epoch Loss: 0.4923064766391631            
Epoch: #52   | Batch: #32   | Batch Loss: 0.6271961331367493            | Epoch Loss: 0.4965217784047127            
Epoch: #52   | Batch: #33   | Batch Loss: 0.4821643531322479            | Epoch Loss: 0.49608670491160767           
Epoch: #52   | Batch: #34   | Batch Loss: 0.5447672009468079            | Epoch Loss: 0.49751848420676065           
Epoch: #52   | Batch: #35   | Batch Loss: 0.4113173186779022            | Epoch Loss: 0.49505559376307895           
Epoch: #52   | Batch: #36   | Batch Loss: 0.5875421762466431            | Epoch Loss: 0.4976246654987335            
Epoch: #52   | Batch: #37   | Batch Loss: 0.36091017723083496           | Epoch Loss: 0.49392967932933085           
Epoch: #52   | Batch: #38   | Batch Loss: 0.555092990398407             | Epoch Loss: 0.4955392401469381            
Epoch: #52   | Batch: #39   | Batch Loss: 0.6157137155532837            | Epoch Loss: 0.498620636952229             
Epoch: #52   | Batch: #40   | Batch Loss: 0.637851893901825             | Epoch Loss: 0.5021014183759689            
Epoch: #52   | Batch: #41   | Batch Loss: 0.46226930618286133           | Epoch Loss: 0.5011299034444298            
Epoch: #52   | Batch: #42   | Batch Loss: 0.433118999004364             | Epoch Loss: 0.49951059619585675           
Epoch: #52   | Batch: #43   | Batch Loss: 0.5083640813827515            | Epoch Loss: 0.4997164912002031            

Classifier Validation Epoch #52
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8363636363636363            
Batch: #2    | Top-1 Accuracy: 0.8272727272727273            
Batch: #3    | Top-1 Accuracy: 0.8227272727272728            
Batch: #4    | Top-1 Accuracy: 0.8340909090909091            
Batch: #5    | Top-1 Accuracy: 0.8363636363636363            
Batch: #6    | Top-1 Accuracy: 0.8386363636363635            
Batch: #7    | Top-1 Accuracy: 0.8415584415584415            
Batch: #8    | Top-1 Accuracy: 0.8443181818181817            
Batch: #9    | Top-1 Accuracy: 0.8424242424242423            
Batch: #10   | Top-1 Accuracy: 0.8409090909090908            
Batch: #11   | Top-1 Accuracy: 0.8396694214876033            
Batch: #12   | Top-1 Accuracy: 0.8416666666666667            
Batch: #13   | Top-1 Accuracy: 0.8412587412587412            
Batch: #14   | Top-1 Accuracy: 0.8405844155844155            
Batch: #15   | Top-1 Accuracy: 0.8396969696969697            
Batch: #16   | Top-1 Accuracy: 0.8411931818181818            
Batch: #17   | Top-1 Accuracy: 0.8425133689839572            

Classifier Training Epoch #53
------------------------------------------
Epoch: #53   | Batch: #1    | Batch Loss: 0.4455008804798126            | Epoch Loss: 0.4455008804798126            
Epoch: #53   | Batch: #2    | Batch Loss: 0.4742331802845001            | Epoch Loss: 0.45986703038215637           
Epoch: #53   | Batch: #3    | Batch Loss: 0.6274160146713257            | Epoch Loss: 0.5157166918118795            
Epoch: #53   | Batch: #4    | Batch Loss: 0.4947642385959625            | Epoch Loss: 0.5104785785079002            
Epoch: #53   | Batch: #5    | Batch Loss: 0.5917990803718567            | Epoch Loss: 0.5267426788806915            
Epoch: #53   | Batch: #6    | Batch Loss: 0.4720960855484009            | Epoch Loss: 0.5176349133253098            
Epoch: #53   | Batch: #7    | Batch Loss: 0.5416724681854248            | Epoch Loss: 0.5210688497338977            
Epoch: #53   | Batch: #8    | Batch Loss: 0.4599562883377075            | Epoch Loss: 0.5134297795593739            
Epoch: #53   | Batch: #9    | Batch Loss: 0.513871967792511             | Epoch Loss: 0.513478911585278             
Epoch: #53   | Batch: #10   | Batch Loss: 0.4331923723220825            | Epoch Loss: 0.5054502576589585            
Epoch: #53   | Batch: #11   | Batch Loss: 0.4804406464099884            | Epoch Loss: 0.5031766566363248            
Epoch: #53   | Batch: #12   | Batch Loss: 0.4754558205604553            | Epoch Loss: 0.5008665869633356            
Epoch: #53   | Batch: #13   | Batch Loss: 0.4778796136379242            | Epoch Loss: 0.49909835824599635           
Epoch: #53   | Batch: #14   | Batch Loss: 0.5091328620910645            | Epoch Loss: 0.49981510852064404           
Epoch: #53   | Batch: #15   | Batch Loss: 0.4119931757450104            | Epoch Loss: 0.49396031300226845           
Epoch: #53   | Batch: #16   | Batch Loss: 0.518791913986206             | Epoch Loss: 0.49551228806376457           
Epoch: #53   | Batch: #17   | Batch Loss: 0.5038077235221863            | Epoch Loss: 0.4960002548554364            
Epoch: #53   | Batch: #18   | Batch Loss: 0.6144633889198303            | Epoch Loss: 0.5025815400812361            
Epoch: #53   | Batch: #19   | Batch Loss: 0.49223753809928894           | Epoch Loss: 0.5020371189242915            
Epoch: #53   | Batch: #20   | Batch Loss: 0.5314866304397583            | Epoch Loss: 0.5035095945000648            
Epoch: #53   | Batch: #21   | Batch Loss: 0.5216537117958069            | Epoch Loss: 0.5043736000855764            
Epoch: #53   | Batch: #22   | Batch Loss: 0.6018921136856079            | Epoch Loss: 0.5088062597946688            
Epoch: #53   | Batch: #23   | Batch Loss: 0.4887005686759949            | Epoch Loss: 0.5079320993112482            
Epoch: #53   | Batch: #24   | Batch Loss: 0.6020016670227051            | Epoch Loss: 0.5118516646325588            
Epoch: #53   | Batch: #25   | Batch Loss: 0.6590893864631653            | Epoch Loss: 0.5177411735057831            
Epoch: #53   | Batch: #26   | Batch Loss: 0.5828357934951782            | Epoch Loss: 0.5202448127361444            
Epoch: #53   | Batch: #27   | Batch Loss: 0.5813121199607849            | Epoch Loss: 0.5225065648555756            
Epoch: #53   | Batch: #28   | Batch Loss: 0.4307635426521301            | Epoch Loss: 0.5192300283483097            
Epoch: #53   | Batch: #29   | Batch Loss: 0.4645096957683563            | Epoch Loss: 0.5173431203283113            
Epoch: #53   | Batch: #30   | Batch Loss: 0.5762118101119995            | Epoch Loss: 0.5193054099877675            
Epoch: #53   | Batch: #31   | Batch Loss: 0.4271710515022278            | Epoch Loss: 0.5163333339075888            
Epoch: #53   | Batch: #32   | Batch Loss: 0.498019278049469             | Epoch Loss: 0.5157610196620226            
Epoch: #53   | Batch: #33   | Batch Loss: 0.5011157989501953            | Epoch Loss: 0.5153172250949976            
Epoch: #53   | Batch: #34   | Batch Loss: 0.4238278567790985            | Epoch Loss: 0.5126263613210005            
Epoch: #53   | Batch: #35   | Batch Loss: 0.4298827350139618            | Epoch Loss: 0.510262257712228             
Epoch: #53   | Batch: #36   | Batch Loss: 0.418722003698349             | Epoch Loss: 0.507719472878509             
Epoch: #53   | Batch: #37   | Batch Loss: 0.46414822340011597           | Epoch Loss: 0.5065418715412552            
Epoch: #53   | Batch: #38   | Batch Loss: 0.454373836517334             | Epoch Loss: 0.5051690285143099            
Epoch: #53   | Batch: #39   | Batch Loss: 0.47632670402526855           | Epoch Loss: 0.5044294817325397            
Epoch: #53   | Batch: #40   | Batch Loss: 0.5119714140892029            | Epoch Loss: 0.5046180300414562            
Epoch: #53   | Batch: #41   | Batch Loss: 0.5222325921058655            | Epoch Loss: 0.5050476535064418            
Epoch: #53   | Batch: #42   | Batch Loss: 0.4600309431552887            | Epoch Loss: 0.5039758270695096            
Epoch: #53   | Batch: #43   | Batch Loss: 0.4561818540096283            | Epoch Loss: 0.5028643393239309            

Classifier Validation Epoch #53
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.7954545454545454            
Batch: #2    | Top-1 Accuracy: 0.8204545454545454            
Batch: #3    | Top-1 Accuracy: 0.8196969696969697            
Batch: #4    | Top-1 Accuracy: 0.8284090909090909            
Batch: #5    | Top-1 Accuracy: 0.8272727272727274            
Batch: #6    | Top-1 Accuracy: 0.8234848484848486            
Batch: #7    | Top-1 Accuracy: 0.82987012987013              
Batch: #8    | Top-1 Accuracy: 0.8335227272727272            
Batch: #9    | Top-1 Accuracy: 0.8313131313131312            
Batch: #10   | Top-1 Accuracy: 0.8331818181818182            
Batch: #11   | Top-1 Accuracy: 0.8363636363636363            
Batch: #12   | Top-1 Accuracy: 0.8374999999999999            
Batch: #13   | Top-1 Accuracy: 0.8374125874125873            
Batch: #14   | Top-1 Accuracy: 0.8425324675324674            
Batch: #15   | Top-1 Accuracy: 0.8445454545454544            
Batch: #16   | Top-1 Accuracy: 0.8465909090909091            
Batch: #17   | Top-1 Accuracy: 0.8454545454545455            

Classifier Training Epoch #54
------------------------------------------
Epoch: #54   | Batch: #1    | Batch Loss: 0.39044463634490967           | Epoch Loss: 0.39044463634490967           
Epoch: #54   | Batch: #2    | Batch Loss: 0.545731782913208             | Epoch Loss: 0.46808820962905884           
Epoch: #54   | Batch: #3    | Batch Loss: 0.4435884654521942            | Epoch Loss: 0.45992162823677063           
Epoch: #54   | Batch: #4    | Batch Loss: 0.681564211845398             | Epoch Loss: 0.5153322741389275            
Epoch: #54   | Batch: #5    | Batch Loss: 0.49187347292900085           | Epoch Loss: 0.5106405138969421            
Epoch: #54   | Batch: #6    | Batch Loss: 0.5212258100509644            | Epoch Loss: 0.5124047299226125            
Epoch: #54   | Batch: #7    | Batch Loss: 0.37334343791007996           | Epoch Loss: 0.4925388310636793            
Epoch: #54   | Batch: #8    | Batch Loss: 0.4427330493927002            | Epoch Loss: 0.4863131083548069            
Epoch: #54   | Batch: #9    | Batch Loss: 0.45624110102653503           | Epoch Loss: 0.48297177420722115           
Epoch: #54   | Batch: #10   | Batch Loss: 0.4830803871154785            | Epoch Loss: 0.4829826354980469            
Epoch: #54   | Batch: #11   | Batch Loss: 0.49703535437583923           | Epoch Loss: 0.484260155396028             
Epoch: #54   | Batch: #12   | Batch Loss: 0.5337932705879211            | Epoch Loss: 0.48838791499535245           
Epoch: #54   | Batch: #13   | Batch Loss: 0.4643808901309967            | Epoch Loss: 0.4865412207750174            
Epoch: #54   | Batch: #14   | Batch Loss: 0.4696408808231354            | Epoch Loss: 0.48533405363559723           
Epoch: #54   | Batch: #15   | Batch Loss: 0.4473692774772644            | Epoch Loss: 0.482803068558375             
Epoch: #54   | Batch: #16   | Batch Loss: 0.5205557346343994            | Epoch Loss: 0.48516261018812656           
Epoch: #54   | Batch: #17   | Batch Loss: 0.5458685755729675            | Epoch Loss: 0.48873354932841134           
Epoch: #54   | Batch: #18   | Batch Loss: 0.5207537412643433            | Epoch Loss: 0.49051244888040757           
Epoch: #54   | Batch: #19   | Batch Loss: 0.503568172454834             | Epoch Loss: 0.49119959222643              
Epoch: #54   | Batch: #20   | Batch Loss: 0.5257428288459778            | Epoch Loss: 0.4929267540574074            
Epoch: #54   | Batch: #21   | Batch Loss: 0.4180910587310791            | Epoch Loss: 0.4893631495180584            
Epoch: #54   | Batch: #22   | Batch Loss: 0.567669153213501             | Epoch Loss: 0.4929225133223967            
Epoch: #54   | Batch: #23   | Batch Loss: 0.37646958231925964           | Epoch Loss: 0.48785934240921686           
Epoch: #54   | Batch: #24   | Batch Loss: 0.48809197545051575           | Epoch Loss: 0.4878690354526043            
Epoch: #54   | Batch: #25   | Batch Loss: 0.4546933174133301            | Epoch Loss: 0.4865420067310333            
Epoch: #54   | Batch: #26   | Batch Loss: 0.4430948495864868            | Epoch Loss: 0.4848709622254738            
Epoch: #54   | Batch: #27   | Batch Loss: 0.4893959164619446            | Epoch Loss: 0.4850385531231209            
Epoch: #54   | Batch: #28   | Batch Loss: 0.5123052000999451            | Epoch Loss: 0.48601236194372177           
Epoch: #54   | Batch: #29   | Batch Loss: 0.5537850260734558            | Epoch Loss: 0.48834935036198845           
Epoch: #54   | Batch: #30   | Batch Loss: 0.3537498414516449            | Epoch Loss: 0.483862700064977             
Epoch: #54   | Batch: #31   | Batch Loss: 0.5071672797203064            | Epoch Loss: 0.48461446069901987           
Epoch: #54   | Batch: #32   | Batch Loss: 0.6014785170555115            | Epoch Loss: 0.48826646246016026           
Epoch: #54   | Batch: #33   | Batch Loss: 0.5445978045463562            | Epoch Loss: 0.48997347282640863           
Epoch: #54   | Batch: #34   | Batch Loss: 0.6039518117904663            | Epoch Loss: 0.4933257769135868            
Epoch: #54   | Batch: #35   | Batch Loss: 0.5323143005371094            | Epoch Loss: 0.49443973473140174           
Epoch: #54   | Batch: #36   | Batch Loss: 0.3873659372329712            | Epoch Loss: 0.4914654625786675            
Epoch: #54   | Batch: #37   | Batch Loss: 0.5368394255638123            | Epoch Loss: 0.4926917859025904            
Epoch: #54   | Batch: #38   | Batch Loss: 0.5244041681289673            | Epoch Loss: 0.4935263222769687            
Epoch: #54   | Batch: #39   | Batch Loss: 0.5551584362983704            | Epoch Loss: 0.4951066328929021            
Epoch: #54   | Batch: #40   | Batch Loss: 0.4991416931152344            | Epoch Loss: 0.49520750939846037           
Epoch: #54   | Batch: #41   | Batch Loss: 0.46743860840797424           | Epoch Loss: 0.49453021913039974           
Epoch: #54   | Batch: #42   | Batch Loss: 0.46955984830856323           | Epoch Loss: 0.4939356864917846            
Epoch: #54   | Batch: #43   | Batch Loss: 0.5908327698707581            | Epoch Loss: 0.49618910703548164           

Classifier Validation Epoch #54
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8454545454545455            
Batch: #2    | Top-1 Accuracy: 0.8477272727272727            
Batch: #3    | Top-1 Accuracy: 0.846969696969697             
Batch: #4    | Top-1 Accuracy: 0.8397727272727273            
Batch: #5    | Top-1 Accuracy: 0.8336363636363636            
Batch: #6    | Top-1 Accuracy: 0.8356060606060606            
Batch: #7    | Top-1 Accuracy: 0.8344155844155844            
Batch: #8    | Top-1 Accuracy: 0.8386363636363636            
Batch: #9    | Top-1 Accuracy: 0.8388888888888889            
Batch: #10   | Top-1 Accuracy: 0.8390909090909091            
Batch: #11   | Top-1 Accuracy: 0.8429752066115703            
Batch: #12   | Top-1 Accuracy: 0.8435606060606061            
Batch: #13   | Top-1 Accuracy: 0.8416083916083916            
Batch: #14   | Top-1 Accuracy: 0.8425324675324676            
Batch: #15   | Top-1 Accuracy: 0.8427272727272729            
Batch: #16   | Top-1 Accuracy: 0.8423295454545455            
Batch: #17   | Top-1 Accuracy: 0.8435828877005348            

Classifier Training Epoch #55
------------------------------------------
Epoch: #55   | Batch: #1    | Batch Loss: 0.5318699479103088            | Epoch Loss: 0.5318699479103088            
Epoch: #55   | Batch: #2    | Batch Loss: 0.5425838828086853            | Epoch Loss: 0.5372269153594971            
Epoch: #55   | Batch: #3    | Batch Loss: 0.4697149097919464            | Epoch Loss: 0.5147229135036469            
Epoch: #55   | Batch: #4    | Batch Loss: 0.4389578700065613            | Epoch Loss: 0.49578165262937546           
Epoch: #55   | Batch: #5    | Batch Loss: 0.48947232961654663           | Epoch Loss: 0.4945197880268097            
Epoch: #55   | Batch: #6    | Batch Loss: 0.5291511416435242            | Epoch Loss: 0.5002916802962621            
Epoch: #55   | Batch: #7    | Batch Loss: 0.5240210294723511            | Epoch Loss: 0.5036815873214177            
Epoch: #55   | Batch: #8    | Batch Loss: 0.5465660691261292            | Epoch Loss: 0.5090421475470066            
Epoch: #55   | Batch: #9    | Batch Loss: 0.4777527451515198            | Epoch Loss: 0.5055655472808414            
Epoch: #55   | Batch: #10   | Batch Loss: 0.5797662734985352            | Epoch Loss: 0.5129856199026108            
Epoch: #55   | Batch: #11   | Batch Loss: 0.5385182499885559            | Epoch Loss: 0.5153067680922422            
Epoch: #55   | Batch: #12   | Batch Loss: 0.5352000594139099            | Epoch Loss: 0.5169645423690478            
Epoch: #55   | Batch: #13   | Batch Loss: 0.46031343936920166           | Epoch Loss: 0.5126067652152135            
Epoch: #55   | Batch: #14   | Batch Loss: 0.5081019401550293            | Epoch Loss: 0.5122849919966289            
Epoch: #55   | Batch: #15   | Batch Loss: 0.5137538313865662            | Epoch Loss: 0.5123829146226248            
Epoch: #55   | Batch: #16   | Batch Loss: 0.44542649388313293           | Epoch Loss: 0.5081981383264065            
Epoch: #55   | Batch: #17   | Batch Loss: 0.5217950940132141            | Epoch Loss: 0.5089979592491599            
Epoch: #55   | Batch: #18   | Batch Loss: 0.6226362586021423            | Epoch Loss: 0.5153111981021034            
Epoch: #55   | Batch: #19   | Batch Loss: 0.46548184752464294           | Epoch Loss: 0.5126886007032896            
Epoch: #55   | Batch: #20   | Batch Loss: 0.5185726881027222            | Epoch Loss: 0.5129828050732612            
Epoch: #55   | Batch: #21   | Batch Loss: 0.5772534608840942            | Epoch Loss: 0.5160433124928248            
Epoch: #55   | Batch: #22   | Batch Loss: 0.47404760122299194           | Epoch Loss: 0.5141344165260141            
Epoch: #55   | Batch: #23   | Batch Loss: 0.49023306369781494           | Epoch Loss: 0.5130952272726141            
Epoch: #55   | Batch: #24   | Batch Loss: 0.5302191376686096            | Epoch Loss: 0.513808723539114             
Epoch: #55   | Batch: #25   | Batch Loss: 0.4461838901042938            | Epoch Loss: 0.5111037302017212            
Epoch: #55   | Batch: #26   | Batch Loss: 0.5411291122436523            | Epoch Loss: 0.5122585525879493            
Epoch: #55   | Batch: #27   | Batch Loss: 0.5212889313697815            | Epoch Loss: 0.5125930110613505            
Epoch: #55   | Batch: #28   | Batch Loss: 0.6290523409843445            | Epoch Loss: 0.5167522728443146            
Epoch: #55   | Batch: #29   | Batch Loss: 0.4381614923477173            | Epoch Loss: 0.5140422459306389            
Epoch: #55   | Batch: #30   | Batch Loss: 0.3986411690711975            | Epoch Loss: 0.5101955433686575            
Epoch: #55   | Batch: #31   | Batch Loss: 0.522284209728241             | Epoch Loss: 0.5105855003479989            
Epoch: #55   | Batch: #32   | Batch Loss: 0.568401038646698             | Epoch Loss: 0.5123922359198332            
Epoch: #55   | Batch: #33   | Batch Loss: 0.5580522418022156            | Epoch Loss: 0.5137758724617235            
Epoch: #55   | Batch: #34   | Batch Loss: 0.7299549579620361            | Epoch Loss: 0.5201340808587915            
Epoch: #55   | Batch: #35   | Batch Loss: 0.6207020282745361            | Epoch Loss: 0.5230074507849557            
Epoch: #55   | Batch: #36   | Batch Loss: 0.5309237837791443            | Epoch Loss: 0.5232273489236832            
Epoch: #55   | Batch: #37   | Batch Loss: 0.4794211983680725            | Epoch Loss: 0.5220433989086667            
Epoch: #55   | Batch: #38   | Batch Loss: 0.4697208106517792            | Epoch Loss: 0.5206664886913801            
Epoch: #55   | Batch: #39   | Batch Loss: 0.4232221841812134            | Epoch Loss: 0.518167916780863             
Epoch: #55   | Batch: #40   | Batch Loss: 0.33329081535339355           | Epoch Loss: 0.5135459892451764            
Epoch: #55   | Batch: #41   | Batch Loss: 0.4245971739292145            | Epoch Loss: 0.511376505944787             
Epoch: #55   | Batch: #42   | Batch Loss: 0.3751739263534546            | Epoch Loss: 0.5081335873830886            
Epoch: #55   | Batch: #43   | Batch Loss: 0.5512074828147888            | Epoch Loss: 0.5091353058815002            

Classifier Validation Epoch #55
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8363636363636363            
Batch: #2    | Top-1 Accuracy: 0.834090909090909             
Batch: #3    | Top-1 Accuracy: 0.8454545454545453            
Batch: #4    | Top-1 Accuracy: 0.85                          
Batch: #5    | Top-1 Accuracy: 0.8427272727272728            
Batch: #6    | Top-1 Accuracy: 0.8484848484848485            
Batch: #7    | Top-1 Accuracy: 0.8448051948051949            
Batch: #8    | Top-1 Accuracy: 0.8471590909090909            
Batch: #9    | Top-1 Accuracy: 0.846969696969697             
Batch: #10   | Top-1 Accuracy: 0.8472727272727273            
Batch: #11   | Top-1 Accuracy: 0.8462809917355372            
Batch: #12   | Top-1 Accuracy: 0.8473484848484848            
Batch: #13   | Top-1 Accuracy: 0.8461538461538461            
Batch: #14   | Top-1 Accuracy: 0.8461038961038961            
Batch: #15   | Top-1 Accuracy: 0.843939393939394             
Batch: #16   | Top-1 Accuracy: 0.8460227272727272            
Batch: #17   | Top-1 Accuracy: 0.8446524064171123            

Classifier Training Epoch #56
------------------------------------------
Epoch: #56   | Batch: #1    | Batch Loss: 0.4380738139152527            | Epoch Loss: 0.4380738139152527            
Epoch: #56   | Batch: #2    | Batch Loss: 0.4728398323059082            | Epoch Loss: 0.45545682311058044           
Epoch: #56   | Batch: #3    | Batch Loss: 0.47706419229507446           | Epoch Loss: 0.4626592795054118            
Epoch: #56   | Batch: #4    | Batch Loss: 0.5313636660575867            | Epoch Loss: 0.4798353761434555            
Epoch: #56   | Batch: #5    | Batch Loss: 0.4688190817832947            | Epoch Loss: 0.47763211727142335           
Epoch: #56   | Batch: #6    | Batch Loss: 0.5313626527786255            | Epoch Loss: 0.4865872065226237            
Epoch: #56   | Batch: #7    | Batch Loss: 0.5137731432914734            | Epoch Loss: 0.4904709117753165            
Epoch: #56   | Batch: #8    | Batch Loss: 0.4820675551891327            | Epoch Loss: 0.48942049220204353           
Epoch: #56   | Batch: #9    | Batch Loss: 0.5603649616241455            | Epoch Loss: 0.49730321102672154           
Epoch: #56   | Batch: #10   | Batch Loss: 0.4166751801967621            | Epoch Loss: 0.4892404079437256            
Epoch: #56   | Batch: #11   | Batch Loss: 0.43350279331207275           | Epoch Loss: 0.4841733520681208            
Epoch: #56   | Batch: #12   | Batch Loss: 0.4865399897098541            | Epoch Loss: 0.48437057187159854           
Epoch: #56   | Batch: #13   | Batch Loss: 0.4569503366947174            | Epoch Loss: 0.4822613230118385            
Epoch: #56   | Batch: #14   | Batch Loss: 0.4657289683818817            | Epoch Loss: 0.48108044053827015           
Epoch: #56   | Batch: #15   | Batch Loss: 0.4186934530735016            | Epoch Loss: 0.4769213080406189            
Epoch: #56   | Batch: #16   | Batch Loss: 0.592232882976532             | Epoch Loss: 0.48412828147411346           
Epoch: #56   | Batch: #17   | Batch Loss: 0.45807838439941406           | Epoch Loss: 0.48259593458736644           
Epoch: #56   | Batch: #18   | Batch Loss: 0.4276576638221741            | Epoch Loss: 0.47954380843374467           
Epoch: #56   | Batch: #19   | Batch Loss: 0.5367266535758972            | Epoch Loss: 0.482553431862279             
Epoch: #56   | Batch: #20   | Batch Loss: 0.48463085293769836           | Epoch Loss: 0.48265730291604997           
Epoch: #56   | Batch: #21   | Batch Loss: 0.461232990026474             | Epoch Loss: 0.4816370975403559            
Epoch: #56   | Batch: #22   | Batch Loss: 0.571050226688385             | Epoch Loss: 0.4857013306834481            
Epoch: #56   | Batch: #23   | Batch Loss: 0.5468135476112366            | Epoch Loss: 0.48835838359335193           
Epoch: #56   | Batch: #24   | Batch Loss: 0.5636986494064331            | Epoch Loss: 0.49149756133556366           
Epoch: #56   | Batch: #25   | Batch Loss: 0.46163997054100037           | Epoch Loss: 0.49030325770378114           
Epoch: #56   | Batch: #26   | Batch Loss: 0.545839786529541             | Epoch Loss: 0.49243927804323345           
Epoch: #56   | Batch: #27   | Batch Loss: 0.49449098110198975           | Epoch Loss: 0.4925152670454096            
Epoch: #56   | Batch: #28   | Batch Loss: 0.38166725635528564           | Epoch Loss: 0.4885564095207623            
Epoch: #56   | Batch: #29   | Batch Loss: 0.4347284734249115            | Epoch Loss: 0.48670027379331915           
Epoch: #56   | Batch: #30   | Batch Loss: 0.4585436284542084            | Epoch Loss: 0.48576171894868214           
Epoch: #56   | Batch: #31   | Batch Loss: 0.5304123759269714            | Epoch Loss: 0.48720206272217537           
Epoch: #56   | Batch: #32   | Batch Loss: 0.4980132579803467            | Epoch Loss: 0.4875399125739932            
Epoch: #56   | Batch: #33   | Batch Loss: 0.5659652352333069            | Epoch Loss: 0.4899164375030633            
Epoch: #56   | Batch: #34   | Batch Loss: 0.4427330791950226            | Epoch Loss: 0.48852869167047386           
Epoch: #56   | Batch: #35   | Batch Loss: 0.5251550078392029            | Epoch Loss: 0.4895751578467233            
Epoch: #56   | Batch: #36   | Batch Loss: 0.4271417558193207            | Epoch Loss: 0.4878408966792954            
Epoch: #56   | Batch: #37   | Batch Loss: 0.4450187385082245            | Epoch Loss: 0.4866835410530503            
Epoch: #56   | Batch: #38   | Batch Loss: 0.5254999399185181            | Epoch Loss: 0.4877050252337205            
Epoch: #56   | Batch: #39   | Batch Loss: 0.4431439936161041            | Epoch Loss: 0.4865624346794226            
Epoch: #56   | Batch: #40   | Batch Loss: 0.5142759680747986            | Epoch Loss: 0.48725527301430704           
Epoch: #56   | Batch: #41   | Batch Loss: 0.5166935324668884            | Epoch Loss: 0.48797327934241874           
Epoch: #56   | Batch: #42   | Batch Loss: 0.4695630371570587            | Epoch Loss: 0.48753494024276733           
Epoch: #56   | Batch: #43   | Batch Loss: 0.39452290534973145           | Epoch Loss: 0.48537186966385953           

Classifier Validation Epoch #56
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8227272727272728            
Batch: #2    | Top-1 Accuracy: 0.8545454545454545            
Batch: #3    | Top-1 Accuracy: 0.8545454545454545            
Batch: #4    | Top-1 Accuracy: 0.85                          
Batch: #5    | Top-1 Accuracy: 0.8427272727272728            
Batch: #6    | Top-1 Accuracy: 0.8416666666666667            
Batch: #7    | Top-1 Accuracy: 0.8428571428571427            
Batch: #8    | Top-1 Accuracy: 0.8443181818181817            
Batch: #9    | Top-1 Accuracy: 0.8454545454545453            
Batch: #10   | Top-1 Accuracy: 0.8486363636363636            
Batch: #11   | Top-1 Accuracy: 0.8495867768595041            
Batch: #12   | Top-1 Accuracy: 0.8488636363636365            
Batch: #13   | Top-1 Accuracy: 0.8493006993006994            
Batch: #14   | Top-1 Accuracy: 0.8509740259740262            
Batch: #15   | Top-1 Accuracy: 0.8500000000000001            
Batch: #16   | Top-1 Accuracy: 0.8485795454545455            
Batch: #17   | Top-1 Accuracy: 0.8494652406417113            

Classifier Training Epoch #57
------------------------------------------
Epoch: #57   | Batch: #1    | Batch Loss: 0.3874310851097107            | Epoch Loss: 0.3874310851097107            
Epoch: #57   | Batch: #2    | Batch Loss: 0.5411004424095154            | Epoch Loss: 0.46426576375961304           
Epoch: #57   | Batch: #3    | Batch Loss: 0.38636496663093567           | Epoch Loss: 0.43829883138338727           
Epoch: #57   | Batch: #4    | Batch Loss: 0.5165247321128845            | Epoch Loss: 0.45785530656576157           
Epoch: #57   | Batch: #5    | Batch Loss: 0.5047612190246582            | Epoch Loss: 0.4672364890575409            
Epoch: #57   | Batch: #6    | Batch Loss: 0.47909340262413025           | Epoch Loss: 0.4692126413186391            
Epoch: #57   | Batch: #7    | Batch Loss: 0.514411449432373             | Epoch Loss: 0.4756696139063154            
Epoch: #57   | Batch: #8    | Batch Loss: 0.40405145287513733           | Epoch Loss: 0.46671734377741814           
Epoch: #57   | Batch: #9    | Batch Loss: 0.5933961272239685            | Epoch Loss: 0.4807927641603682            
Epoch: #57   | Batch: #10   | Batch Loss: 0.4728696346282959            | Epoch Loss: 0.48000045120716095           
Epoch: #57   | Batch: #11   | Batch Loss: 0.527955949306488             | Epoch Loss: 0.48436004194346344           
Epoch: #57   | Batch: #12   | Batch Loss: 0.518135666847229             | Epoch Loss: 0.48717467735211056           
Epoch: #57   | Batch: #13   | Batch Loss: 0.498383492231369             | Epoch Loss: 0.4880368938812843            
Epoch: #57   | Batch: #14   | Batch Loss: 0.5108242630958557            | Epoch Loss: 0.4896645631108965            
Epoch: #57   | Batch: #15   | Batch Loss: 0.5033876299858093            | Epoch Loss: 0.49057943423589073           
Epoch: #57   | Batch: #16   | Batch Loss: 0.4628313481807709            | Epoch Loss: 0.4888451788574457            
Epoch: #57   | Batch: #17   | Batch Loss: 0.5334423184394836            | Epoch Loss: 0.4914685400093303            
Epoch: #57   | Batch: #18   | Batch Loss: 0.49678751826286316           | Epoch Loss: 0.49176403880119324           
Epoch: #57   | Batch: #19   | Batch Loss: 0.5545158386230469            | Epoch Loss: 0.4950667651076066            
Epoch: #57   | Batch: #20   | Batch Loss: 0.47847452759742737           | Epoch Loss: 0.4942371532320976            
Epoch: #57   | Batch: #21   | Batch Loss: 0.5417761206626892            | Epoch Loss: 0.49650091358593534           
Epoch: #57   | Batch: #22   | Batch Loss: 0.5815199613571167            | Epoch Loss: 0.5003654157573526            
Epoch: #57   | Batch: #23   | Batch Loss: 0.47007277607917786           | Epoch Loss: 0.49904834446699725           
Epoch: #57   | Batch: #24   | Batch Loss: 0.5151538848876953            | Epoch Loss: 0.49971940865119296           
Epoch: #57   | Batch: #25   | Batch Loss: 0.3823360800743103            | Epoch Loss: 0.49502407550811767           
Epoch: #57   | Batch: #26   | Batch Loss: 0.5089377760887146            | Epoch Loss: 0.4955592178381406            
Epoch: #57   | Batch: #27   | Batch Loss: 0.6737756133079529            | Epoch Loss: 0.5021598250777634            
Epoch: #57   | Batch: #28   | Batch Loss: 0.5537145137786865            | Epoch Loss: 0.5040010639599392            
Epoch: #57   | Batch: #29   | Batch Loss: 0.40278393030166626           | Epoch Loss: 0.5005108179717228            
Epoch: #57   | Batch: #30   | Batch Loss: 0.5008509159088135            | Epoch Loss: 0.5005221545696259            
Epoch: #57   | Batch: #31   | Batch Loss: 0.45490047335624695           | Epoch Loss: 0.4990504874337104            
Epoch: #57   | Batch: #32   | Batch Loss: 0.5945777297019958            | Epoch Loss: 0.5020357137545943            
Epoch: #57   | Batch: #33   | Batch Loss: 0.4791063070297241            | Epoch Loss: 0.50134088324778              
Epoch: #57   | Batch: #34   | Batch Loss: 0.4700802266597748            | Epoch Loss: 0.5004214521716622            
Epoch: #57   | Batch: #35   | Batch Loss: 0.5337222218513489            | Epoch Loss: 0.5013729027339391            
Epoch: #57   | Batch: #36   | Batch Loss: 0.4826503396034241            | Epoch Loss: 0.5008528315358691            
Epoch: #57   | Batch: #37   | Batch Loss: 0.4363718330860138            | Epoch Loss: 0.4991101018480352            
Epoch: #57   | Batch: #38   | Batch Loss: 0.4667017459869385            | Epoch Loss: 0.4982572503780064            
Epoch: #57   | Batch: #39   | Batch Loss: 0.4571840465068817            | Epoch Loss: 0.4972040913043878            
Epoch: #57   | Batch: #40   | Batch Loss: 0.4269629716873169            | Epoch Loss: 0.495448063313961             
Epoch: #57   | Batch: #41   | Batch Loss: 0.4785054922103882            | Epoch Loss: 0.4950348298724105            
Epoch: #57   | Batch: #42   | Batch Loss: 0.5674448609352112            | Epoch Loss: 0.4967588782310486            
Epoch: #57   | Batch: #43   | Batch Loss: 0.5549795627593994            | Epoch Loss: 0.49811284763868463           

Classifier Validation Epoch #57
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8181818181818182            
Batch: #2    | Top-1 Accuracy: 0.8340909090909091            
Batch: #3    | Top-1 Accuracy: 0.8454545454545453            
Batch: #4    | Top-1 Accuracy: 0.8465909090909091            
Batch: #5    | Top-1 Accuracy: 0.8427272727272728            
Batch: #6    | Top-1 Accuracy: 0.846969696969697             
Batch: #7    | Top-1 Accuracy: 0.8564935064935065            
Batch: #8    | Top-1 Accuracy: 0.8551136363636365            
Batch: #9    | Top-1 Accuracy: 0.8560606060606061            
Batch: #10   | Top-1 Accuracy: 0.8577272727272728            
Batch: #11   | Top-1 Accuracy: 0.8578512396694216            
Batch: #12   | Top-1 Accuracy: 0.8590909090909092            
Batch: #13   | Top-1 Accuracy: 0.8559440559440561            
Batch: #14   | Top-1 Accuracy: 0.8532467532467534            
Batch: #15   | Top-1 Accuracy: 0.8515151515151517            
Batch: #16   | Top-1 Accuracy: 0.8517045454545455            
Batch: #17   | Top-1 Accuracy: 0.851336898395722             

Classifier Training Epoch #58
------------------------------------------
Epoch: #58   | Batch: #1    | Batch Loss: 0.3804249167442322            | Epoch Loss: 0.3804249167442322            
Epoch: #58   | Batch: #2    | Batch Loss: 0.42321500182151794           | Epoch Loss: 0.40181995928287506           
Epoch: #58   | Batch: #3    | Batch Loss: 0.5032368302345276            | Epoch Loss: 0.4356255829334259            
Epoch: #58   | Batch: #4    | Batch Loss: 0.5204659700393677            | Epoch Loss: 0.45683567970991135           
Epoch: #58   | Batch: #5    | Batch Loss: 0.49170640110969543           | Epoch Loss: 0.46380982398986814           
Epoch: #58   | Batch: #6    | Batch Loss: 0.3854069113731384            | Epoch Loss: 0.4507426718870799            
Epoch: #58   | Batch: #7    | Batch Loss: 0.4378592073917389            | Epoch Loss: 0.448902176959174             
Epoch: #58   | Batch: #8    | Batch Loss: 0.5074232816696167            | Epoch Loss: 0.45621731504797935           
Epoch: #58   | Batch: #9    | Batch Loss: 0.39823177456855774           | Epoch Loss: 0.44977447721693253           
Epoch: #58   | Batch: #10   | Batch Loss: 0.4434194266796112            | Epoch Loss: 0.44913897216320037           
Epoch: #58   | Batch: #11   | Batch Loss: 0.4254385232925415            | Epoch Loss: 0.4469843859022314            
Epoch: #58   | Batch: #12   | Batch Loss: 0.5389108657836914            | Epoch Loss: 0.45464492589235306           
Epoch: #58   | Batch: #13   | Batch Loss: 0.611586332321167             | Epoch Loss: 0.4667173417714926            
Epoch: #58   | Batch: #14   | Batch Loss: 0.4333430230617523            | Epoch Loss: 0.464333461863654             
Epoch: #58   | Batch: #15   | Batch Loss: 0.36892908811569214           | Epoch Loss: 0.45797317028045653           
Epoch: #58   | Batch: #16   | Batch Loss: 0.3781657814979553            | Epoch Loss: 0.4529852084815502            
Epoch: #58   | Batch: #17   | Batch Loss: 0.5701795220375061            | Epoch Loss: 0.45987899163190055           
Epoch: #58   | Batch: #18   | Batch Loss: 0.619949221611023             | Epoch Loss: 0.4687717821862962            
Epoch: #58   | Batch: #19   | Batch Loss: 0.39628592133522034           | Epoch Loss: 0.4649567368783449            
Epoch: #58   | Batch: #20   | Batch Loss: 0.6543444991111755            | Epoch Loss: 0.47442612498998643           
Epoch: #58   | Batch: #21   | Batch Loss: 0.509414792060852             | Epoch Loss: 0.47609225199336097           
Epoch: #58   | Batch: #22   | Batch Loss: 0.43413713574409485           | Epoch Loss: 0.474185201254758             
Epoch: #58   | Batch: #23   | Batch Loss: 0.3826696574687958            | Epoch Loss: 0.4702062645684118            
Epoch: #58   | Batch: #24   | Batch Loss: 0.4736682176589966            | Epoch Loss: 0.4703505126138528            
Epoch: #58   | Batch: #25   | Batch Loss: 0.5124187469482422            | Epoch Loss: 0.4720332419872284            
Epoch: #58   | Batch: #26   | Batch Loss: 0.6015123128890991            | Epoch Loss: 0.47701320625268495           
Epoch: #58   | Batch: #27   | Batch Loss: 0.44500163197517395           | Epoch Loss: 0.47582759239055494           
Epoch: #58   | Batch: #28   | Batch Loss: 0.513209879398346             | Epoch Loss: 0.4771626740694046            
Epoch: #58   | Batch: #29   | Batch Loss: 0.588631808757782             | Epoch Loss: 0.4810064373345211            
Epoch: #58   | Batch: #30   | Batch Loss: 0.4613207280635834            | Epoch Loss: 0.4803502470254898            
Epoch: #58   | Batch: #31   | Batch Loss: 0.45168647170066833           | Epoch Loss: 0.4794256091117859            
Epoch: #58   | Batch: #32   | Batch Loss: 0.48900410532951355           | Epoch Loss: 0.4797249371185899            
Epoch: #58   | Batch: #33   | Batch Loss: 0.6543392539024353            | Epoch Loss: 0.48501628005143366           
Epoch: #58   | Batch: #34   | Batch Loss: 0.3983917832374573            | Epoch Loss: 0.48246850073337555           
Epoch: #58   | Batch: #35   | Batch Loss: 0.6432256698608398            | Epoch Loss: 0.48706156270844597           
Epoch: #58   | Batch: #36   | Batch Loss: 0.6078965663909912            | Epoch Loss: 0.49041809058851665           
Epoch: #58   | Batch: #37   | Batch Loss: 0.6082422137260437            | Epoch Loss: 0.4936025263489904            
Epoch: #58   | Batch: #38   | Batch Loss: 0.4578687250614166            | Epoch Loss: 0.4926621631572121            
Epoch: #58   | Batch: #39   | Batch Loss: 0.49678054451942444           | Epoch Loss: 0.4927677626793201            
Epoch: #58   | Batch: #40   | Batch Loss: 0.47106319665908813           | Epoch Loss: 0.4922251485288143            
Epoch: #58   | Batch: #41   | Batch Loss: 0.6275545954704285            | Epoch Loss: 0.49552586674690247           
Epoch: #58   | Batch: #42   | Batch Loss: 0.5296927690505981            | Epoch Loss: 0.49633936442079996           
Epoch: #58   | Batch: #43   | Batch Loss: 0.6675406098365784            | Epoch Loss: 0.5003207887327948            

Classifier Validation Epoch #58
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8590909090909091            
Batch: #2    | Top-1 Accuracy: 0.8568181818181818            
Batch: #3    | Top-1 Accuracy: 0.8651515151515152            
Batch: #4    | Top-1 Accuracy: 0.8636363636363638            
Batch: #5    | Top-1 Accuracy: 0.8545454545454547            
Batch: #6    | Top-1 Accuracy: 0.8492424242424242            
Batch: #7    | Top-1 Accuracy: 0.8506493506493508            
Batch: #8    | Top-1 Accuracy: 0.8471590909090909            
Batch: #9    | Top-1 Accuracy: 0.8444444444444444            
Batch: #10   | Top-1 Accuracy: 0.8427272727272728            
Batch: #11   | Top-1 Accuracy: 0.8380165289256198            
Batch: #12   | Top-1 Accuracy: 0.8390151515151515            
Batch: #13   | Top-1 Accuracy: 0.8402097902097901            
Batch: #14   | Top-1 Accuracy: 0.8399350649350649            
Batch: #15   | Top-1 Accuracy: 0.8390909090909091            
Batch: #16   | Top-1 Accuracy: 0.8397727272727273            
Batch: #17   | Top-1 Accuracy: 0.8427807486631016            

Classifier Training Epoch #59
------------------------------------------
Epoch: #59   | Batch: #1    | Batch Loss: 0.4049045443534851            | Epoch Loss: 0.4049045443534851            
Epoch: #59   | Batch: #2    | Batch Loss: 0.4081679880619049            | Epoch Loss: 0.406536266207695             
Epoch: #59   | Batch: #3    | Batch Loss: 0.44880107045173645           | Epoch Loss: 0.4206245342890422            
Epoch: #59   | Batch: #4    | Batch Loss: 0.6422035694122314            | Epoch Loss: 0.4760192930698395            
Epoch: #59   | Batch: #5    | Batch Loss: 0.5417320728302002            | Epoch Loss: 0.4891618490219116            
Epoch: #59   | Batch: #6    | Batch Loss: 0.4042961895465851            | Epoch Loss: 0.47501757244269055           
Epoch: #59   | Batch: #7    | Batch Loss: 0.4969145655632019            | Epoch Loss: 0.4781457143170493            
Epoch: #59   | Batch: #8    | Batch Loss: 0.48096296191215515           | Epoch Loss: 0.47849787026643753           
Epoch: #59   | Batch: #9    | Batch Loss: 0.5654391050338745            | Epoch Loss: 0.4881580074628194            
Epoch: #59   | Batch: #10   | Batch Loss: 0.5458001494407654            | Epoch Loss: 0.493922221660614             
Epoch: #59   | Batch: #11   | Batch Loss: 0.4099350869655609            | Epoch Loss: 0.48628702759742737           
Epoch: #59   | Batch: #12   | Batch Loss: 0.41829368472099304           | Epoch Loss: 0.48062091569105786           
Epoch: #59   | Batch: #13   | Batch Loss: 0.5460717082023621            | Epoch Loss: 0.48565559203808123           
Epoch: #59   | Batch: #14   | Batch Loss: 0.5626176595687866            | Epoch Loss: 0.49115288257598877           
Epoch: #59   | Batch: #15   | Batch Loss: 0.5860387682914734            | Epoch Loss: 0.4974786082903544            
Epoch: #59   | Batch: #16   | Batch Loss: 0.5640361309051514            | Epoch Loss: 0.5016384534537792            
Epoch: #59   | Batch: #17   | Batch Loss: 0.4416751265525818            | Epoch Loss: 0.49811119893017936           
Epoch: #59   | Batch: #18   | Batch Loss: 0.5249992609024048            | Epoch Loss: 0.4996049801508586            
Epoch: #59   | Batch: #19   | Batch Loss: 0.47458264231681824           | Epoch Loss: 0.49828801500169856           
Epoch: #59   | Batch: #20   | Batch Loss: 0.4180290699005127            | Epoch Loss: 0.49427506774663926           
Epoch: #59   | Batch: #21   | Batch Loss: 0.4892469346523285            | Epoch Loss: 0.4940356328373864            
Epoch: #59   | Batch: #22   | Batch Loss: 0.41847488284111023           | Epoch Loss: 0.4906010532921011            
Epoch: #59   | Batch: #23   | Batch Loss: 0.4222578704357147            | Epoch Loss: 0.4876296105592147            
Epoch: #59   | Batch: #24   | Batch Loss: 0.4240172207355499            | Epoch Loss: 0.484979094316562             
Epoch: #59   | Batch: #25   | Batch Loss: 0.48772698640823364           | Epoch Loss: 0.48508901000022886           
Epoch: #59   | Batch: #26   | Batch Loss: 0.4601227641105652            | Epoch Loss: 0.48412876977370334           
Epoch: #59   | Batch: #27   | Batch Loss: 0.5558428168296814            | Epoch Loss: 0.48678484559059143           
Epoch: #59   | Batch: #28   | Batch Loss: 0.5309633016586304            | Epoch Loss: 0.4883626475930214            
Epoch: #59   | Batch: #29   | Batch Loss: 0.5129408836364746            | Epoch Loss: 0.4892101729738301            
Epoch: #59   | Batch: #30   | Batch Loss: 0.5852499604225159            | Epoch Loss: 0.49241149922211963           
Epoch: #59   | Batch: #31   | Batch Loss: 0.5247078537940979            | Epoch Loss: 0.4934533171115383            
Epoch: #59   | Batch: #32   | Batch Loss: 0.5112508535385132            | Epoch Loss: 0.49400949012488127           
Epoch: #59   | Batch: #33   | Batch Loss: 0.54753178358078              | Epoch Loss: 0.49563137780536304           
Epoch: #59   | Batch: #34   | Batch Loss: 0.5960330367088318            | Epoch Loss: 0.4985843677731121            
Epoch: #59   | Batch: #35   | Batch Loss: 0.39915505051612854           | Epoch Loss: 0.4957435301371983            
Epoch: #59   | Batch: #36   | Batch Loss: 0.5316433906555176            | Epoch Loss: 0.4967407484849294            
Epoch: #59   | Batch: #37   | Batch Loss: 0.3674060106277466            | Epoch Loss: 0.4932452150293299            
Epoch: #59   | Batch: #38   | Batch Loss: 0.6120085716247559            | Epoch Loss: 0.49637056651868317           
Epoch: #59   | Batch: #39   | Batch Loss: 0.5112655162811279            | Epoch Loss: 0.4967524883074638            
Epoch: #59   | Batch: #40   | Batch Loss: 0.5001499056816101            | Epoch Loss: 0.4968374237418175            
Epoch: #59   | Batch: #41   | Batch Loss: 0.3922984302043915            | Epoch Loss: 0.49428769219212415           
Epoch: #59   | Batch: #42   | Batch Loss: 0.5077615976333618            | Epoch Loss: 0.49460849946453456           
Epoch: #59   | Batch: #43   | Batch Loss: 0.5050579905509949            | Epoch Loss: 0.49485151088514995           

Classifier Validation Epoch #59
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8590909090909091            
Batch: #2    | Top-1 Accuracy: 0.8704545454545455            
Batch: #3    | Top-1 Accuracy: 0.8575757575757575            
Batch: #4    | Top-1 Accuracy: 0.8477272727272728            
Batch: #5    | Top-1 Accuracy: 0.8518181818181819            
Batch: #6    | Top-1 Accuracy: 0.8575757575757578            
Batch: #7    | Top-1 Accuracy: 0.8525974025974027            
Batch: #8    | Top-1 Accuracy: 0.852840909090909             
Batch: #9    | Top-1 Accuracy: 0.8494949494949494            
Batch: #10   | Top-1 Accuracy: 0.8509090909090908            
Batch: #11   | Top-1 Accuracy: 0.8475206611570247            
Batch: #12   | Top-1 Accuracy: 0.8477272727272727            
Batch: #13   | Top-1 Accuracy: 0.8482517482517482            
Batch: #14   | Top-1 Accuracy: 0.8474025974025974            
Batch: #15   | Top-1 Accuracy: 0.8466666666666666            
Batch: #16   | Top-1 Accuracy: 0.8477272727272727            
Batch: #17   | Top-1 Accuracy: 0.8478609625668448            

Classifier Training Epoch #60
------------------------------------------
Epoch: #60   | Batch: #1    | Batch Loss: 0.5017653107643127            | Epoch Loss: 0.5017653107643127            
Epoch: #60   | Batch: #2    | Batch Loss: 0.49105361104011536           | Epoch Loss: 0.49640946090221405           
Epoch: #60   | Batch: #3    | Batch Loss: 0.611788272857666             | Epoch Loss: 0.5348690648873647            
Epoch: #60   | Batch: #4    | Batch Loss: 0.5594714283943176            | Epoch Loss: 0.5410196557641029            
Epoch: #60   | Batch: #5    | Batch Loss: 0.5619252920150757            | Epoch Loss: 0.5452007830142975            
Epoch: #60   | Batch: #6    | Batch Loss: 0.469308465719223             | Epoch Loss: 0.5325520634651184            
Epoch: #60   | Batch: #7    | Batch Loss: 0.4332485496997833            | Epoch Loss: 0.5183658472129277            
Epoch: #60   | Batch: #8    | Batch Loss: 0.46342334151268005           | Epoch Loss: 0.5114980340003967            
Epoch: #60   | Batch: #9    | Batch Loss: 0.4032518267631531            | Epoch Loss: 0.49947067764070296           
Epoch: #60   | Batch: #10   | Batch Loss: 0.5623155236244202            | Epoch Loss: 0.5057551622390747            
Epoch: #60   | Batch: #11   | Batch Loss: 0.5763949751853943            | Epoch Loss: 0.5121769634160128            
Epoch: #60   | Batch: #12   | Batch Loss: 0.5306816101074219            | Epoch Loss: 0.5137190173069636            
Epoch: #60   | Batch: #13   | Batch Loss: 0.5086116790771484            | Epoch Loss: 0.5133261451354394            
Epoch: #60   | Batch: #14   | Batch Loss: 0.48739588260650635           | Epoch Loss: 0.5114739835262299            
Epoch: #60   | Batch: #15   | Batch Loss: 0.43498992919921875           | Epoch Loss: 0.5063750465710958            
Epoch: #60   | Batch: #16   | Batch Loss: 0.5658809542655945            | Epoch Loss: 0.510094165802002             
Epoch: #60   | Batch: #17   | Batch Loss: 0.48605582118034363           | Epoch Loss: 0.5086801455301397            
Epoch: #60   | Batch: #18   | Batch Loss: 0.43505504727363586           | Epoch Loss: 0.5045898622936673            
Epoch: #60   | Batch: #19   | Batch Loss: 0.35691019892692566           | Epoch Loss: 0.49681724843225983           
Epoch: #60   | Batch: #20   | Batch Loss: 0.5423080921173096            | Epoch Loss: 0.4990917906165123            
Epoch: #60   | Batch: #21   | Batch Loss: 0.5308418869972229            | Epoch Loss: 0.5006036999679747            
Epoch: #60   | Batch: #22   | Batch Loss: 0.4423760771751404            | Epoch Loss: 0.4979569898410277            
Epoch: #60   | Batch: #23   | Batch Loss: 0.5803093910217285            | Epoch Loss: 0.5015375290227972            
Epoch: #60   | Batch: #24   | Batch Loss: 0.4841652810573578            | Epoch Loss: 0.5008136853575706            
Epoch: #60   | Batch: #25   | Batch Loss: 0.48404303193092346           | Epoch Loss: 0.5001428592205047            
Epoch: #60   | Batch: #26   | Batch Loss: 0.4394676089286804            | Epoch Loss: 0.4978091957477423            
Epoch: #60   | Batch: #27   | Batch Loss: 0.4702037572860718            | Epoch Loss: 0.49678677210101374           
Epoch: #60   | Batch: #28   | Batch Loss: 0.5168625712394714            | Epoch Loss: 0.49750376492738724           
Epoch: #60   | Batch: #29   | Batch Loss: 0.5122698545455933            | Epoch Loss: 0.4980129404314633            
Epoch: #60   | Batch: #30   | Batch Loss: 0.4430123269557953            | Epoch Loss: 0.496179586648941             
Epoch: #60   | Batch: #31   | Batch Loss: 0.4876329004764557            | Epoch Loss: 0.4959038870949899            
Epoch: #60   | Batch: #32   | Batch Loss: 0.30721035599708557           | Epoch Loss: 0.4900072142481804            
Epoch: #60   | Batch: #33   | Batch Loss: 0.5958266258239746            | Epoch Loss: 0.49321386308381054           
Epoch: #60   | Batch: #34   | Batch Loss: 0.49202418327331543           | Epoch Loss: 0.4931788725011489            
Epoch: #60   | Batch: #35   | Batch Loss: 0.49260103702545166           | Epoch Loss: 0.49316236291612897           
Epoch: #60   | Batch: #36   | Batch Loss: 0.5160331130027771            | Epoch Loss: 0.493797661529647             
Epoch: #60   | Batch: #37   | Batch Loss: 0.5333604216575623            | Epoch Loss: 0.4948669253168879            
Epoch: #60   | Batch: #38   | Batch Loss: 0.4198558032512665            | Epoch Loss: 0.4928929484204242            
Epoch: #60   | Batch: #39   | Batch Loss: 0.5105757713317871            | Epoch Loss: 0.4933463541361002            
Epoch: #60   | Batch: #40   | Batch Loss: 0.5229863524436951            | Epoch Loss: 0.49408735409379007           
Epoch: #60   | Batch: #41   | Batch Loss: 0.5000563263893127            | Epoch Loss: 0.49423293878392477           
Epoch: #60   | Batch: #42   | Batch Loss: 0.4886625409126282            | Epoch Loss: 0.4941003102631796            
Epoch: #60   | Batch: #43   | Batch Loss: 0.44717758893966675           | Epoch Loss: 0.4930090841858886            

Classifier Validation Epoch #60
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8454545454545455            
Batch: #2    | Top-1 Accuracy: 0.8636363636363636            
Batch: #3    | Top-1 Accuracy: 0.8545454545454545            
Batch: #4    | Top-1 Accuracy: 0.8477272727272727            
Batch: #5    | Top-1 Accuracy: 0.8545454545454545            
Batch: #6    | Top-1 Accuracy: 0.8613636363636363            
Batch: #7    | Top-1 Accuracy: 0.8590909090909091            
Batch: #8    | Top-1 Accuracy: 0.8517045454545454            
Batch: #9    | Top-1 Accuracy: 0.8505050505050504            
Batch: #10   | Top-1 Accuracy: 0.8454545454545455            
Batch: #11   | Top-1 Accuracy: 0.8433884297520661            
Batch: #12   | Top-1 Accuracy: 0.8424242424242424            
Batch: #13   | Top-1 Accuracy: 0.843006993006993             
Batch: #14   | Top-1 Accuracy: 0.8451298701298702            
Batch: #15   | Top-1 Accuracy: 0.8457575757575758            
Batch: #16   | Top-1 Accuracy: 0.8457386363636363            
Batch: #17   | Top-1 Accuracy: 0.8467914438502673            

Classifier Training Epoch #61
------------------------------------------
Epoch: #61   | Batch: #1    | Batch Loss: 0.5853894352912903            | Epoch Loss: 0.5853894352912903            
Epoch: #61   | Batch: #2    | Batch Loss: 0.4811025559902191            | Epoch Loss: 0.5332459956407547            
Epoch: #61   | Batch: #3    | Batch Loss: 0.4652487337589264            | Epoch Loss: 0.5105802416801453            
Epoch: #61   | Batch: #4    | Batch Loss: 0.4267844259738922            | Epoch Loss: 0.489631287753582             
Epoch: #61   | Batch: #5    | Batch Loss: 0.5430912971496582            | Epoch Loss: 0.5003232896327973            
Epoch: #61   | Batch: #6    | Batch Loss: 0.49268630146980286           | Epoch Loss: 0.49905045827229816           
Epoch: #61   | Batch: #7    | Batch Loss: 0.5787320733070374            | Epoch Loss: 0.5104335461344037            
Epoch: #61   | Batch: #8    | Batch Loss: 0.4295712411403656            | Epoch Loss: 0.500325758010149             
Epoch: #61   | Batch: #9    | Batch Loss: 0.4587874114513397            | Epoch Loss: 0.4957103861702813            
Epoch: #61   | Batch: #10   | Batch Loss: 0.49665722250938416           | Epoch Loss: 0.4958050698041916            
Epoch: #61   | Batch: #11   | Batch Loss: 0.42174601554870605           | Epoch Loss: 0.48907242850823834           
Epoch: #61   | Batch: #12   | Batch Loss: 0.46202659606933594           | Epoch Loss: 0.4868186091383298            
Epoch: #61   | Batch: #13   | Batch Loss: 0.429198294878006             | Epoch Loss: 0.48238627727215105           
Epoch: #61   | Batch: #14   | Batch Loss: 0.5108818411827087            | Epoch Loss: 0.4844216746943338            
Epoch: #61   | Batch: #15   | Batch Loss: 0.4976920485496521            | Epoch Loss: 0.4853063662846883            
Epoch: #61   | Batch: #16   | Batch Loss: 0.4900863766670227            | Epoch Loss: 0.4856051169335842            
Epoch: #61   | Batch: #17   | Batch Loss: 0.4575045704841614            | Epoch Loss: 0.4839521436130299            
Epoch: #61   | Batch: #18   | Batch Loss: 0.4830012023448944            | Epoch Loss: 0.483899313542578             
Epoch: #61   | Batch: #19   | Batch Loss: 0.42500177025794983           | Epoch Loss: 0.480799442843387             
Epoch: #61   | Batch: #20   | Batch Loss: 0.4694981873035431            | Epoch Loss: 0.4802343800663948            
Epoch: #61   | Batch: #21   | Batch Loss: 0.46831294894218445           | Epoch Loss: 0.47966669287000385           
Epoch: #61   | Batch: #22   | Batch Loss: 0.5346531867980957            | Epoch Loss: 0.48216607895764435           
Epoch: #61   | Batch: #23   | Batch Loss: 0.5203208923339844            | Epoch Loss: 0.4838249838870505            
Epoch: #61   | Batch: #24   | Batch Loss: 0.3966638743877411            | Epoch Loss: 0.4801932709912459            
Epoch: #61   | Batch: #25   | Batch Loss: 0.4654425084590912            | Epoch Loss: 0.4796032404899597            
Epoch: #61   | Batch: #26   | Batch Loss: 0.5098280906677246            | Epoch Loss: 0.48076573472756606           
Epoch: #61   | Batch: #27   | Batch Loss: 0.46674269437789917           | Epoch Loss: 0.48024636286276357           
Epoch: #61   | Batch: #28   | Batch Loss: 0.5604854822158813            | Epoch Loss: 0.4831120456968035            
Epoch: #61   | Batch: #29   | Batch Loss: 0.5594901442527771            | Epoch Loss: 0.4857457732332164            
Epoch: #61   | Batch: #30   | Batch Loss: 0.44750991463661194           | Epoch Loss: 0.4844712446133296            
Epoch: #61   | Batch: #31   | Batch Loss: 0.5269809365272522            | Epoch Loss: 0.4858425249976496            
Epoch: #61   | Batch: #32   | Batch Loss: 0.4965215027332306            | Epoch Loss: 0.48617624305188656           
Epoch: #61   | Batch: #33   | Batch Loss: 0.5113129615783691            | Epoch Loss: 0.4869379617951133            
Epoch: #61   | Batch: #34   | Batch Loss: 0.5134984254837036            | Epoch Loss: 0.48771915190360127           
Epoch: #61   | Batch: #35   | Batch Loss: 0.4259735345840454            | Epoch Loss: 0.4859549914087568            
Epoch: #61   | Batch: #36   | Batch Loss: 0.3951159715652466            | Epoch Loss: 0.48343168530199265           
Epoch: #61   | Batch: #37   | Batch Loss: 0.4930977523326874            | Epoch Loss: 0.48369293035687627           
Epoch: #61   | Batch: #38   | Batch Loss: 0.4130757451057434            | Epoch Loss: 0.4818345833765833            
Epoch: #61   | Batch: #39   | Batch Loss: 0.5122857689857483            | Epoch Loss: 0.48261538300758755           
Epoch: #61   | Batch: #40   | Batch Loss: 0.36443015933036804           | Epoch Loss: 0.47966075241565703           
Epoch: #61   | Batch: #41   | Batch Loss: 0.6644951701164246            | Epoch Loss: 0.4841689089449441            
Epoch: #61   | Batch: #42   | Batch Loss: 0.4966546893119812            | Epoch Loss: 0.4844661894298735            
Epoch: #61   | Batch: #43   | Batch Loss: 0.44620922207832336           | Epoch Loss: 0.4835764925147212            

Classifier Validation Epoch #61
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8363636363636363            
Batch: #2    | Top-1 Accuracy: 0.8431818181818181            
Batch: #3    | Top-1 Accuracy: 0.8393939393939394            
Batch: #4    | Top-1 Accuracy: 0.8386363636363636            
Batch: #5    | Top-1 Accuracy: 0.8363636363636363            
Batch: #6    | Top-1 Accuracy: 0.8416666666666667            
Batch: #7    | Top-1 Accuracy: 0.8435064935064934            
Batch: #8    | Top-1 Accuracy: 0.8409090909090908            
Batch: #9    | Top-1 Accuracy: 0.8398989898989898            
Batch: #10   | Top-1 Accuracy: 0.848181818181818             
Batch: #11   | Top-1 Accuracy: 0.8454545454545453            
Batch: #12   | Top-1 Accuracy: 0.8496212121212121            
Batch: #13   | Top-1 Accuracy: 0.8465034965034964            
Batch: #14   | Top-1 Accuracy: 0.8467532467532467            
Batch: #15   | Top-1 Accuracy: 0.8466666666666666            
Batch: #16   | Top-1 Accuracy: 0.8463068181818181            
Batch: #17   | Top-1 Accuracy: 0.8465240641711229            

Classifier Training Epoch #62
------------------------------------------
Epoch: #62   | Batch: #1    | Batch Loss: 0.4909207820892334            | Epoch Loss: 0.4909207820892334            
Epoch: #62   | Batch: #2    | Batch Loss: 0.438481867313385             | Epoch Loss: 0.4647013247013092            
Epoch: #62   | Batch: #3    | Batch Loss: 0.4619176983833313            | Epoch Loss: 0.4637734492619832            
Epoch: #62   | Batch: #4    | Batch Loss: 0.538591206073761             | Epoch Loss: 0.4824778884649277            
Epoch: #62   | Batch: #5    | Batch Loss: 0.547927975654602             | Epoch Loss: 0.49556790590286254           
Epoch: #62   | Batch: #6    | Batch Loss: 0.5099397301673889            | Epoch Loss: 0.49796320994695026           
Epoch: #62   | Batch: #7    | Batch Loss: 0.46660375595092773           | Epoch Loss: 0.4934832879475185            
Epoch: #62   | Batch: #8    | Batch Loss: 0.5128119587898254            | Epoch Loss: 0.49589937180280685           
Epoch: #62   | Batch: #9    | Batch Loss: 0.387588232755661             | Epoch Loss: 0.48386480079756844           
Epoch: #62   | Batch: #10   | Batch Loss: 0.5215316414833069            | Epoch Loss: 0.4876314848661423            
Epoch: #62   | Batch: #11   | Batch Loss: 0.4953632056713104            | Epoch Loss: 0.488334368575703             
Epoch: #62   | Batch: #12   | Batch Loss: 0.4482283294200897            | Epoch Loss: 0.4849921986460686            
Epoch: #62   | Batch: #13   | Batch Loss: 0.43805187940597534           | Epoch Loss: 0.4813814048583691            
Epoch: #62   | Batch: #14   | Batch Loss: 0.5691372752189636            | Epoch Loss: 0.48764968131269726           
Epoch: #62   | Batch: #15   | Batch Loss: 0.5709004402160645            | Epoch Loss: 0.4931997319062551            
Epoch: #62   | Batch: #16   | Batch Loss: 0.6018118858337402            | Epoch Loss: 0.4999879915267229            
Epoch: #62   | Batch: #17   | Batch Loss: 0.41199803352355957           | Epoch Loss: 0.49481211164418387           
Epoch: #62   | Batch: #18   | Batch Loss: 0.45147326588630676           | Epoch Loss: 0.49240439799096847           
Epoch: #62   | Batch: #19   | Batch Loss: 0.4905381202697754            | Epoch Loss: 0.4923061728477478            
Epoch: #62   | Batch: #20   | Batch Loss: 0.4765203595161438            | Epoch Loss: 0.4915168821811676            
Epoch: #62   | Batch: #21   | Batch Loss: 0.40354540944099426           | Epoch Loss: 0.48732776443163556           
Epoch: #62   | Batch: #22   | Batch Loss: 0.5379031896591187            | Epoch Loss: 0.4896266473965211            
Epoch: #62   | Batch: #23   | Batch Loss: 0.46347060799598694           | Epoch Loss: 0.4884894282921501            
Epoch: #62   | Batch: #24   | Batch Loss: 0.44393646717071533           | Epoch Loss: 0.4866330549120903            
Epoch: #62   | Batch: #25   | Batch Loss: 0.5047920942306519            | Epoch Loss: 0.4873594164848328            
Epoch: #62   | Batch: #26   | Batch Loss: 0.5023340582847595            | Epoch Loss: 0.4879353642463684            
Epoch: #62   | Batch: #27   | Batch Loss: 0.526533305644989             | Epoch Loss: 0.4893649176315025            
Epoch: #62   | Batch: #28   | Batch Loss: 0.3810109496116638            | Epoch Loss: 0.4854951330593654            
Epoch: #62   | Batch: #29   | Batch Loss: 0.4419912099838257            | Epoch Loss: 0.4839949977808985            
Epoch: #62   | Batch: #30   | Batch Loss: 0.44180721044540405           | Epoch Loss: 0.4825887382030487            
Epoch: #62   | Batch: #31   | Batch Loss: 0.4907214045524597            | Epoch Loss: 0.48285108227883616           
Epoch: #62   | Batch: #32   | Batch Loss: 0.6362944841384888            | Epoch Loss: 0.4876461885869503            
Epoch: #62   | Batch: #33   | Batch Loss: 0.5068886280059814            | Epoch Loss: 0.4882292928117694            
Epoch: #62   | Batch: #34   | Batch Loss: 0.5338393449783325            | Epoch Loss: 0.4895707649343154            
Epoch: #62   | Batch: #35   | Batch Loss: 0.4162023365497589            | Epoch Loss: 0.4874745241233281            
Epoch: #62   | Batch: #36   | Batch Loss: 0.3780454695224762            | Epoch Loss: 0.4844348281621933            
Epoch: #62   | Batch: #37   | Batch Loss: 0.5164281725883484            | Epoch Loss: 0.48529951314668396           
Epoch: #62   | Batch: #38   | Batch Loss: 0.4896523356437683            | Epoch Loss: 0.4854140611071336            
Epoch: #62   | Batch: #39   | Batch Loss: 0.3966312110424042            | Epoch Loss: 0.4831375777721405            
Epoch: #62   | Batch: #40   | Batch Loss: 0.4150792956352234            | Epoch Loss: 0.48143612071871755           
Epoch: #62   | Batch: #41   | Batch Loss: 0.5386282205581665            | Epoch Loss: 0.4828310499830944            
Epoch: #62   | Batch: #42   | Batch Loss: 0.6018415093421936            | Epoch Loss: 0.4856646323487872            
Epoch: #62   | Batch: #43   | Batch Loss: 0.4523245692253113            | Epoch Loss: 0.4848892820435901            

Classifier Validation Epoch #62
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8727272727272727            
Batch: #2    | Top-1 Accuracy: 0.8772727272727272            
Batch: #3    | Top-1 Accuracy: 0.8515151515151516            
Batch: #4    | Top-1 Accuracy: 0.8522727272727273            
Batch: #5    | Top-1 Accuracy: 0.8472727272727273            
Batch: #6    | Top-1 Accuracy: 0.8454545454545453            
Batch: #7    | Top-1 Accuracy: 0.8493506493506493            
Batch: #8    | Top-1 Accuracy: 0.8454545454545455            
Batch: #9    | Top-1 Accuracy: 0.8449494949494949            
Batch: #10   | Top-1 Accuracy: 0.8445454545454545            
Batch: #11   | Top-1 Accuracy: 0.8438016528925619            
Batch: #12   | Top-1 Accuracy: 0.8435606060606061            
Batch: #13   | Top-1 Accuracy: 0.8447552447552448            
Batch: #14   | Top-1 Accuracy: 0.8464285714285715            
Batch: #15   | Top-1 Accuracy: 0.848787878787879             
Batch: #16   | Top-1 Accuracy: 0.8454545454545455            
Batch: #17   | Top-1 Accuracy: 0.8425133689839572            

Classifier Training Epoch #63
------------------------------------------
Epoch: #63   | Batch: #1    | Batch Loss: 0.4734489917755127            | Epoch Loss: 0.4734489917755127            
Epoch: #63   | Batch: #2    | Batch Loss: 0.4543422758579254            | Epoch Loss: 0.46389563381671906           
Epoch: #63   | Batch: #3    | Batch Loss: 0.5266633629798889            | Epoch Loss: 0.4848182102044423            
Epoch: #63   | Batch: #4    | Batch Loss: 0.6011530160903931            | Epoch Loss: 0.51390191167593              
Epoch: #63   | Batch: #5    | Batch Loss: 0.4750811457633972            | Epoch Loss: 0.5061377584934235            
Epoch: #63   | Batch: #6    | Batch Loss: 0.46494603157043457           | Epoch Loss: 0.4992724706729253            
Epoch: #63   | Batch: #7    | Batch Loss: 0.4659140706062317            | Epoch Loss: 0.49450698494911194           
Epoch: #63   | Batch: #8    | Batch Loss: 0.5431999564170837            | Epoch Loss: 0.5005936063826084            
Epoch: #63   | Batch: #9    | Batch Loss: 0.5984179377555847            | Epoch Loss: 0.5114629765351614            
Epoch: #63   | Batch: #10   | Batch Loss: 0.4800446629524231            | Epoch Loss: 0.5083211451768875            
Epoch: #63   | Batch: #11   | Batch Loss: 0.46976327896118164           | Epoch Loss: 0.5048158846118234            
Epoch: #63   | Batch: #12   | Batch Loss: 0.45503121614456177           | Epoch Loss: 0.5006671622395515            
Epoch: #63   | Batch: #13   | Batch Loss: 0.5538123846054077            | Epoch Loss: 0.5047552562676944            
Epoch: #63   | Batch: #14   | Batch Loss: 0.41542768478393555           | Epoch Loss: 0.49837471544742584           
Epoch: #63   | Batch: #15   | Batch Loss: 0.43456798791885376           | Epoch Loss: 0.49412093361218773           
Epoch: #63   | Batch: #16   | Batch Loss: 0.436429500579834             | Epoch Loss: 0.4905152190476656            
Epoch: #63   | Batch: #17   | Batch Loss: 0.6483418345451355            | Epoch Loss: 0.4997991376063403            
Epoch: #63   | Batch: #18   | Batch Loss: 0.41045522689819336           | Epoch Loss: 0.49483558701144326           
Epoch: #63   | Batch: #19   | Batch Loss: 0.42678898572921753           | Epoch Loss: 0.49125418694395767           
Epoch: #63   | Batch: #20   | Batch Loss: 0.5041487812995911            | Epoch Loss: 0.49189891666173935           
Epoch: #63   | Batch: #21   | Batch Loss: 0.4480949640274048            | Epoch Loss: 0.48981301415534245           
Epoch: #63   | Batch: #22   | Batch Loss: 0.5163533687591553            | Epoch Loss: 0.4910193939100612            
Epoch: #63   | Batch: #23   | Batch Loss: 0.5125809907913208            | Epoch Loss: 0.49195685464402905           
Epoch: #63   | Batch: #24   | Batch Loss: 0.4398582875728607            | Epoch Loss: 0.4897860810160637            
Epoch: #63   | Batch: #25   | Batch Loss: 0.49870482087135315           | Epoch Loss: 0.49014283061027525           
Epoch: #63   | Batch: #26   | Batch Loss: 0.418757826089859             | Epoch Loss: 0.4873972535133362            
Epoch: #63   | Batch: #27   | Batch Loss: 0.47679004073143005           | Epoch Loss: 0.487004393780673             
Epoch: #63   | Batch: #28   | Batch Loss: 0.4896765947341919            | Epoch Loss: 0.48709982952901293           
Epoch: #63   | Batch: #29   | Batch Loss: 0.5459513664245605            | Epoch Loss: 0.4891291928702387            
Epoch: #63   | Batch: #30   | Batch Loss: 0.49867722392082214           | Epoch Loss: 0.48944746057192484           
Epoch: #63   | Batch: #31   | Batch Loss: 0.524935245513916             | Epoch Loss: 0.4905922278281181            
Epoch: #63   | Batch: #32   | Batch Loss: 0.43751633167266846           | Epoch Loss: 0.4889336060732603            
Epoch: #63   | Batch: #33   | Batch Loss: 0.5558705925941467            | Epoch Loss: 0.4909619996041963            
Epoch: #63   | Batch: #34   | Batch Loss: 0.5404898524284363            | Epoch Loss: 0.49241870115785036           
Epoch: #63   | Batch: #35   | Batch Loss: 0.46277371048927307           | Epoch Loss: 0.49157170142446244           
Epoch: #63   | Batch: #36   | Batch Loss: 0.3948320150375366            | Epoch Loss: 0.4888844879137145            
Epoch: #63   | Batch: #37   | Batch Loss: 0.4635387063026428            | Epoch Loss: 0.48819946678909093           
Epoch: #63   | Batch: #38   | Batch Loss: 0.5029385089874268            | Epoch Loss: 0.4885873363206261            
Epoch: #63   | Batch: #39   | Batch Loss: 0.4744112193584442            | Epoch Loss: 0.4882238461421086            
Epoch: #63   | Batch: #40   | Batch Loss: 0.5443307757377625            | Epoch Loss: 0.48962651938199997           
Epoch: #63   | Batch: #41   | Batch Loss: 0.46402138471603394           | Epoch Loss: 0.4890020039023423            
Epoch: #63   | Batch: #42   | Batch Loss: 0.5341933369636536            | Epoch Loss: 0.4900779880228497            
Epoch: #63   | Batch: #43   | Batch Loss: 0.40535417199134827           | Epoch Loss: 0.4881076667197915            

Classifier Validation Epoch #63
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8409090909090909            
Batch: #2    | Top-1 Accuracy: 0.8500000000000001            
Batch: #3    | Top-1 Accuracy: 0.846969696969697             
Batch: #4    | Top-1 Accuracy: 0.8431818181818183            
Batch: #5    | Top-1 Accuracy: 0.8436363636363637            
Batch: #6    | Top-1 Accuracy: 0.8409090909090909            
Batch: #7    | Top-1 Accuracy: 0.8370129870129871            
Batch: #8    | Top-1 Accuracy: 0.8369318181818182            
Batch: #9    | Top-1 Accuracy: 0.8373737373737373            
Batch: #10   | Top-1 Accuracy: 0.8363636363636363            
Batch: #11   | Top-1 Accuracy: 0.8359504132231405            
Batch: #12   | Top-1 Accuracy: 0.8359848484848484            
Batch: #13   | Top-1 Accuracy: 0.8377622377622378            
Batch: #14   | Top-1 Accuracy: 0.836038961038961             
Batch: #15   | Top-1 Accuracy: 0.8357575757575758            
Batch: #16   | Top-1 Accuracy: 0.8363636363636364            
Batch: #17   | Top-1 Accuracy: 0.8398395721925134            

Classifier Training Epoch #64
------------------------------------------
Epoch: #64   | Batch: #1    | Batch Loss: 0.4684479832649231            | Epoch Loss: 0.4684479832649231            
Epoch: #64   | Batch: #2    | Batch Loss: 0.665173351764679             | Epoch Loss: 0.566810667514801             
Epoch: #64   | Batch: #3    | Batch Loss: 0.4515957236289978            | Epoch Loss: 0.5284056862195333            
Epoch: #64   | Batch: #4    | Batch Loss: 0.35807424783706665           | Epoch Loss: 0.4858228266239166            
Epoch: #64   | Batch: #5    | Batch Loss: 0.43274927139282227           | Epoch Loss: 0.47520811557769777           
Epoch: #64   | Batch: #6    | Batch Loss: 0.4045954942703247            | Epoch Loss: 0.46343934535980225           
Epoch: #64   | Batch: #7    | Batch Loss: 0.45050978660583496           | Epoch Loss: 0.4615922655378069            
Epoch: #64   | Batch: #8    | Batch Loss: 0.4811379909515381            | Epoch Loss: 0.4640354812145233            
Epoch: #64   | Batch: #9    | Batch Loss: 0.3989483714103699            | Epoch Loss: 0.4568035801251729            
Epoch: #64   | Batch: #10   | Batch Loss: 0.473761647939682             | Epoch Loss: 0.45849938690662384           
Epoch: #64   | Batch: #11   | Batch Loss: 0.4947586953639984            | Epoch Loss: 0.4617956876754761            
Epoch: #64   | Batch: #12   | Batch Loss: 0.4972698986530304            | Epoch Loss: 0.4647518719236056            
Epoch: #64   | Batch: #13   | Batch Loss: 0.3246682584285736            | Epoch Loss: 0.4539762093470647            
Epoch: #64   | Batch: #14   | Batch Loss: 0.5722290873527527            | Epoch Loss: 0.4624228434903281            
Epoch: #64   | Batch: #15   | Batch Loss: 0.5371940732002258            | Epoch Loss: 0.46740759213765465           
Epoch: #64   | Batch: #16   | Batch Loss: 0.5435512661933899            | Epoch Loss: 0.4721665717661381            
Epoch: #64   | Batch: #17   | Batch Loss: 0.41768181324005127           | Epoch Loss: 0.46896158597048593           
Epoch: #64   | Batch: #18   | Batch Loss: 0.5266779661178589            | Epoch Loss: 0.47216805153422886           
Epoch: #64   | Batch: #19   | Batch Loss: 0.5343752503395081            | Epoch Loss: 0.4754421146292436            
Epoch: #64   | Batch: #20   | Batch Loss: 0.5043924450874329            | Epoch Loss: 0.476889631152153             
Epoch: #64   | Batch: #21   | Batch Loss: 0.477415531873703             | Epoch Loss: 0.4769146740436554            
Epoch: #64   | Batch: #22   | Batch Loss: 0.6056894659996033            | Epoch Loss: 0.48276807367801666           
Epoch: #64   | Batch: #23   | Batch Loss: 0.5903016924858093            | Epoch Loss: 0.4874434484087903            
Epoch: #64   | Batch: #24   | Batch Loss: 0.5326245427131653            | Epoch Loss: 0.48932599400480586           
Epoch: #64   | Batch: #25   | Batch Loss: 0.48139768838882446           | Epoch Loss: 0.4890088617801666            
Epoch: #64   | Batch: #26   | Batch Loss: 0.45066267251968384           | Epoch Loss: 0.4875340083470711            
Epoch: #64   | Batch: #27   | Batch Loss: 0.5075677633285522            | Epoch Loss: 0.4882759992723112            
Epoch: #64   | Batch: #28   | Batch Loss: 0.568020224571228             | Epoch Loss: 0.49112400731870104           
Epoch: #64   | Batch: #29   | Batch Loss: 0.5198192000389099            | Epoch Loss: 0.4921134967228462            
Epoch: #64   | Batch: #30   | Batch Loss: 0.33796605467796326           | Epoch Loss: 0.48697524865468345           
Epoch: #64   | Batch: #31   | Batch Loss: 0.5128383040428162            | Epoch Loss: 0.48780954076397803           
Epoch: #64   | Batch: #32   | Batch Loss: 0.5513718128204346            | Epoch Loss: 0.4897958617657423            
Epoch: #64   | Batch: #33   | Batch Loss: 0.4826321303844452            | Epoch Loss: 0.48957877899661206           
Epoch: #64   | Batch: #34   | Batch Loss: 0.4089377522468567            | Epoch Loss: 0.48720698409220753           
Epoch: #64   | Batch: #35   | Batch Loss: 0.43942156434059143           | Epoch Loss: 0.4858416863850185            
Epoch: #64   | Batch: #36   | Batch Loss: 0.4925409257411957            | Epoch Loss: 0.4860277763671345            
Epoch: #64   | Batch: #37   | Batch Loss: 0.5412941575050354            | Epoch Loss: 0.48752146234383453           
Epoch: #64   | Batch: #38   | Batch Loss: 0.43650779128074646           | Epoch Loss: 0.48617899731585856           
Epoch: #64   | Batch: #39   | Batch Loss: 0.47820138931274414           | Epoch Loss: 0.4859744432644966            
Epoch: #64   | Batch: #40   | Batch Loss: 0.5063302516937256            | Epoch Loss: 0.48648333847522734           
Epoch: #64   | Batch: #41   | Batch Loss: 0.4353441298007965            | Epoch Loss: 0.4852360407026803            
Epoch: #64   | Batch: #42   | Batch Loss: 0.5760989189147949            | Epoch Loss: 0.48739944256487344           
Epoch: #64   | Batch: #43   | Batch Loss: 0.34471964836120605           | Epoch Loss: 0.484081307815951             

Classifier Validation Epoch #64
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8363636363636363            
Batch: #2    | Top-1 Accuracy: 0.8431818181818181            
Batch: #3    | Top-1 Accuracy: 0.8454545454545453            
Batch: #4    | Top-1 Accuracy: 0.8511363636363636            
Batch: #5    | Top-1 Accuracy: 0.8472727272727273            
Batch: #6    | Top-1 Accuracy: 0.8462121212121212            
Batch: #7    | Top-1 Accuracy: 0.8474025974025974            
Batch: #8    | Top-1 Accuracy: 0.8488636363636364            
Batch: #9    | Top-1 Accuracy: 0.8444444444444444            
Batch: #10   | Top-1 Accuracy: 0.8477272727272727            
Batch: #11   | Top-1 Accuracy: 0.8483471074380166            
Batch: #12   | Top-1 Accuracy: 0.8462121212121212            
Batch: #13   | Top-1 Accuracy: 0.8482517482517482            
Batch: #14   | Top-1 Accuracy: 0.8487012987012987            
Batch: #15   | Top-1 Accuracy: 0.8509090909090911            
Batch: #16   | Top-1 Accuracy: 0.8514204545454546            
Batch: #17   | Top-1 Accuracy: 0.8513368983957219            

Classifier Training Epoch #65
------------------------------------------
Epoch: #65   | Batch: #1    | Batch Loss: 0.47838085889816284           | Epoch Loss: 0.47838085889816284           
Epoch: #65   | Batch: #2    | Batch Loss: 0.5096522569656372            | Epoch Loss: 0.4940165579319               
Epoch: #65   | Batch: #3    | Batch Loss: 0.44120103120803833           | Epoch Loss: 0.4764113823572795            
Epoch: #65   | Batch: #4    | Batch Loss: 0.44326692819595337           | Epoch Loss: 0.46812526881694794           
Epoch: #65   | Batch: #5    | Batch Loss: 0.43040552735328674           | Epoch Loss: 0.4605813205242157            
Epoch: #65   | Batch: #6    | Batch Loss: 0.33243992924690247           | Epoch Loss: 0.4392244219779968            
Epoch: #65   | Batch: #7    | Batch Loss: 0.4646078944206238            | Epoch Loss: 0.44285063232694355           
Epoch: #65   | Batch: #8    | Batch Loss: 0.4936363697052002            | Epoch Loss: 0.4491988494992256            
Epoch: #65   | Batch: #9    | Batch Loss: 0.3417603373527527            | Epoch Loss: 0.4372612370385064            
Epoch: #65   | Batch: #10   | Batch Loss: 0.521246075630188             | Epoch Loss: 0.4456597208976746            
Epoch: #65   | Batch: #11   | Batch Loss: 0.4398963153362274            | Epoch Loss: 0.445135774937543             
Epoch: #65   | Batch: #12   | Batch Loss: 0.45295804738998413           | Epoch Loss: 0.44578763097524643           
Epoch: #65   | Batch: #13   | Batch Loss: 0.4174181818962097            | Epoch Loss: 0.44360536566147435           
Epoch: #65   | Batch: #14   | Batch Loss: 0.6202604174613953            | Epoch Loss: 0.456223583647183             
Epoch: #65   | Batch: #15   | Batch Loss: 0.49023810029029846           | Epoch Loss: 0.4584912180900574            
Epoch: #65   | Batch: #16   | Batch Loss: 0.5229968428611755            | Epoch Loss: 0.46252281963825226           
Epoch: #65   | Batch: #17   | Batch Loss: 0.48188766837120056           | Epoch Loss: 0.46366192838724923           
Epoch: #65   | Batch: #18   | Batch Loss: 0.40271249413490295           | Epoch Loss: 0.4602758487065633            
Epoch: #65   | Batch: #19   | Batch Loss: 0.46381455659866333           | Epoch Loss: 0.4604620964903581            
Epoch: #65   | Batch: #20   | Batch Loss: 0.621010959148407             | Epoch Loss: 0.4684895396232605            
Epoch: #65   | Batch: #21   | Batch Loss: 0.3882725238800049            | Epoch Loss: 0.46466968173072454           
Epoch: #65   | Batch: #22   | Batch Loss: 0.6493364572525024            | Epoch Loss: 0.47306362607262353           
Epoch: #65   | Batch: #23   | Batch Loss: 0.47213441133499146           | Epoch Loss: 0.4730232254318569            
Epoch: #65   | Batch: #24   | Batch Loss: 0.5138251781463623            | Epoch Loss: 0.4747233067949613            
Epoch: #65   | Batch: #25   | Batch Loss: 0.4898495078086853            | Epoch Loss: 0.4753283548355103            
Epoch: #65   | Batch: #26   | Batch Loss: 0.41061830520629883           | Epoch Loss: 0.47283950677284825           
Epoch: #65   | Batch: #27   | Batch Loss: 0.5778551697731018            | Epoch Loss: 0.47672897577285767           
Epoch: #65   | Batch: #28   | Batch Loss: 0.4155116081237793            | Epoch Loss: 0.474542641213962             
Epoch: #65   | Batch: #29   | Batch Loss: 0.3591266870498657            | Epoch Loss: 0.4705627807255449            
Epoch: #65   | Batch: #30   | Batch Loss: 0.4265095889568329            | Epoch Loss: 0.46909434099992114           
Epoch: #65   | Batch: #31   | Batch Loss: 0.47948214411735535           | Epoch Loss: 0.4694294314230642            
Epoch: #65   | Batch: #32   | Batch Loss: 0.5273483395576477            | Epoch Loss: 0.47123939730226994           
Epoch: #65   | Batch: #33   | Batch Loss: 0.6218324899673462            | Epoch Loss: 0.4758028243527268            
Epoch: #65   | Batch: #34   | Batch Loss: 0.5464574098587036            | Epoch Loss: 0.47788090039702025           
Epoch: #65   | Batch: #35   | Batch Loss: 0.44879621267318726           | Epoch Loss: 0.47704990931919644           
Epoch: #65   | Batch: #36   | Batch Loss: 0.585237979888916             | Epoch Loss: 0.48005513350168866           
Epoch: #65   | Batch: #37   | Batch Loss: 0.4387652575969696            | Epoch Loss: 0.4789391909096692            
Epoch: #65   | Batch: #38   | Batch Loss: 0.5619102716445923            | Epoch Loss: 0.4811226404026935            
Epoch: #65   | Batch: #39   | Batch Loss: 0.42021360993385315           | Epoch Loss: 0.47956087039067197           
Epoch: #65   | Batch: #40   | Batch Loss: 0.5632600784301758            | Epoch Loss: 0.48165335059165953           
Epoch: #65   | Batch: #41   | Batch Loss: 0.4542679786682129            | Epoch Loss: 0.4809854146910877            
Epoch: #65   | Batch: #42   | Batch Loss: 0.4853416383266449            | Epoch Loss: 0.4810891343014581            
Epoch: #65   | Batch: #43   | Batch Loss: 0.5458552837371826            | Epoch Loss: 0.4825953238232191            

Classifier Validation Epoch #65
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.85                          
Batch: #2    | Top-1 Accuracy: 0.8477272727272727            
Batch: #3    | Top-1 Accuracy: 0.8363636363636363            
Batch: #4    | Top-1 Accuracy: 0.8386363636363636            
Batch: #5    | Top-1 Accuracy: 0.8390909090909091            
Batch: #6    | Top-1 Accuracy: 0.8446969696969697            
Batch: #7    | Top-1 Accuracy: 0.8493506493506493            
Batch: #8    | Top-1 Accuracy: 0.852840909090909             
Batch: #9    | Top-1 Accuracy: 0.8535353535353535            
Batch: #10   | Top-1 Accuracy: 0.8504545454545454            
Batch: #11   | Top-1 Accuracy: 0.8495867768595041            
Batch: #12   | Top-1 Accuracy: 0.8488636363636365            
Batch: #13   | Top-1 Accuracy: 0.8496503496503498            
Batch: #14   | Top-1 Accuracy: 0.8490259740259741            
Batch: #15   | Top-1 Accuracy: 0.8493939393939395            
Batch: #16   | Top-1 Accuracy: 0.8482954545454546            
Batch: #17   | Top-1 Accuracy: 0.8473262032085562            

Classifier Training Epoch #66
------------------------------------------
Epoch: #66   | Batch: #1    | Batch Loss: 0.5163096785545349            | Epoch Loss: 0.5163096785545349            
Epoch: #66   | Batch: #2    | Batch Loss: 0.45386868715286255           | Epoch Loss: 0.48508918285369873           
Epoch: #66   | Batch: #3    | Batch Loss: 0.5351043343544006            | Epoch Loss: 0.5017609000205994            
Epoch: #66   | Batch: #4    | Batch Loss: 0.5471978187561035            | Epoch Loss: 0.5131201297044754            
Epoch: #66   | Batch: #5    | Batch Loss: 0.4962330162525177            | Epoch Loss: 0.5097427070140839            
Epoch: #66   | Batch: #6    | Batch Loss: 0.5412361025810242            | Epoch Loss: 0.5149916062752405            
Epoch: #66   | Batch: #7    | Batch Loss: 0.5590631365776062            | Epoch Loss: 0.5212875391755786            
Epoch: #66   | Batch: #8    | Batch Loss: 0.5731860399246216            | Epoch Loss: 0.5277748517692089            
Epoch: #66   | Batch: #9    | Batch Loss: 0.4847123622894287            | Epoch Loss: 0.5229901307159               
Epoch: #66   | Batch: #10   | Batch Loss: 0.42350032925605774           | Epoch Loss: 0.5130411505699157            
Epoch: #66   | Batch: #11   | Batch Loss: 0.46036866307258606           | Epoch Loss: 0.508252742615613             
Epoch: #66   | Batch: #12   | Batch Loss: 0.4754454791545868            | Epoch Loss: 0.5055188039938608            
Epoch: #66   | Batch: #13   | Batch Loss: 0.5462384819984436            | Epoch Loss: 0.5086510869172903            
Epoch: #66   | Batch: #14   | Batch Loss: 0.4526732563972473            | Epoch Loss: 0.5046526704515729            
Epoch: #66   | Batch: #15   | Batch Loss: 0.4937162697315216            | Epoch Loss: 0.5039235770702362            
Epoch: #66   | Batch: #16   | Batch Loss: 0.43534985184669495           | Epoch Loss: 0.4996377192437649            
Epoch: #66   | Batch: #17   | Batch Loss: 0.45237812399864197           | Epoch Loss: 0.4968577430528753            
Epoch: #66   | Batch: #18   | Batch Loss: 0.5402041673660278            | Epoch Loss: 0.4992658777369393            
Epoch: #66   | Batch: #19   | Batch Loss: 0.4948040843009949            | Epoch Loss: 0.4990310465034686            
Epoch: #66   | Batch: #20   | Batch Loss: 0.4117773175239563            | Epoch Loss: 0.49466836005449294           
Epoch: #66   | Batch: #21   | Batch Loss: 0.47717300057411194           | Epoch Loss: 0.49383524769828435           
Epoch: #66   | Batch: #22   | Batch Loss: 0.4774315357208252            | Epoch Loss: 0.49308962442658166           
Epoch: #66   | Batch: #23   | Batch Loss: 0.4376300573348999            | Epoch Loss: 0.4906783389008563            
Epoch: #66   | Batch: #24   | Batch Loss: 0.4941970705986023            | Epoch Loss: 0.49082495272159576           
Epoch: #66   | Batch: #25   | Batch Loss: 0.6090071201324463            | Epoch Loss: 0.4955522394180298            
Epoch: #66   | Batch: #26   | Batch Loss: 0.46805423498153687           | Epoch Loss: 0.49449462386278004           
Epoch: #66   | Batch: #27   | Batch Loss: 0.6593204736709595            | Epoch Loss: 0.5005992849667867            
Epoch: #66   | Batch: #28   | Batch Loss: 0.6749187707901001            | Epoch Loss: 0.5068249808890479            
Epoch: #66   | Batch: #29   | Batch Loss: 0.46235302090644836           | Epoch Loss: 0.5052914650275789            
Epoch: #66   | Batch: #30   | Batch Loss: 0.5300502181053162            | Epoch Loss: 0.5061167567968369            
Epoch: #66   | Batch: #31   | Batch Loss: 0.5772613883018494            | Epoch Loss: 0.5084117449099018            
Epoch: #66   | Batch: #32   | Batch Loss: 0.46530869603157043           | Epoch Loss: 0.5070647746324539            
Epoch: #66   | Batch: #33   | Batch Loss: 0.4590210020542145            | Epoch Loss: 0.5056089027361437            
Epoch: #66   | Batch: #34   | Batch Loss: 0.6994579434394836            | Epoch Loss: 0.5113103451097712            
Epoch: #66   | Batch: #35   | Batch Loss: 0.47842130064964294           | Epoch Loss: 0.5103706581251962            
Epoch: #66   | Batch: #36   | Batch Loss: 0.4330524206161499            | Epoch Loss: 0.5082229293055005            
Epoch: #66   | Batch: #37   | Batch Loss: 0.5314555168151855            | Epoch Loss: 0.5088508370760325            
Epoch: #66   | Batch: #38   | Batch Loss: 0.4547421932220459            | Epoch Loss: 0.5074269253956644            
Epoch: #66   | Batch: #39   | Batch Loss: 0.5143516063690186            | Epoch Loss: 0.5076044813180581            
Epoch: #66   | Batch: #40   | Batch Loss: 0.42587584257125854           | Epoch Loss: 0.5055612653493882            
Epoch: #66   | Batch: #41   | Batch Loss: 0.4250386953353882            | Epoch Loss: 0.5035973002270955            
Epoch: #66   | Batch: #42   | Batch Loss: 0.40943971276283264           | Epoch Loss: 0.5013554529065177            
Epoch: #66   | Batch: #43   | Batch Loss: 0.4313001334667206            | Epoch Loss: 0.49972625943117366           

Classifier Validation Epoch #66
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8454545454545455            
Batch: #2    | Top-1 Accuracy: 0.8545454545454545            
Batch: #3    | Top-1 Accuracy: 0.8393939393939394            
Batch: #4    | Top-1 Accuracy: 0.8352272727272727            
Batch: #5    | Top-1 Accuracy: 0.8400000000000001            
Batch: #6    | Top-1 Accuracy: 0.8386363636363637            
Batch: #7    | Top-1 Accuracy: 0.8454545454545456            
Batch: #8    | Top-1 Accuracy: 0.8448863636363636            
Batch: #9    | Top-1 Accuracy: 0.8454545454545453            
Batch: #10   | Top-1 Accuracy: 0.8463636363636363            
Batch: #11   | Top-1 Accuracy: 0.8475206611570247            
Batch: #12   | Top-1 Accuracy: 0.8496212121212121            
Batch: #13   | Top-1 Accuracy: 0.8513986013986015            
Batch: #14   | Top-1 Accuracy: 0.8506493506493508            
Batch: #15   | Top-1 Accuracy: 0.8460606060606061            
Batch: #16   | Top-1 Accuracy: 0.8446022727272727            
Batch: #17   | Top-1 Accuracy: 0.8454545454545455            

Classifier Training Epoch #67
------------------------------------------
Epoch: #67   | Batch: #1    | Batch Loss: 0.44405093789100647           | Epoch Loss: 0.44405093789100647           
Epoch: #67   | Batch: #2    | Batch Loss: 0.4942074120044708            | Epoch Loss: 0.46912917494773865           
Epoch: #67   | Batch: #3    | Batch Loss: 0.4402385354042053            | Epoch Loss: 0.45949896176656085           
Epoch: #67   | Batch: #4    | Batch Loss: 0.43020525574684143           | Epoch Loss: 0.452175535261631             
Epoch: #67   | Batch: #5    | Batch Loss: 0.5205289721488953            | Epoch Loss: 0.4658462226390839            
Epoch: #67   | Batch: #6    | Batch Loss: 0.3994912803173065            | Epoch Loss: 0.4547870655854543            
Epoch: #67   | Batch: #7    | Batch Loss: 0.5283522009849548            | Epoch Loss: 0.4652963706425258            
Epoch: #67   | Batch: #8    | Batch Loss: 0.5488407015800476            | Epoch Loss: 0.47573941200971603           
Epoch: #67   | Batch: #9    | Batch Loss: 0.5450946688652039            | Epoch Loss: 0.48344555166032577           
Epoch: #67   | Batch: #10   | Batch Loss: 0.47258496284484863           | Epoch Loss: 0.4823594927787781            
Epoch: #67   | Batch: #11   | Batch Loss: 0.46516019105911255           | Epoch Loss: 0.4807959198951721            
Epoch: #67   | Batch: #12   | Batch Loss: 0.5813997983932495            | Epoch Loss: 0.4891795764366786            
Epoch: #67   | Batch: #13   | Batch Loss: 0.4643045663833618            | Epoch Loss: 0.48726611412488496           
Epoch: #67   | Batch: #14   | Batch Loss: 0.6269977688789368            | Epoch Loss: 0.4972469466073172            
Epoch: #67   | Batch: #15   | Batch Loss: 0.4770684540271759            | Epoch Loss: 0.4959017137686412            
Epoch: #67   | Batch: #16   | Batch Loss: 0.5683354735374451            | Epoch Loss: 0.5004288237541914            
Epoch: #67   | Batch: #17   | Batch Loss: 0.48306429386138916           | Epoch Loss: 0.4994073808193207            
Epoch: #67   | Batch: #18   | Batch Loss: 0.5437614321708679            | Epoch Loss: 0.5018714947832955            
Epoch: #67   | Batch: #19   | Batch Loss: 0.49732041358947754           | Epoch Loss: 0.5016319641941472            
Epoch: #67   | Batch: #20   | Batch Loss: 0.42559805512428284           | Epoch Loss: 0.497830268740654             
Epoch: #67   | Batch: #21   | Batch Loss: 0.5370785593986511            | Epoch Loss: 0.4996992349624634            
Epoch: #67   | Batch: #22   | Batch Loss: 0.5713024735450745            | Epoch Loss: 0.5029539276253093            
Epoch: #67   | Batch: #23   | Batch Loss: 0.5216363072395325            | Epoch Loss: 0.5037662049998408            
Epoch: #67   | Batch: #24   | Batch Loss: 0.525153398513794             | Epoch Loss: 0.5046573380629221            
Epoch: #67   | Batch: #25   | Batch Loss: 0.5792465806007385            | Epoch Loss: 0.5076409077644348            
Epoch: #67   | Batch: #26   | Batch Loss: 0.3544011116027832            | Epoch Loss: 0.5017470694505252            
Epoch: #67   | Batch: #27   | Batch Loss: 0.456694632768631             | Epoch Loss: 0.5000784606845291            
Epoch: #67   | Batch: #28   | Batch Loss: 0.5091963410377502            | Epoch Loss: 0.5004040992685727            
Epoch: #67   | Batch: #29   | Batch Loss: 0.5362909436225891            | Epoch Loss: 0.5016415766600905            
Epoch: #67   | Batch: #30   | Batch Loss: 0.4526399075984955            | Epoch Loss: 0.5000081876913707            
Epoch: #67   | Batch: #31   | Batch Loss: 0.5002152323722839            | Epoch Loss: 0.5000148665520453            
Epoch: #67   | Batch: #32   | Batch Loss: 0.5338423252105713            | Epoch Loss: 0.5010719746351242            
Epoch: #67   | Batch: #33   | Batch Loss: 0.4458722174167633            | Epoch Loss: 0.4993992547194163            
Epoch: #67   | Batch: #34   | Batch Loss: 0.5620590448379517            | Epoch Loss: 0.5012421897229027            
Epoch: #67   | Batch: #35   | Batch Loss: 0.5012502074241638            | Epoch Loss: 0.5012424188000816            
Epoch: #67   | Batch: #36   | Batch Loss: 0.5727488994598389            | Epoch Loss: 0.5032287099295192            
Epoch: #67   | Batch: #37   | Batch Loss: 0.6118382811546326            | Epoch Loss: 0.5061641037464142            
Epoch: #67   | Batch: #38   | Batch Loss: 0.3594236671924591            | Epoch Loss: 0.5023025133107838            
Epoch: #67   | Batch: #39   | Batch Loss: 0.43911513686180115           | Epoch Loss: 0.5006823241710663            
Epoch: #67   | Batch: #40   | Batch Loss: 0.47808194160461426           | Epoch Loss: 0.500117314606905             
Epoch: #67   | Batch: #41   | Batch Loss: 0.4913121163845062            | Epoch Loss: 0.49990255367465136           
Epoch: #67   | Batch: #42   | Batch Loss: 0.5257997512817383            | Epoch Loss: 0.5005191536176772            
Epoch: #67   | Batch: #43   | Batch Loss: 0.5308976769447327            | Epoch Loss: 0.5012256309043529            

Classifier Validation Epoch #67
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8863636363636364            
Batch: #2    | Top-1 Accuracy: 0.865909090909091             
Batch: #3    | Top-1 Accuracy: 0.8560606060606061            
Batch: #4    | Top-1 Accuracy: 0.8613636363636364            
Batch: #5    | Top-1 Accuracy: 0.8563636363636364            
Batch: #6    | Top-1 Accuracy: 0.8522727272727274            
Batch: #7    | Top-1 Accuracy: 0.8493506493506494            
Batch: #8    | Top-1 Accuracy: 0.8545454545454545            
Batch: #9    | Top-1 Accuracy: 0.8449494949494949            
Batch: #10   | Top-1 Accuracy: 0.8409090909090908            
Batch: #11   | Top-1 Accuracy: 0.8413223140495867            
Batch: #12   | Top-1 Accuracy: 0.8416666666666667            
Batch: #13   | Top-1 Accuracy: 0.8426573426573426            
Batch: #14   | Top-1 Accuracy: 0.844155844155844             
Batch: #15   | Top-1 Accuracy: 0.8442424242424241            
Batch: #16   | Top-1 Accuracy: 0.8477272727272727            
Batch: #17   | Top-1 Accuracy: 0.85                          

Classifier Training Epoch #68
------------------------------------------
Epoch: #68   | Batch: #1    | Batch Loss: 0.6518750190734863            | Epoch Loss: 0.6518750190734863            
Epoch: #68   | Batch: #2    | Batch Loss: 0.49722781777381897           | Epoch Loss: 0.5745514184236526            
Epoch: #68   | Batch: #3    | Batch Loss: 0.5266309976577759            | Epoch Loss: 0.5585779448350271            
Epoch: #68   | Batch: #4    | Batch Loss: 0.44888609647750854           | Epoch Loss: 0.5311549827456474            
Epoch: #68   | Batch: #5    | Batch Loss: 0.45244431495666504           | Epoch Loss: 0.5154128491878509            
Epoch: #68   | Batch: #6    | Batch Loss: 0.4686497747898102            | Epoch Loss: 0.5076190034548441            
Epoch: #68   | Batch: #7    | Batch Loss: 0.45867958664894104           | Epoch Loss: 0.500627658196858             
Epoch: #68   | Batch: #8    | Batch Loss: 0.440908819437027             | Epoch Loss: 0.4931628033518791            
Epoch: #68   | Batch: #9    | Batch Loss: 0.4326525330543518            | Epoch Loss: 0.4864394399854872            
Epoch: #68   | Batch: #10   | Batch Loss: 0.3461158871650696            | Epoch Loss: 0.47240708470344545           
Epoch: #68   | Batch: #11   | Batch Loss: 0.42527130246162415           | Epoch Loss: 0.4681220135905526            
Epoch: #68   | Batch: #12   | Batch Loss: 0.4803701937198639            | Epoch Loss: 0.4691426952679952            
Epoch: #68   | Batch: #13   | Batch Loss: 0.6412361264228821            | Epoch Loss: 0.4823806515106788            
Epoch: #68   | Batch: #14   | Batch Loss: 0.5064923167228699            | Epoch Loss: 0.4841029133115496            
Epoch: #68   | Batch: #15   | Batch Loss: 0.37931549549102783           | Epoch Loss: 0.47711708545684817           
Epoch: #68   | Batch: #16   | Batch Loss: 0.629947304725647             | Epoch Loss: 0.48666897416114807           
Epoch: #68   | Batch: #17   | Batch Loss: 0.4862237274646759            | Epoch Loss: 0.48664278317900267           
Epoch: #68   | Batch: #18   | Batch Loss: 0.49651095271110535           | Epoch Loss: 0.48719101481967503           
Epoch: #68   | Batch: #19   | Batch Loss: 0.5734528303146362            | Epoch Loss: 0.4917311103720414            
Epoch: #68   | Batch: #20   | Batch Loss: 0.45661523938179016           | Epoch Loss: 0.4899753168225288            
Epoch: #68   | Batch: #21   | Batch Loss: 0.4766179621219635            | Epoch Loss: 0.4893392523129781            
Epoch: #68   | Batch: #22   | Batch Loss: 0.6113768815994263            | Epoch Loss: 0.49488641728054394           
Epoch: #68   | Batch: #23   | Batch Loss: 0.39657023549079895           | Epoch Loss: 0.4906118006809898            
Epoch: #68   | Batch: #24   | Batch Loss: 0.3540276288986206            | Epoch Loss: 0.48492079352339107           
Epoch: #68   | Batch: #25   | Batch Loss: 0.39856162667274475           | Epoch Loss: 0.48146642684936525           
Epoch: #68   | Batch: #26   | Batch Loss: 0.43985506892204285           | Epoch Loss: 0.47986599000600666           
Epoch: #68   | Batch: #27   | Batch Loss: 0.5429899096488953            | Epoch Loss: 0.4822039129557433            
Epoch: #68   | Batch: #28   | Batch Loss: 0.42183637619018555           | Epoch Loss: 0.4800479294998305            
Epoch: #68   | Batch: #29   | Batch Loss: 0.4841209650039673            | Epoch Loss: 0.4801883789999732            
Epoch: #68   | Batch: #30   | Batch Loss: 0.4709436595439911            | Epoch Loss: 0.4798802216847738            
Epoch: #68   | Batch: #31   | Batch Loss: 0.4391135275363922            | Epoch Loss: 0.478565167034826             
Epoch: #68   | Batch: #32   | Batch Loss: 0.4102688729763031            | Epoch Loss: 0.47643090784549713           
Epoch: #68   | Batch: #33   | Batch Loss: 0.49205395579338074           | Epoch Loss: 0.47690433354088757           
Epoch: #68   | Batch: #34   | Batch Loss: 0.570304274559021             | Epoch Loss: 0.47965139062965617           
Epoch: #68   | Batch: #35   | Batch Loss: 0.5257732272148132            | Epoch Loss: 0.48096915738923207           
Epoch: #68   | Batch: #36   | Batch Loss: 0.5106787085533142            | Epoch Loss: 0.48179442269934547           
Epoch: #68   | Batch: #37   | Batch Loss: 0.4717661738395691            | Epoch Loss: 0.4815233889463785            
Epoch: #68   | Batch: #38   | Batch Loss: 0.49413132667541504           | Epoch Loss: 0.4818551767813532            
Epoch: #68   | Batch: #39   | Batch Loss: 0.45404449105262756           | Epoch Loss: 0.48114208227548844           
Epoch: #68   | Batch: #40   | Batch Loss: 0.4581630825996399            | Epoch Loss: 0.4805676072835922            
Epoch: #68   | Batch: #41   | Batch Loss: 0.36386168003082275           | Epoch Loss: 0.4777211212530369            
Epoch: #68   | Batch: #42   | Batch Loss: 0.6957928538322449            | Epoch Loss: 0.4829133053620656            
Epoch: #68   | Batch: #43   | Batch Loss: 0.4370892643928528            | Epoch Loss: 0.4818476299906886            

Classifier Validation Epoch #68
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8636363636363636            
Batch: #2    | Top-1 Accuracy: 0.8590909090909091            
Batch: #3    | Top-1 Accuracy: 0.8727272727272727            
Batch: #4    | Top-1 Accuracy: 0.8545454545454545            
Batch: #5    | Top-1 Accuracy: 0.8527272727272728            
Batch: #6    | Top-1 Accuracy: 0.8568181818181818            
Batch: #7    | Top-1 Accuracy: 0.8545454545454545            
Batch: #8    | Top-1 Accuracy: 0.8500000000000001            
Batch: #9    | Top-1 Accuracy: 0.8494949494949496            
Batch: #10   | Top-1 Accuracy: 0.8495454545454546            
Batch: #11   | Top-1 Accuracy: 0.8495867768595041            
Batch: #12   | Top-1 Accuracy: 0.846969696969697             
Batch: #13   | Top-1 Accuracy: 0.8479020979020979            
Batch: #14   | Top-1 Accuracy: 0.8493506493506493            
Batch: #15   | Top-1 Accuracy: 0.8478787878787879            
Batch: #16   | Top-1 Accuracy: 0.85                          
Batch: #17   | Top-1 Accuracy: 0.8483957219251337            

Classifier Training Epoch #69
------------------------------------------
Epoch: #69   | Batch: #1    | Batch Loss: 0.4466656744480133            | Epoch Loss: 0.4466656744480133            
Epoch: #69   | Batch: #2    | Batch Loss: 0.4452087879180908            | Epoch Loss: 0.44593723118305206           
Epoch: #69   | Batch: #3    | Batch Loss: 0.5302728414535522            | Epoch Loss: 0.4740491012732188            
Epoch: #69   | Batch: #4    | Batch Loss: 0.43034660816192627           | Epoch Loss: 0.46312347799539566           
Epoch: #69   | Batch: #5    | Batch Loss: 0.42623400688171387           | Epoch Loss: 0.4557455837726593            
Epoch: #69   | Batch: #6    | Batch Loss: 0.5329911708831787            | Epoch Loss: 0.4686198482910792            
Epoch: #69   | Batch: #7    | Batch Loss: 0.5043191909790039            | Epoch Loss: 0.47371975438935415           
Epoch: #69   | Batch: #8    | Batch Loss: 0.48529669642448425           | Epoch Loss: 0.4751668721437454            
Epoch: #69   | Batch: #9    | Batch Loss: 0.5291258096694946            | Epoch Loss: 0.48116230964660645           
Epoch: #69   | Batch: #10   | Batch Loss: 0.40726783871650696           | Epoch Loss: 0.4737728625535965            
Epoch: #69   | Batch: #11   | Batch Loss: 0.4436173141002655            | Epoch Loss: 0.4710314490578391            
Epoch: #69   | Batch: #12   | Batch Loss: 0.485217422246933             | Epoch Loss: 0.47221361349026364           
Epoch: #69   | Batch: #13   | Batch Loss: 0.49309927225112915           | Epoch Loss: 0.47382020262571484           
Epoch: #69   | Batch: #14   | Batch Loss: 0.5913726091384888            | Epoch Loss: 0.48221680309091297           
Epoch: #69   | Batch: #15   | Batch Loss: 0.5575878024101257            | Epoch Loss: 0.4872415363788605            
Epoch: #69   | Batch: #16   | Batch Loss: 0.649824321269989             | Epoch Loss: 0.497402960434556             
Epoch: #69   | Batch: #17   | Batch Loss: 0.46478989720344543           | Epoch Loss: 0.49548454495037303           
Epoch: #69   | Batch: #18   | Batch Loss: 0.5292139649391174            | Epoch Loss: 0.49735840161641437           
Epoch: #69   | Batch: #19   | Batch Loss: 0.5848322510719299            | Epoch Loss: 0.5019622884298626            
Epoch: #69   | Batch: #20   | Batch Loss: 0.43024951219558716           | Epoch Loss: 0.4983766496181488            
Epoch: #69   | Batch: #21   | Batch Loss: 0.4427032172679901            | Epoch Loss: 0.4957255337919508            
Epoch: #69   | Batch: #22   | Batch Loss: 0.4182107448577881            | Epoch Loss: 0.49220213429494336           
Epoch: #69   | Batch: #23   | Batch Loss: 0.4124406576156616            | Epoch Loss: 0.4887342440045398            
Epoch: #69   | Batch: #24   | Batch Loss: 0.5507579445838928            | Epoch Loss: 0.49131856486201286           
Epoch: #69   | Batch: #25   | Batch Loss: 0.4932875633239746            | Epoch Loss: 0.4913973248004913            
Epoch: #69   | Batch: #26   | Batch Loss: 0.38164353370666504           | Epoch Loss: 0.4871760251430365            
Epoch: #69   | Batch: #27   | Batch Loss: 0.5519723892211914            | Epoch Loss: 0.4895758904792644            
Epoch: #69   | Batch: #28   | Batch Loss: 0.5102303624153137            | Epoch Loss: 0.4903135501912662            
Epoch: #69   | Batch: #29   | Batch Loss: 0.5937211513519287            | Epoch Loss: 0.4938793295416339            
Epoch: #69   | Batch: #30   | Batch Loss: 0.5113651156425476            | Epoch Loss: 0.494462189078331             
Epoch: #69   | Batch: #31   | Batch Loss: 0.44452494382858276           | Epoch Loss: 0.49285131019930684           
Epoch: #69   | Batch: #32   | Batch Loss: 0.41934889554977417           | Epoch Loss: 0.49055435974150896           
Epoch: #69   | Batch: #33   | Batch Loss: 0.5701429843902588            | Epoch Loss: 0.49296613624601654           
Epoch: #69   | Batch: #34   | Batch Loss: 0.5526048541069031            | Epoch Loss: 0.4947202161831014            
Epoch: #69   | Batch: #35   | Batch Loss: 0.6275731325149536            | Epoch Loss: 0.4985160137925829            
Epoch: #69   | Batch: #36   | Batch Loss: 0.45838308334350586           | Epoch Loss: 0.49740121016899747           
Epoch: #69   | Batch: #37   | Batch Loss: 0.48079997301101685           | Epoch Loss: 0.4969525280836466            
Epoch: #69   | Batch: #38   | Batch Loss: 0.4156091809272766            | Epoch Loss: 0.49481191368479477           
Epoch: #69   | Batch: #39   | Batch Loss: 0.5102986097335815            | Epoch Loss: 0.4952090084552765            
Epoch: #69   | Batch: #40   | Batch Loss: 0.36360490322113037           | Epoch Loss: 0.4919189058244228            
Epoch: #69   | Batch: #41   | Batch Loss: 0.5084973573684692            | Epoch Loss: 0.4923232583011069            
Epoch: #69   | Batch: #42   | Batch Loss: 0.5292920470237732            | Epoch Loss: 0.4932034675564085            
Epoch: #69   | Batch: #43   | Batch Loss: 0.4662107527256012            | Epoch Loss: 0.49257573000220367           

Classifier Validation Epoch #69
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8454545454545455            
Batch: #2    | Top-1 Accuracy: 0.8590909090909091            
Batch: #3    | Top-1 Accuracy: 0.853030303030303             
Batch: #4    | Top-1 Accuracy: 0.8386363636363636            
Batch: #5    | Top-1 Accuracy: 0.8309090909090908            
Batch: #6    | Top-1 Accuracy: 0.8219696969696969            
Batch: #7    | Top-1 Accuracy: 0.8331168831168831            
Batch: #8    | Top-1 Accuracy: 0.825                         
Batch: #9    | Top-1 Accuracy: 0.8257575757575757            
Batch: #10   | Top-1 Accuracy: 0.8295454545454545            
Batch: #11   | Top-1 Accuracy: 0.8342975206611569            
Batch: #12   | Top-1 Accuracy: 0.8363636363636363            
Batch: #13   | Top-1 Accuracy: 0.8377622377622378            
Batch: #14   | Top-1 Accuracy: 0.8379870129870131            
Batch: #15   | Top-1 Accuracy: 0.8406060606060606            
Batch: #16   | Top-1 Accuracy: 0.8400568181818182            
Batch: #17   | Top-1 Accuracy: 0.8403743315508022            

Classifier Training Epoch #70
------------------------------------------
Epoch: #70   | Batch: #1    | Batch Loss: 0.5789650082588196            | Epoch Loss: 0.5789650082588196            
Epoch: #70   | Batch: #2    | Batch Loss: 0.5073323249816895            | Epoch Loss: 0.5431486666202545            
Epoch: #70   | Batch: #3    | Batch Loss: 0.4336605966091156            | Epoch Loss: 0.5066526432832082            
Epoch: #70   | Batch: #4    | Batch Loss: 0.4358460307121277            | Epoch Loss: 0.4889509901404381            
Epoch: #70   | Batch: #5    | Batch Loss: 0.474167138338089             | Epoch Loss: 0.48599421977996826           
Epoch: #70   | Batch: #6    | Batch Loss: 0.5179633498191833            | Epoch Loss: 0.49132240811983746           
Epoch: #70   | Batch: #7    | Batch Loss: 0.5836349725723267            | Epoch Loss: 0.5045099173273359            
Epoch: #70   | Batch: #8    | Batch Loss: 0.4135625660419464            | Epoch Loss: 0.4931414984166622            
Epoch: #70   | Batch: #9    | Batch Loss: 0.6169021129608154            | Epoch Loss: 0.506892677810457             
Epoch: #70   | Batch: #10   | Batch Loss: 0.42740219831466675           | Epoch Loss: 0.498943629860878             
Epoch: #70   | Batch: #11   | Batch Loss: 0.4753555655479431            | Epoch Loss: 0.4967992603778839            
Epoch: #70   | Batch: #12   | Batch Loss: 0.6368901133537292            | Epoch Loss: 0.5084734981258711            
Epoch: #70   | Batch: #13   | Batch Loss: 0.5072472095489502            | Epoch Loss: 0.5083791682353387            
Epoch: #70   | Batch: #14   | Batch Loss: 0.6123276352882385            | Epoch Loss: 0.5158040587391172            
Epoch: #70   | Batch: #15   | Batch Loss: 0.49859774112701416           | Epoch Loss: 0.5146569708983103            
Epoch: #70   | Batch: #16   | Batch Loss: 0.5999141335487366            | Epoch Loss: 0.519985543563962             
Epoch: #70   | Batch: #17   | Batch Loss: 0.548392117023468             | Epoch Loss: 0.5216565184733447            
Epoch: #70   | Batch: #18   | Batch Loss: 0.5096341967582703            | Epoch Loss: 0.5209886117113961            
Epoch: #70   | Batch: #19   | Batch Loss: 0.554072380065918             | Epoch Loss: 0.5227298626774236            
Epoch: #70   | Batch: #20   | Batch Loss: 0.4447054862976074            | Epoch Loss: 0.5188286438584327            
Epoch: #70   | Batch: #21   | Batch Loss: 0.456563800573349             | Epoch Loss: 0.5158636513210478            
Epoch: #70   | Batch: #22   | Batch Loss: 0.486978143453598             | Epoch Loss: 0.5145506736907092            
Epoch: #70   | Batch: #23   | Batch Loss: 0.5332476496696472            | Epoch Loss: 0.5153635856897935            
Epoch: #70   | Batch: #24   | Batch Loss: 0.5640493035316467            | Epoch Loss: 0.5173921572665373            
Epoch: #70   | Batch: #25   | Batch Loss: 0.5443716645240784            | Epoch Loss: 0.5184713375568389            
Epoch: #70   | Batch: #26   | Batch Loss: 0.629726767539978             | Epoch Loss: 0.5227503925561905            
Epoch: #70   | Batch: #27   | Batch Loss: 0.5282400250434875            | Epoch Loss: 0.5229537122779422            
Epoch: #70   | Batch: #28   | Batch Loss: 0.4106370508670807            | Epoch Loss: 0.51894240294184              
Epoch: #70   | Batch: #29   | Batch Loss: 0.44000503420829773           | Epoch Loss: 0.5162204247096489            
Epoch: #70   | Batch: #30   | Batch Loss: 0.562696099281311             | Epoch Loss: 0.5177696138620377            
Epoch: #70   | Batch: #31   | Batch Loss: 0.5484066605567932            | Epoch Loss: 0.5187579056909007            
Epoch: #70   | Batch: #32   | Batch Loss: 0.45295801758766174           | Epoch Loss: 0.5167016591876745            
Epoch: #70   | Batch: #33   | Batch Loss: 0.5307610630989075            | Epoch Loss: 0.5171277017304392            
Epoch: #70   | Batch: #34   | Batch Loss: 0.6397653818130493            | Epoch Loss: 0.5207346923211041            
Epoch: #70   | Batch: #35   | Batch Loss: 0.3885341286659241            | Epoch Loss: 0.5169575333595275            
Epoch: #70   | Batch: #36   | Batch Loss: 0.4840952157974243            | Epoch Loss: 0.5160446912050247            
Epoch: #70   | Batch: #37   | Batch Loss: 0.5610595941543579            | Epoch Loss: 0.5172613102036554            
Epoch: #70   | Batch: #38   | Batch Loss: 0.47052061557769775           | Epoch Loss: 0.5160312919240249            
Epoch: #70   | Batch: #39   | Batch Loss: 0.5252189040184021            | Epoch Loss: 0.5162668717213166            
Epoch: #70   | Batch: #40   | Batch Loss: 0.6929469704627991            | Epoch Loss: 0.5206838741898536            
Epoch: #70   | Batch: #41   | Batch Loss: 0.6143752932548523            | Epoch Loss: 0.5229690307524146            
Epoch: #70   | Batch: #42   | Batch Loss: 0.5280173420906067            | Epoch Loss: 0.5230892286414192            
Epoch: #70   | Batch: #43   | Batch Loss: 0.42291319370269775           | Epoch Loss: 0.5207595534102861            

Classifier Validation Epoch #70
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.9181818181818182            
Batch: #2    | Top-1 Accuracy: 0.8863636363636364            
Batch: #3    | Top-1 Accuracy: 0.8666666666666667            
Batch: #4    | Top-1 Accuracy: 0.8477272727272728            
Batch: #5    | Top-1 Accuracy: 0.8363636363636363            
Batch: #6    | Top-1 Accuracy: 0.8409090909090908            
Batch: #7    | Top-1 Accuracy: 0.8396103896103896            
Batch: #8    | Top-1 Accuracy: 0.8392045454545454            
Batch: #9    | Top-1 Accuracy: 0.8388888888888888            
Batch: #10   | Top-1 Accuracy: 0.8395454545454545            
Batch: #11   | Top-1 Accuracy: 0.8417355371900825            
Batch: #12   | Top-1 Accuracy: 0.8443181818181817            
Batch: #13   | Top-1 Accuracy: 0.8465034965034964            
Batch: #14   | Top-1 Accuracy: 0.8477272727272727            
Batch: #15   | Top-1 Accuracy: 0.8478787878787878            
Batch: #16   | Top-1 Accuracy: 0.8488636363636364            
Batch: #17   | Top-1 Accuracy: 0.8494652406417113            

Classifier Training Epoch #71
------------------------------------------
Epoch: #71   | Batch: #1    | Batch Loss: 0.46448957920074463           | Epoch Loss: 0.46448957920074463           
Epoch: #71   | Batch: #2    | Batch Loss: 0.3877607583999634            | Epoch Loss: 0.426125168800354             
Epoch: #71   | Batch: #3    | Batch Loss: 0.4857286512851715            | Epoch Loss: 0.44599299629529315           
Epoch: #71   | Batch: #4    | Batch Loss: 0.4697502851486206            | Epoch Loss: 0.45193231850862503           
Epoch: #71   | Batch: #5    | Batch Loss: 0.5183361768722534            | Epoch Loss: 0.4652130901813507            
Epoch: #71   | Batch: #6    | Batch Loss: 0.4289993941783905            | Epoch Loss: 0.45917747418085736           
Epoch: #71   | Batch: #7    | Batch Loss: 0.5069115161895752            | Epoch Loss: 0.4659966230392456            
Epoch: #71   | Batch: #8    | Batch Loss: 0.5112919807434082            | Epoch Loss: 0.47165854275226593           
Epoch: #71   | Batch: #9    | Batch Loss: 0.5234673023223877            | Epoch Loss: 0.4774150715933906            
Epoch: #71   | Batch: #10   | Batch Loss: 0.5620883107185364            | Epoch Loss: 0.4858823955059052            
Epoch: #71   | Batch: #11   | Batch Loss: 0.39177805185317993           | Epoch Loss: 0.47732745517383923           
Epoch: #71   | Batch: #12   | Batch Loss: 0.5088256001472473            | Epoch Loss: 0.4799523005882899            
Epoch: #71   | Batch: #13   | Batch Loss: 0.5134979486465454            | Epoch Loss: 0.48253273505430955           
Epoch: #71   | Batch: #14   | Batch Loss: 0.48848363757133484           | Epoch Loss: 0.4829577995198114            
Epoch: #71   | Batch: #15   | Batch Loss: 0.4369867742061615            | Epoch Loss: 0.47989306449890134           
Epoch: #71   | Batch: #16   | Batch Loss: 0.4278678596019745            | Epoch Loss: 0.47664148919284344           
Epoch: #71   | Batch: #17   | Batch Loss: 0.40917760133743286           | Epoch Loss: 0.4726730252013487            
Epoch: #71   | Batch: #18   | Batch Loss: 0.4755553603172302            | Epoch Loss: 0.47283315493000877           
Epoch: #71   | Batch: #19   | Batch Loss: 0.4875912368297577            | Epoch Loss: 0.47360989608262716           
Epoch: #71   | Batch: #20   | Batch Loss: 0.472514808177948             | Epoch Loss: 0.4735551416873932            
Epoch: #71   | Batch: #21   | Batch Loss: 0.39843565225601196           | Epoch Loss: 0.4699780231430417            
Epoch: #71   | Batch: #22   | Batch Loss: 0.4236214756965637            | Epoch Loss: 0.46787090735001996           
Epoch: #71   | Batch: #23   | Batch Loss: 0.5449777245521545            | Epoch Loss: 0.47122337766315625           
Epoch: #71   | Batch: #24   | Batch Loss: 0.47777697443962097           | Epoch Loss: 0.47149644419550896           
Epoch: #71   | Batch: #25   | Batch Loss: 0.5245707631111145            | Epoch Loss: 0.47361941695213317           
Epoch: #71   | Batch: #26   | Batch Loss: 0.3656872510910034            | Epoch Loss: 0.4694681798036282            
Epoch: #71   | Batch: #27   | Batch Loss: 0.5997097492218018            | Epoch Loss: 0.4742919416339309            
Epoch: #71   | Batch: #28   | Batch Loss: 0.4210183918476105            | Epoch Loss: 0.47238931485584806           
Epoch: #71   | Batch: #29   | Batch Loss: 0.4628596007823944            | Epoch Loss: 0.47206070402572897           
Epoch: #71   | Batch: #30   | Batch Loss: 0.5649667978286743            | Epoch Loss: 0.47515757381916046           
Epoch: #71   | Batch: #31   | Batch Loss: 0.4673718810081482            | Epoch Loss: 0.47490642243816006           
Epoch: #71   | Batch: #32   | Batch Loss: 0.36645254492759705           | Epoch Loss: 0.47151723876595497           
Epoch: #71   | Batch: #33   | Batch Loss: 0.42494457960128784           | Epoch Loss: 0.47010594606399536           
Epoch: #71   | Batch: #34   | Batch Loss: 0.5747185945510864            | Epoch Loss: 0.4731827886665569            
Epoch: #71   | Batch: #35   | Batch Loss: 0.4731455147266388            | Epoch Loss: 0.4731817236968449            
Epoch: #71   | Batch: #36   | Batch Loss: 0.37798479199409485           | Epoch Loss: 0.47053736448287964           
Epoch: #71   | Batch: #37   | Batch Loss: 0.39809608459472656           | Epoch Loss: 0.4685794920534701            
Epoch: #71   | Batch: #38   | Batch Loss: 0.462742418050766             | Epoch Loss: 0.4684258848428726            
Epoch: #71   | Batch: #39   | Batch Loss: 0.4547904133796692            | Epoch Loss: 0.4680762573694571            
Epoch: #71   | Batch: #40   | Batch Loss: 0.3768479824066162            | Epoch Loss: 0.4657955504953861            
Epoch: #71   | Batch: #41   | Batch Loss: 0.35471370816230774           | Epoch Loss: 0.46308623726775006           
Epoch: #71   | Batch: #42   | Batch Loss: 0.349353551864624             | Epoch Loss: 0.46037831618672326           
Epoch: #71   | Batch: #43   | Batch Loss: 0.5211939811706543            | Epoch Loss: 0.46179263397704723           

Classifier Validation Epoch #71
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.7772727272727272            
Batch: #2    | Top-1 Accuracy: 0.8181818181818181            
Batch: #3    | Top-1 Accuracy: 0.8303030303030302            
Batch: #4    | Top-1 Accuracy: 0.8397727272727272            
Batch: #5    | Top-1 Accuracy: 0.8345454545454546            
Batch: #6    | Top-1 Accuracy: 0.8393939393939394            
Batch: #7    | Top-1 Accuracy: 0.8409090909090908            
Batch: #8    | Top-1 Accuracy: 0.8409090909090908            
Batch: #9    | Top-1 Accuracy: 0.8398989898989898            
Batch: #10   | Top-1 Accuracy: 0.8418181818181818            
Batch: #11   | Top-1 Accuracy: 0.8396694214876033            
Batch: #12   | Top-1 Accuracy: 0.840530303030303             
Batch: #13   | Top-1 Accuracy: 0.8398601398601399            
Batch: #14   | Top-1 Accuracy: 0.8386363636363636            
Batch: #15   | Top-1 Accuracy: 0.8396969696969696            
Batch: #16   | Top-1 Accuracy: 0.8403409090909091            
Batch: #17   | Top-1 Accuracy: 0.8425133689839572            

Classifier Training Epoch #72
------------------------------------------
Epoch: #72   | Batch: #1    | Batch Loss: 0.5501933097839355            | Epoch Loss: 0.5501933097839355            
Epoch: #72   | Batch: #2    | Batch Loss: 0.5149816274642944            | Epoch Loss: 0.532587468624115             
Epoch: #72   | Batch: #3    | Batch Loss: 0.5289859175682068            | Epoch Loss: 0.5313869516054789            
Epoch: #72   | Batch: #4    | Batch Loss: 0.44162610173225403           | Epoch Loss: 0.5089467391371727            
Epoch: #72   | Batch: #5    | Batch Loss: 0.4358363747596741            | Epoch Loss: 0.494324666261673             
Epoch: #72   | Batch: #6    | Batch Loss: 0.468318372964859             | Epoch Loss: 0.4899902840455373            
Epoch: #72   | Batch: #7    | Batch Loss: 0.5110650062561035            | Epoch Loss: 0.4930009586470468            
Epoch: #72   | Batch: #8    | Batch Loss: 0.326217919588089             | Epoch Loss: 0.47215307876467705           
Epoch: #72   | Batch: #9    | Batch Loss: 0.4946869909763336            | Epoch Loss: 0.4746568467881944            
Epoch: #72   | Batch: #10   | Batch Loss: 0.5331032872200012            | Epoch Loss: 0.4805014908313751            
Epoch: #72   | Batch: #11   | Batch Loss: 0.5835186839103699            | Epoch Loss: 0.4898666902021928            
Epoch: #72   | Batch: #12   | Batch Loss: 0.44466105103492737           | Epoch Loss: 0.4860995536049207            
Epoch: #72   | Batch: #13   | Batch Loss: 0.5528493523597717            | Epoch Loss: 0.49123415350914              
Epoch: #72   | Batch: #14   | Batch Loss: 0.619593620300293             | Epoch Loss: 0.5004026868513652            
Epoch: #72   | Batch: #15   | Batch Loss: 0.5565651059150696            | Epoch Loss: 0.5041468481222788            
Epoch: #72   | Batch: #16   | Batch Loss: 0.49817946553230286           | Epoch Loss: 0.5037738867104053            
Epoch: #72   | Batch: #17   | Batch Loss: 0.3938899338245392            | Epoch Loss: 0.4973101247759426            
Epoch: #72   | Batch: #18   | Batch Loss: 0.46460768580436707           | Epoch Loss: 0.4954933226108551            
Epoch: #72   | Batch: #19   | Batch Loss: 0.48658856749534607           | Epoch Loss: 0.4950246512889862            
Epoch: #72   | Batch: #20   | Batch Loss: 0.35794970393180847           | Epoch Loss: 0.48817090392112733           
Epoch: #72   | Batch: #21   | Batch Loss: 0.4694695770740509            | Epoch Loss: 0.487280364547457             
Epoch: #72   | Batch: #22   | Batch Loss: 0.4384886622428894            | Epoch Loss: 0.4850625598972494            
Epoch: #72   | Batch: #23   | Batch Loss: 0.4144253432750702            | Epoch Loss: 0.4819913765658503            
Epoch: #72   | Batch: #24   | Batch Loss: 0.5170871615409851            | Epoch Loss: 0.48345370093981427           
Epoch: #72   | Batch: #25   | Batch Loss: 0.4676385819911957            | Epoch Loss: 0.4828210961818695            
Epoch: #72   | Batch: #26   | Batch Loss: 0.428281307220459             | Epoch Loss: 0.480723411991046             
Epoch: #72   | Batch: #27   | Batch Loss: 0.4065552353858948            | Epoch Loss: 0.47797644248715154           
Epoch: #72   | Batch: #28   | Batch Loss: 0.4847165048122406            | Epoch Loss: 0.4782171589987619            
Epoch: #72   | Batch: #29   | Batch Loss: 0.41483694314956665           | Epoch Loss: 0.47603163431430684           
Epoch: #72   | Batch: #30   | Batch Loss: 0.5203412175178528            | Epoch Loss: 0.4775086204210917            
Epoch: #72   | Batch: #31   | Batch Loss: 0.40897390246391296           | Epoch Loss: 0.47529782306763435           
Epoch: #72   | Batch: #32   | Batch Loss: 0.36451205611228943           | Epoch Loss: 0.4718357678502798            
Epoch: #72   | Batch: #33   | Batch Loss: 0.48075953125953674           | Epoch Loss: 0.4721061849232876            
Epoch: #72   | Batch: #34   | Batch Loss: 0.4788570702075958            | Epoch Loss: 0.47230474037282605           
Epoch: #72   | Batch: #35   | Batch Loss: 0.5777304172515869            | Epoch Loss: 0.4753169025693621            
Epoch: #72   | Batch: #36   | Batch Loss: 0.4820800721645355            | Epoch Loss: 0.4755047683914502            
Epoch: #72   | Batch: #37   | Batch Loss: 0.5520254373550415            | Epoch Loss: 0.4775728945796554            
Epoch: #72   | Batch: #38   | Batch Loss: 0.5773965716362               | Epoch Loss: 0.48019983344956446           
Epoch: #72   | Batch: #39   | Batch Loss: 0.526607871055603             | Epoch Loss: 0.4813897831317706            
Epoch: #72   | Batch: #40   | Batch Loss: 0.597900927066803             | Epoch Loss: 0.4843025617301464            
Epoch: #72   | Batch: #41   | Batch Loss: 0.4389713406562805            | Epoch Loss: 0.4831969221917594            
Epoch: #72   | Batch: #42   | Batch Loss: 0.49580618739128113           | Epoch Loss: 0.48349714279174805           
Epoch: #72   | Batch: #43   | Batch Loss: 0.48990583419799805           | Epoch Loss: 0.4836461821267771            

Classifier Validation Epoch #72
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8772727272727273            
Batch: #2    | Top-1 Accuracy: 0.8386363636363636            
Batch: #3    | Top-1 Accuracy: 0.846969696969697             
Batch: #4    | Top-1 Accuracy: 0.8465909090909092            
Batch: #5    | Top-1 Accuracy: 0.8472727272727273            
Batch: #6    | Top-1 Accuracy: 0.8439393939393939            
Batch: #7    | Top-1 Accuracy: 0.8454545454545455            
Batch: #8    | Top-1 Accuracy: 0.8482954545454545            
Batch: #9    | Top-1 Accuracy: 0.848989898989899             
Batch: #10   | Top-1 Accuracy: 0.8518181818181818            
Batch: #11   | Top-1 Accuracy: 0.847107438016529             
Batch: #12   | Top-1 Accuracy: 0.8458333333333333            
Batch: #13   | Top-1 Accuracy: 0.8440559440559441            
Batch: #14   | Top-1 Accuracy: 0.8457792207792207            
Batch: #15   | Top-1 Accuracy: 0.8466666666666666            
Batch: #16   | Top-1 Accuracy: 0.8477272727272727            
Batch: #17   | Top-1 Accuracy: 0.8457219251336898            

Classifier Training Epoch #73
------------------------------------------
Epoch: #73   | Batch: #1    | Batch Loss: 0.35531026124954224           | Epoch Loss: 0.35531026124954224           
Epoch: #73   | Batch: #2    | Batch Loss: 0.4868175983428955            | Epoch Loss: 0.42106392979621887           
Epoch: #73   | Batch: #3    | Batch Loss: 0.5240828990936279            | Epoch Loss: 0.45540358622868854           
Epoch: #73   | Batch: #4    | Batch Loss: 0.5004012584686279            | Epoch Loss: 0.4666530042886734            
Epoch: #73   | Batch: #5    | Batch Loss: 0.43325236439704895           | Epoch Loss: 0.4599728763103485            
Epoch: #73   | Batch: #6    | Batch Loss: 0.40859103202819824           | Epoch Loss: 0.4514092355966568            
Epoch: #73   | Batch: #7    | Batch Loss: 0.5123693346977234            | Epoch Loss: 0.4601178211825235            
Epoch: #73   | Batch: #8    | Batch Loss: 0.5119205713272095            | Epoch Loss: 0.4665931649506092            
Epoch: #73   | Batch: #9    | Batch Loss: 0.4536498188972473            | Epoch Loss: 0.46515501538912457           
Epoch: #73   | Batch: #10   | Batch Loss: 0.5160425901412964            | Epoch Loss: 0.4702437728643417            
Epoch: #73   | Batch: #11   | Batch Loss: 0.5868838429450989            | Epoch Loss: 0.480847415598956             
Epoch: #73   | Batch: #12   | Batch Loss: 0.5175743103027344            | Epoch Loss: 0.4839079901576042            
Epoch: #73   | Batch: #13   | Batch Loss: 0.49734848737716675           | Epoch Loss: 0.48494187455910903           
Epoch: #73   | Batch: #14   | Batch Loss: 0.41840222477912903           | Epoch Loss: 0.4801890424319676            
Epoch: #73   | Batch: #15   | Batch Loss: 0.5814136862754822            | Epoch Loss: 0.48693735202153526           
Epoch: #73   | Batch: #16   | Batch Loss: 0.5779889822006226            | Epoch Loss: 0.4926280789077282            
Epoch: #73   | Batch: #17   | Batch Loss: 0.5651656985282898            | Epoch Loss: 0.4968949977089377            
Epoch: #73   | Batch: #18   | Batch Loss: 0.5937222838401794            | Epoch Loss: 0.5022742913828956            
Epoch: #73   | Batch: #19   | Batch Loss: 0.5712668895721436            | Epoch Loss: 0.5059054807612771            
Epoch: #73   | Batch: #20   | Batch Loss: 0.45038527250289917           | Epoch Loss: 0.5031294703483582            
Epoch: #73   | Batch: #21   | Batch Loss: 0.4603557288646698            | Epoch Loss: 0.5010926255158016            
Epoch: #73   | Batch: #22   | Batch Loss: 0.43986138701438904           | Epoch Loss: 0.498309387402101             
Epoch: #73   | Batch: #23   | Batch Loss: 0.48540300130844116           | Epoch Loss: 0.4977482401806375            
Epoch: #73   | Batch: #24   | Batch Loss: 0.706080973148346             | Epoch Loss: 0.5064287707209587            
Epoch: #73   | Batch: #25   | Batch Loss: 0.5054922699928284            | Epoch Loss: 0.5063913106918335            
Epoch: #73   | Batch: #26   | Batch Loss: 0.6286872625350952            | Epoch Loss: 0.5110950011473435            
Epoch: #73   | Batch: #27   | Batch Loss: 0.46942731738090515           | Epoch Loss: 0.5095517536004385            
Epoch: #73   | Batch: #28   | Batch Loss: 0.5796384811401367            | Epoch Loss: 0.5120548510125705            
Epoch: #73   | Batch: #29   | Batch Loss: 0.4905700087547302            | Epoch Loss: 0.5113139943829899            
Epoch: #73   | Batch: #30   | Batch Loss: 0.6806306838989258            | Epoch Loss: 0.516957884033521             
Epoch: #73   | Batch: #31   | Batch Loss: 0.6087630391120911            | Epoch Loss: 0.5199193406489587            
Epoch: #73   | Batch: #32   | Batch Loss: 0.4880848824977875            | Epoch Loss: 0.5189245138317347            
Epoch: #73   | Batch: #33   | Batch Loss: 0.46026527881622314           | Epoch Loss: 0.5171469612555071            
Epoch: #73   | Batch: #34   | Batch Loss: 0.40238943696022034           | Epoch Loss: 0.5137717399527045            
Epoch: #73   | Batch: #35   | Batch Loss: 0.44238266348838806           | Epoch Loss: 0.511732052053724             
Epoch: #73   | Batch: #36   | Batch Loss: 0.4754548966884613            | Epoch Loss: 0.5107243532935778            
Epoch: #73   | Batch: #37   | Batch Loss: 0.4338141977787018            | Epoch Loss: 0.5086457004418244            
Epoch: #73   | Batch: #38   | Batch Loss: 0.5228946805000305            | Epoch Loss: 0.5090206736012509            
Epoch: #73   | Batch: #39   | Batch Loss: 0.48558399081230164           | Epoch Loss: 0.5084197330169189            
Epoch: #73   | Batch: #40   | Batch Loss: 0.41440558433532715           | Epoch Loss: 0.506069379299879             
Epoch: #73   | Batch: #41   | Batch Loss: 0.5219499468803406            | Epoch Loss: 0.5064567102164756            
Epoch: #73   | Batch: #42   | Batch Loss: 0.45401695370674133           | Epoch Loss: 0.5052081445852915            
Epoch: #73   | Batch: #43   | Batch Loss: 0.4513023793697357            | Epoch Loss: 0.5039545221384182            

Classifier Validation Epoch #73
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8409090909090909            
Batch: #2    | Top-1 Accuracy: 0.8431818181818183            
Batch: #3    | Top-1 Accuracy: 0.8378787878787879            
Batch: #4    | Top-1 Accuracy: 0.8375                        
Batch: #5    | Top-1 Accuracy: 0.8436363636363637            
Batch: #6    | Top-1 Accuracy: 0.8371212121212123            
Batch: #7    | Top-1 Accuracy: 0.8344155844155845            
Batch: #8    | Top-1 Accuracy: 0.83125                       
Batch: #9    | Top-1 Accuracy: 0.8373737373737374            
Batch: #10   | Top-1 Accuracy: 0.8368181818181819            
Batch: #11   | Top-1 Accuracy: 0.8367768595041323            
Batch: #12   | Top-1 Accuracy: 0.8386363636363637            
Batch: #13   | Top-1 Accuracy: 0.8416083916083916            
Batch: #14   | Top-1 Accuracy: 0.8444805194805196            
Batch: #15   | Top-1 Accuracy: 0.844848484848485             
Batch: #16   | Top-1 Accuracy: 0.8460227272727273            
Batch: #17   | Top-1 Accuracy: 0.8435828877005348            

Classifier Training Epoch #74
------------------------------------------
Epoch: #74   | Batch: #1    | Batch Loss: 0.44549864530563354           | Epoch Loss: 0.44549864530563354           
Epoch: #74   | Batch: #2    | Batch Loss: 0.44053640961647034           | Epoch Loss: 0.44301752746105194           
Epoch: #74   | Batch: #3    | Batch Loss: 0.5200899839401245            | Epoch Loss: 0.4687083462874095            
Epoch: #74   | Batch: #4    | Batch Loss: 0.40032607316970825           | Epoch Loss: 0.45161277800798416           
Epoch: #74   | Batch: #5    | Batch Loss: 0.4806055724620819            | Epoch Loss: 0.4574113368988037            
Epoch: #74   | Batch: #6    | Batch Loss: 0.455708771944046             | Epoch Loss: 0.45712757607301074           
Epoch: #74   | Batch: #7    | Batch Loss: 0.46386459469795227           | Epoch Loss: 0.45809000730514526           
Epoch: #74   | Batch: #8    | Batch Loss: 0.5092648267745972            | Epoch Loss: 0.46448685973882675           
Epoch: #74   | Batch: #9    | Batch Loss: 0.4617674648761749            | Epoch Loss: 0.4641847047540877            
Epoch: #74   | Batch: #10   | Batch Loss: 0.637851893901825             | Epoch Loss: 0.4815514236688614            
Epoch: #74   | Batch: #11   | Batch Loss: 0.4016437530517578            | Epoch Loss: 0.4742870899763974            
Epoch: #74   | Batch: #12   | Batch Loss: 0.47803881764411926           | Epoch Loss: 0.4745997339487076            
Epoch: #74   | Batch: #13   | Batch Loss: 0.5240956544876099            | Epoch Loss: 0.4784071124517001            
Epoch: #74   | Batch: #14   | Batch Loss: 0.3480733633041382            | Epoch Loss: 0.46909755894115995           
Epoch: #74   | Batch: #15   | Batch Loss: 0.4335901141166687            | Epoch Loss: 0.46673039595286053           
Epoch: #74   | Batch: #16   | Batch Loss: 0.5225333571434021            | Epoch Loss: 0.47021808102726936           
Epoch: #74   | Batch: #17   | Batch Loss: 0.48047927021980286           | Epoch Loss: 0.47082168039153605           
Epoch: #74   | Batch: #18   | Batch Loss: 0.42904892563819885           | Epoch Loss: 0.4685009717941284            
Epoch: #74   | Batch: #19   | Batch Loss: 0.5818064212799072            | Epoch Loss: 0.47446441650390625           
Epoch: #74   | Batch: #20   | Batch Loss: 0.45953047275543213           | Epoch Loss: 0.47371771931648254           
Epoch: #74   | Batch: #21   | Batch Loss: 0.4593050479888916            | Epoch Loss: 0.4730314016342163            
Epoch: #74   | Batch: #22   | Batch Loss: 0.42176878452301025           | Epoch Loss: 0.47070128267461603           
Epoch: #74   | Batch: #23   | Batch Loss: 0.4846856892108917            | Epoch Loss: 0.47130930035010626           
Epoch: #74   | Batch: #24   | Batch Loss: 0.3945600688457489            | Epoch Loss: 0.46811141570409137           
Epoch: #74   | Batch: #25   | Batch Loss: 0.504132091999054             | Epoch Loss: 0.4695522427558899            
Epoch: #74   | Batch: #26   | Batch Loss: 0.44669654965400696           | Epoch Loss: 0.4686731776365867            
Epoch: #74   | Batch: #27   | Batch Loss: 0.5702208876609802            | Epoch Loss: 0.4724342039337865            
Epoch: #74   | Batch: #28   | Batch Loss: 0.48710906505584717           | Epoch Loss: 0.4729583061167172            
Epoch: #74   | Batch: #29   | Batch Loss: 0.3895207941532135            | Epoch Loss: 0.4700811505317688            
Epoch: #74   | Batch: #30   | Batch Loss: 0.44047966599464417           | Epoch Loss: 0.46909443438053133           
Epoch: #74   | Batch: #31   | Batch Loss: 0.6008474826812744            | Epoch Loss: 0.47334453271281335           
Epoch: #74   | Batch: #32   | Batch Loss: 0.4606274664402008            | Epoch Loss: 0.4729471243917942            
Epoch: #74   | Batch: #33   | Batch Loss: 0.4518294632434845            | Epoch Loss: 0.47230719526608783           
Epoch: #74   | Batch: #34   | Batch Loss: 0.41273611783981323           | Epoch Loss: 0.47055510475355034           
Epoch: #74   | Batch: #35   | Batch Loss: 0.43038398027420044           | Epoch Loss: 0.46940735833985464           
Epoch: #74   | Batch: #36   | Batch Loss: 0.5323517918586731            | Epoch Loss: 0.4711558148264885            
Epoch: #74   | Batch: #37   | Batch Loss: 0.29237034916877747           | Epoch Loss: 0.46632377521411794           
Epoch: #74   | Batch: #38   | Batch Loss: 0.5022512674331665            | Epoch Loss: 0.46726923553567185           
Epoch: #74   | Batch: #39   | Batch Loss: 0.48896950483322144           | Epoch Loss: 0.4678256526971475            
Epoch: #74   | Batch: #40   | Batch Loss: 0.5034340620040894            | Epoch Loss: 0.46871586292982104           
Epoch: #74   | Batch: #41   | Batch Loss: 0.5037477612495422            | Epoch Loss: 0.4695702994742045            
Epoch: #74   | Batch: #42   | Batch Loss: 0.3819839358329773            | Epoch Loss: 0.46748490986369906           
Epoch: #74   | Batch: #43   | Batch Loss: 0.46151143312454224           | Epoch Loss: 0.46734599179999775           

Classifier Validation Epoch #74
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8409090909090909            
Batch: #2    | Top-1 Accuracy: 0.8386363636363636            
Batch: #3    | Top-1 Accuracy: 0.846969696969697             
Batch: #4    | Top-1 Accuracy: 0.8340909090909091            
Batch: #5    | Top-1 Accuracy: 0.8372727272727273            
Batch: #6    | Top-1 Accuracy: 0.8446969696969697            
Batch: #7    | Top-1 Accuracy: 0.8474025974025974            
Batch: #8    | Top-1 Accuracy: 0.8397727272727273            
Batch: #9    | Top-1 Accuracy: 0.8383838383838385            
Batch: #10   | Top-1 Accuracy: 0.8418181818181818            
Batch: #11   | Top-1 Accuracy: 0.8425619834710744            
Batch: #12   | Top-1 Accuracy: 0.8454545454545453            
Batch: #13   | Top-1 Accuracy: 0.8447552447552447            
Batch: #14   | Top-1 Accuracy: 0.8457792207792207            
Batch: #15   | Top-1 Accuracy: 0.846060606060606             
Batch: #16   | Top-1 Accuracy: 0.8508522727272727            
Batch: #17   | Top-1 Accuracy: 0.8508021390374331            

Classifier Training Epoch #75
------------------------------------------
Epoch: #75   | Batch: #1    | Batch Loss: 0.4723584055900574            | Epoch Loss: 0.4723584055900574            
Epoch: #75   | Batch: #2    | Batch Loss: 0.3757391571998596            | Epoch Loss: 0.4240487813949585            
Epoch: #75   | Batch: #3    | Batch Loss: 0.5004199147224426            | Epoch Loss: 0.4495058258374532            
Epoch: #75   | Batch: #4    | Batch Loss: 0.4303079843521118            | Epoch Loss: 0.44470636546611786           
Epoch: #75   | Batch: #5    | Batch Loss: 0.5294374227523804            | Epoch Loss: 0.46165257692337036           
Epoch: #75   | Batch: #6    | Batch Loss: 0.44106313586235046           | Epoch Loss: 0.4582210034132004            
Epoch: #75   | Batch: #7    | Batch Loss: 0.5178369283676147            | Epoch Loss: 0.46673756412097384           
Epoch: #75   | Batch: #8    | Batch Loss: 0.41598692536354065           | Epoch Loss: 0.4603937342762947            
Epoch: #75   | Batch: #9    | Batch Loss: 0.4119695723056793            | Epoch Loss: 0.4550132718351152            
Epoch: #75   | Batch: #10   | Batch Loss: 0.41310733556747437           | Epoch Loss: 0.45082267820835115           
Epoch: #75   | Batch: #11   | Batch Loss: 0.5417599678039551            | Epoch Loss: 0.45908970453522424           
Epoch: #75   | Batch: #12   | Batch Loss: 0.5815653800964355            | Epoch Loss: 0.46929601083199185           
Epoch: #75   | Batch: #13   | Batch Loss: 0.41290348768234253           | Epoch Loss: 0.464958124435865             
Epoch: #75   | Batch: #14   | Batch Loss: 0.40573951601982117           | Epoch Loss: 0.46072822383471895           
Epoch: #75   | Batch: #15   | Batch Loss: 0.4755042791366577            | Epoch Loss: 0.4617132941881816            
Epoch: #75   | Batch: #16   | Batch Loss: 0.5366304516792297            | Epoch Loss: 0.46639561653137207           
Epoch: #75   | Batch: #17   | Batch Loss: 0.48623529076576233           | Epoch Loss: 0.46756265619221854           
Epoch: #75   | Batch: #18   | Batch Loss: 0.38478684425354004           | Epoch Loss: 0.4629639999734031            
Epoch: #75   | Batch: #19   | Batch Loss: 0.7185156941413879            | Epoch Loss: 0.4764140891401391            
Epoch: #75   | Batch: #20   | Batch Loss: 0.44460126757621765           | Epoch Loss: 0.47482344806194304           
Epoch: #75   | Batch: #21   | Batch Loss: 0.4017062485218048            | Epoch Loss: 0.4713416766552698            
Epoch: #75   | Batch: #22   | Batch Loss: 0.4331187903881073            | Epoch Loss: 0.46960427273403516           
Epoch: #75   | Batch: #23   | Batch Loss: 0.535713791847229             | Epoch Loss: 0.4724785996520001            
Epoch: #75   | Batch: #24   | Batch Loss: 0.4260467290878296            | Epoch Loss: 0.470543938378493             
Epoch: #75   | Batch: #25   | Batch Loss: 0.3846888840198517            | Epoch Loss: 0.46710973620414736           
Epoch: #75   | Batch: #26   | Batch Loss: 0.47879981994628906           | Epoch Loss: 0.4675593548096143            
Epoch: #75   | Batch: #27   | Batch Loss: 0.5225006341934204            | Epoch Loss: 0.46959421700901455           
Epoch: #75   | Batch: #28   | Batch Loss: 0.4327269494533539            | Epoch Loss: 0.4682775288820267            
Epoch: #75   | Batch: #29   | Batch Loss: 0.4731983244419098            | Epoch Loss: 0.46844721148753987           
Epoch: #75   | Batch: #30   | Batch Loss: 0.5105320811271667            | Epoch Loss: 0.4698500404755274            
Epoch: #75   | Batch: #31   | Batch Loss: 0.4004075527191162            | Epoch Loss: 0.46760996022532064           
Epoch: #75   | Batch: #32   | Batch Loss: 0.5022910833358765            | Epoch Loss: 0.4686937453225255            
Epoch: #75   | Batch: #33   | Batch Loss: 0.4604097306728363            | Epoch Loss: 0.46844271457556524           
Epoch: #75   | Batch: #34   | Batch Loss: 0.4143598973751068            | Epoch Loss: 0.4668520434814341            
Epoch: #75   | Batch: #35   | Batch Loss: 0.4015995264053345            | Epoch Loss: 0.4649876858506884            
Epoch: #75   | Batch: #36   | Batch Loss: 0.4830360412597656            | Epoch Loss: 0.4654890290564961            
Epoch: #75   | Batch: #37   | Batch Loss: 0.501833438873291             | Epoch Loss: 0.46647131040289597           
Epoch: #75   | Batch: #38   | Batch Loss: 0.512439489364624             | Epoch Loss: 0.46768099932294144           
Epoch: #75   | Batch: #39   | Batch Loss: 0.5214836001396179            | Epoch Loss: 0.4690605531900357            
Epoch: #75   | Batch: #40   | Batch Loss: 0.5344059467315674            | Epoch Loss: 0.470694188028574             
Epoch: #75   | Batch: #41   | Batch Loss: 0.44353246688842773           | Epoch Loss: 0.4700317070251558            
Epoch: #75   | Batch: #42   | Batch Loss: 0.44019564986228943           | Epoch Loss: 0.4693213247117542            
Epoch: #75   | Batch: #43   | Batch Loss: 0.4550217390060425            | Epoch Loss: 0.4689887762069702            

Classifier Validation Epoch #75
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.85                          
Batch: #2    | Top-1 Accuracy: 0.8545454545454545            
Batch: #3    | Top-1 Accuracy: 0.85                          
Batch: #4    | Top-1 Accuracy: 0.85                          
Batch: #5    | Top-1 Accuracy: 0.8527272727272728            
Batch: #6    | Top-1 Accuracy: 0.853030303030303             
Batch: #7    | Top-1 Accuracy: 0.8558441558441559            
Batch: #8    | Top-1 Accuracy: 0.8585227272727273            
Batch: #9    | Top-1 Accuracy: 0.856060606060606             
Batch: #10   | Top-1 Accuracy: 0.8545454545454545            
Batch: #11   | Top-1 Accuracy: 0.8557851239669422            
Batch: #12   | Top-1 Accuracy: 0.8537878787878789            
Batch: #13   | Top-1 Accuracy: 0.8548951048951049            
Batch: #14   | Top-1 Accuracy: 0.852922077922078             
Batch: #15   | Top-1 Accuracy: 0.8512121212121213            
Batch: #16   | Top-1 Accuracy: 0.8494318181818182            
Batch: #17   | Top-1 Accuracy: 0.8505347593582888            

Classifier Training Epoch #76
------------------------------------------
Epoch: #76   | Batch: #1    | Batch Loss: 0.4583907425403595            | Epoch Loss: 0.4583907425403595            
Epoch: #76   | Batch: #2    | Batch Loss: 0.4629940986633301            | Epoch Loss: 0.4606924206018448            
Epoch: #76   | Batch: #3    | Batch Loss: 0.5594286322593689            | Epoch Loss: 0.49360449115435284           
Epoch: #76   | Batch: #4    | Batch Loss: 0.5029553174972534            | Epoch Loss: 0.495942197740078             
Epoch: #76   | Batch: #5    | Batch Loss: 0.4337009787559509            | Epoch Loss: 0.48349395394325256           
Epoch: #76   | Batch: #6    | Batch Loss: 0.3786979615688324            | Epoch Loss: 0.46602795521418255           
Epoch: #76   | Batch: #7    | Batch Loss: 0.657870888710022             | Epoch Loss: 0.493434088570731             
Epoch: #76   | Batch: #8    | Batch Loss: 0.5283204317092896            | Epoch Loss: 0.49779488146305084           
Epoch: #76   | Batch: #9    | Batch Loss: 0.46270236372947693           | Epoch Loss: 0.4938957128259871            
Epoch: #76   | Batch: #10   | Batch Loss: 0.45635026693344116           | Epoch Loss: 0.4901411682367325            
Epoch: #76   | Batch: #11   | Batch Loss: 0.4006839096546173            | Epoch Loss: 0.4820086901838129            
Epoch: #76   | Batch: #12   | Batch Loss: 0.39420610666275024           | Epoch Loss: 0.47469180822372437           
Epoch: #76   | Batch: #13   | Batch Loss: 0.5727596879005432            | Epoch Loss: 0.48223549127578735           
Epoch: #76   | Batch: #14   | Batch Loss: 0.5189089775085449            | Epoch Loss: 0.4848550260066986            
Epoch: #76   | Batch: #15   | Batch Loss: 0.3965100646018982            | Epoch Loss: 0.47896536191304523           
Epoch: #76   | Batch: #16   | Batch Loss: 0.5193762183189392            | Epoch Loss: 0.4814910404384136            
Epoch: #76   | Batch: #17   | Batch Loss: 0.3975740969181061            | Epoch Loss: 0.4765547496431014            
Epoch: #76   | Batch: #18   | Batch Loss: 0.45822063088417053           | Epoch Loss: 0.47553618748982746           
Epoch: #76   | Batch: #19   | Batch Loss: 0.5180762410163879            | Epoch Loss: 0.47777513767543595           
Epoch: #76   | Batch: #20   | Batch Loss: 0.6189796924591064            | Epoch Loss: 0.48483536541461947           
Epoch: #76   | Batch: #21   | Batch Loss: 0.5161808729171753            | Epoch Loss: 0.48632800862902686           
Epoch: #76   | Batch: #22   | Batch Loss: 0.4724586009979248            | Epoch Loss: 0.48569758100943133           
Epoch: #76   | Batch: #23   | Batch Loss: 0.36938247084617615           | Epoch Loss: 0.4806404023066811            
Epoch: #76   | Batch: #24   | Batch Loss: 0.47032466530799866           | Epoch Loss: 0.480210579931736             
Epoch: #76   | Batch: #25   | Batch Loss: 0.49099019169807434           | Epoch Loss: 0.48064176440238954           
Epoch: #76   | Batch: #26   | Batch Loss: 0.44790738821029663           | Epoch Loss: 0.47938274993346286           
Epoch: #76   | Batch: #27   | Batch Loss: 0.45712172985076904           | Epoch Loss: 0.47855826770817794           
Epoch: #76   | Batch: #28   | Batch Loss: 0.48958998918533325           | Epoch Loss: 0.4789522577609335            
Epoch: #76   | Batch: #29   | Batch Loss: 0.39487114548683167           | Epoch Loss: 0.4760529090618265            
Epoch: #76   | Batch: #30   | Batch Loss: 0.48778486251831055           | Epoch Loss: 0.47644397417704265           
Epoch: #76   | Batch: #31   | Batch Loss: 0.44835856556892395           | Epoch Loss: 0.4755379932542001            
Epoch: #76   | Batch: #32   | Batch Loss: 0.5049551725387573            | Epoch Loss: 0.4764572801068425            
Epoch: #76   | Batch: #33   | Batch Loss: 0.44850313663482666           | Epoch Loss: 0.47561018485011475           
Epoch: #76   | Batch: #34   | Batch Loss: 0.448241263628006             | Epoch Loss: 0.47480521657887625           
Epoch: #76   | Batch: #35   | Batch Loss: 0.5675752758979797            | Epoch Loss: 0.47745578970227925           
Epoch: #76   | Batch: #36   | Batch Loss: 0.5398072004318237            | Epoch Loss: 0.4791877733336555            
Epoch: #76   | Batch: #37   | Batch Loss: 0.49302151799201965           | Epoch Loss: 0.47956165832442205           
Epoch: #76   | Batch: #38   | Batch Loss: 0.43938952684402466           | Epoch Loss: 0.4785044969696748            
Epoch: #76   | Batch: #39   | Batch Loss: 0.5786462426185608            | Epoch Loss: 0.4810722340375949            
Epoch: #76   | Batch: #40   | Batch Loss: 0.5213170647621155            | Epoch Loss: 0.48207835480570793           
Epoch: #76   | Batch: #41   | Batch Loss: 0.5298215746879578            | Epoch Loss: 0.48324282358332377           
Epoch: #76   | Batch: #42   | Batch Loss: 0.4494673013687134            | Epoch Loss: 0.4824386444829759            
Epoch: #76   | Batch: #43   | Batch Loss: 0.5499061346054077            | Epoch Loss: 0.484007655881172             

Classifier Validation Epoch #76
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8590909090909091            
Batch: #2    | Top-1 Accuracy: 0.8727272727272728            
Batch: #3    | Top-1 Accuracy: 0.8651515151515152            
Batch: #4    | Top-1 Accuracy: 0.8681818181818183            
Batch: #5    | Top-1 Accuracy: 0.8681818181818184            
Batch: #6    | Top-1 Accuracy: 0.8583333333333334            
Batch: #7    | Top-1 Accuracy: 0.8512987012987013            
Batch: #8    | Top-1 Accuracy: 0.8494318181818181            
Batch: #9    | Top-1 Accuracy: 0.8505050505050504            
Batch: #10   | Top-1 Accuracy: 0.8509090909090908            
Batch: #11   | Top-1 Accuracy: 0.8487603305785124            
Batch: #12   | Top-1 Accuracy: 0.8477272727272727            
Batch: #13   | Top-1 Accuracy: 0.8486013986013986            
Batch: #14   | Top-1 Accuracy: 0.8448051948051948            
Batch: #15   | Top-1 Accuracy: 0.8484848484848484            
Batch: #16   | Top-1 Accuracy: 0.8477272727272727            
Batch: #17   | Top-1 Accuracy: 0.8475935828877005            

Classifier Training Epoch #77
------------------------------------------
Epoch: #77   | Batch: #1    | Batch Loss: 0.3985781967639923            | Epoch Loss: 0.3985781967639923            
Epoch: #77   | Batch: #2    | Batch Loss: 0.4471702575683594            | Epoch Loss: 0.42287422716617584           
Epoch: #77   | Batch: #3    | Batch Loss: 0.47220268845558167           | Epoch Loss: 0.4393170475959778            
Epoch: #77   | Batch: #4    | Batch Loss: 0.4765128195285797            | Epoch Loss: 0.44861599057912827           
Epoch: #77   | Batch: #5    | Batch Loss: 0.5040210485458374            | Epoch Loss: 0.4596970021724701            
Epoch: #77   | Batch: #6    | Batch Loss: 0.38297590613365173           | Epoch Loss: 0.44691015283266705           
Epoch: #77   | Batch: #7    | Batch Loss: 0.5058938264846802            | Epoch Loss: 0.45533639192581177           
Epoch: #77   | Batch: #8    | Batch Loss: 0.4998774230480194            | Epoch Loss: 0.4609040208160877            
Epoch: #77   | Batch: #9    | Batch Loss: 0.433663934469223             | Epoch Loss: 0.45787734455532497           
Epoch: #77   | Batch: #10   | Batch Loss: 0.4022093117237091            | Epoch Loss: 0.4523105412721634            
Epoch: #77   | Batch: #11   | Batch Loss: 0.5020806193351746            | Epoch Loss: 0.45683509382334625           
Epoch: #77   | Batch: #12   | Batch Loss: 0.41601434350013733           | Epoch Loss: 0.4534333646297455            
Epoch: #77   | Batch: #13   | Batch Loss: 0.46060311794281006           | Epoch Loss: 0.45398488411536586           
Epoch: #77   | Batch: #14   | Batch Loss: 0.3698202669620514            | Epoch Loss: 0.44797312574727194           
Epoch: #77   | Batch: #15   | Batch Loss: 0.5436897873878479            | Epoch Loss: 0.45435423652331036           
Epoch: #77   | Batch: #16   | Batch Loss: 0.593894362449646             | Epoch Loss: 0.4630754943937063            
Epoch: #77   | Batch: #17   | Batch Loss: 0.4681251049041748            | Epoch Loss: 0.4633725303060868            
Epoch: #77   | Batch: #18   | Batch Loss: 0.48698997497558594           | Epoch Loss: 0.4646846105655034            
Epoch: #77   | Batch: #19   | Batch Loss: 0.445322185754776             | Epoch Loss: 0.46366553557546514           
Epoch: #77   | Batch: #20   | Batch Loss: 0.5011497735977173            | Epoch Loss: 0.4655397474765778            
Epoch: #77   | Batch: #21   | Batch Loss: 0.43354347348213196           | Epoch Loss: 0.46401611538160414           
Epoch: #77   | Batch: #22   | Batch Loss: 0.5696268081665039            | Epoch Loss: 0.46881660141728143           
Epoch: #77   | Batch: #23   | Batch Loss: 0.46850505471229553           | Epoch Loss: 0.46880305590836896           
Epoch: #77   | Batch: #24   | Batch Loss: 0.4948863983154297            | Epoch Loss: 0.4698898618419965            
Epoch: #77   | Batch: #25   | Batch Loss: 0.3907037079334259            | Epoch Loss: 0.4667224156856537            
Epoch: #77   | Batch: #26   | Batch Loss: 0.453847736120224             | Epoch Loss: 0.4662272357023679            
Epoch: #77   | Batch: #27   | Batch Loss: 0.4644164741039276            | Epoch Loss: 0.46616017045798125           
Epoch: #77   | Batch: #28   | Batch Loss: 0.5190570950508118            | Epoch Loss: 0.46804934633629663           
Epoch: #77   | Batch: #29   | Batch Loss: 0.48953983187675476           | Epoch Loss: 0.46879039756182966           
Epoch: #77   | Batch: #30   | Batch Loss: 0.5226784944534302            | Epoch Loss: 0.47058666745821637           
Epoch: #77   | Batch: #31   | Batch Loss: 0.42473575472831726           | Epoch Loss: 0.46910760575725186           
Epoch: #77   | Batch: #32   | Batch Loss: 0.5146301984786987            | Epoch Loss: 0.4705301867797971            
Epoch: #77   | Batch: #33   | Batch Loss: 0.5535086393356323            | Epoch Loss: 0.4730446853420951            
Epoch: #77   | Batch: #34   | Batch Loss: 0.5323749780654907            | Epoch Loss: 0.47478969395160675           
Epoch: #77   | Batch: #35   | Batch Loss: 0.6284167766571045            | Epoch Loss: 0.47917903917176385           
Epoch: #77   | Batch: #36   | Batch Loss: 0.3662180006504059            | Epoch Loss: 0.47604123254617053           
Epoch: #77   | Batch: #37   | Batch Loss: 0.4632717967033386            | Epoch Loss: 0.47569611265852646           
Epoch: #77   | Batch: #38   | Batch Loss: 0.49744078516960144           | Epoch Loss: 0.4762683408825021            
Epoch: #77   | Batch: #39   | Batch Loss: 0.5368267893791199            | Epoch Loss: 0.47782112161318463           
Epoch: #77   | Batch: #40   | Batch Loss: 0.5952588319778442            | Epoch Loss: 0.4807570643723011            
Epoch: #77   | Batch: #41   | Batch Loss: 0.5750414133071899            | Epoch Loss: 0.4830566826390057            
Epoch: #77   | Batch: #42   | Batch Loss: 0.40056589245796204           | Epoch Loss: 0.4810926162061237            
Epoch: #77   | Batch: #43   | Batch Loss: 0.4492885172367096            | Epoch Loss: 0.4803529859975327            

Classifier Validation Epoch #77
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8363636363636363            
Batch: #2    | Top-1 Accuracy: 0.8318181818181818            
Batch: #3    | Top-1 Accuracy: 0.8378787878787879            
Batch: #4    | Top-1 Accuracy: 0.8409090909090909            
Batch: #5    | Top-1 Accuracy: 0.8445454545454545            
Batch: #6    | Top-1 Accuracy: 0.8446969696969697            
Batch: #7    | Top-1 Accuracy: 0.8422077922077922            
Batch: #8    | Top-1 Accuracy: 0.8448863636363636            
Batch: #9    | Top-1 Accuracy: 0.8449494949494949            
Batch: #10   | Top-1 Accuracy: 0.849090909090909             
Batch: #11   | Top-1 Accuracy: 0.8487603305785124            
Batch: #12   | Top-1 Accuracy: 0.8477272727272727            
Batch: #13   | Top-1 Accuracy: 0.8433566433566433            
Batch: #14   | Top-1 Accuracy: 0.8444805194805195            
Batch: #15   | Top-1 Accuracy: 0.8436363636363636            
Batch: #16   | Top-1 Accuracy: 0.8446022727272727            
Batch: #17   | Top-1 Accuracy: 0.8462566844919787            

Classifier Training Epoch #78
------------------------------------------
Epoch: #78   | Batch: #1    | Batch Loss: 0.45859476923942566           | Epoch Loss: 0.45859476923942566           
Epoch: #78   | Batch: #2    | Batch Loss: 0.3735722005367279            | Epoch Loss: 0.4160834848880768            
Epoch: #78   | Batch: #3    | Batch Loss: 0.5172041058540344            | Epoch Loss: 0.449790358543396             
Epoch: #78   | Batch: #4    | Batch Loss: 0.41967645287513733           | Epoch Loss: 0.44226188212633133           
Epoch: #78   | Batch: #5    | Batch Loss: 0.431774765253067             | Epoch Loss: 0.4401644587516785            
Epoch: #78   | Batch: #6    | Batch Loss: 0.4098251760005951            | Epoch Loss: 0.4351079116264979            
Epoch: #78   | Batch: #7    | Batch Loss: 0.44656121730804443           | Epoch Loss: 0.43674409815243315           
Epoch: #78   | Batch: #8    | Batch Loss: 0.37092676758766174           | Epoch Loss: 0.4285169318318367            
Epoch: #78   | Batch: #9    | Batch Loss: 0.4637792110443115            | Epoch Loss: 0.432434962855445             
Epoch: #78   | Batch: #10   | Batch Loss: 0.5619744062423706            | Epoch Loss: 0.4453889071941376            
Epoch: #78   | Batch: #11   | Batch Loss: 0.5334126353263855            | Epoch Loss: 0.4533910642970692            
Epoch: #78   | Batch: #12   | Batch Loss: 0.5470371842384338            | Epoch Loss: 0.46119490762551624           
Epoch: #78   | Batch: #13   | Batch Loss: 0.47214531898498535           | Epoch Loss: 0.46203724696086              
Epoch: #78   | Batch: #14   | Batch Loss: 0.42113468050956726           | Epoch Loss: 0.45911563507148195           
Epoch: #78   | Batch: #15   | Batch Loss: 0.47094103693962097           | Epoch Loss: 0.4599039951960246            
Epoch: #78   | Batch: #16   | Batch Loss: 0.5406618118286133            | Epoch Loss: 0.46495135873556137           
Epoch: #78   | Batch: #17   | Batch Loss: 0.5448174476623535            | Epoch Loss: 0.46964936396654916           
Epoch: #78   | Batch: #18   | Batch Loss: 0.3901698589324951            | Epoch Loss: 0.4652338359091017            
Epoch: #78   | Batch: #19   | Batch Loss: 0.4310525059700012            | Epoch Loss: 0.4634348185438859            
Epoch: #78   | Batch: #20   | Batch Loss: 0.4303976893424988            | Epoch Loss: 0.46178296208381653           
Epoch: #78   | Batch: #21   | Batch Loss: 0.48576828837394714           | Epoch Loss: 0.46292512047858464           
Epoch: #78   | Batch: #22   | Batch Loss: 0.43260568380355835           | Epoch Loss: 0.46154696426608344           
Epoch: #78   | Batch: #23   | Batch Loss: 0.5261156558990479            | Epoch Loss: 0.464354298684908             
Epoch: #78   | Batch: #24   | Batch Loss: 0.5678977370262146            | Epoch Loss: 0.4686686086157958            
Epoch: #78   | Batch: #25   | Batch Loss: 0.513900637626648             | Epoch Loss: 0.47047788977622984           
Epoch: #78   | Batch: #26   | Batch Loss: 0.4524822235107422            | Epoch Loss: 0.4697857487660188            
Epoch: #78   | Batch: #27   | Batch Loss: 0.5414075255393982            | Epoch Loss: 0.4724384071650328            
Epoch: #78   | Batch: #28   | Batch Loss: 0.41005387902259827           | Epoch Loss: 0.47021038830280304           
Epoch: #78   | Batch: #29   | Batch Loss: 0.4820956885814667            | Epoch Loss: 0.4706202262434466            
Epoch: #78   | Batch: #30   | Batch Loss: 0.46031513810157776           | Epoch Loss: 0.47027672330538434           
Epoch: #78   | Batch: #31   | Batch Loss: 0.5558683276176453            | Epoch Loss: 0.4730377427993282            
Epoch: #78   | Batch: #32   | Batch Loss: 0.4831370413303375            | Epoch Loss: 0.47335334587842226           
Epoch: #78   | Batch: #33   | Batch Loss: 0.4374662935733795            | Epoch Loss: 0.4722658594449361            
Epoch: #78   | Batch: #34   | Batch Loss: 0.6120147109031677            | Epoch Loss: 0.47637611978194294           
Epoch: #78   | Batch: #35   | Batch Loss: 0.49649670720100403           | Epoch Loss: 0.4769509937082018            
Epoch: #78   | Batch: #36   | Batch Loss: 0.43509721755981445           | Epoch Loss: 0.47578838881519103           
Epoch: #78   | Batch: #37   | Batch Loss: 0.4092482626438141            | Epoch Loss: 0.47399000702677546           
Epoch: #78   | Batch: #38   | Batch Loss: 0.42206400632858276           | Epoch Loss: 0.47262353332419144           
Epoch: #78   | Batch: #39   | Batch Loss: 0.4549119472503662            | Epoch Loss: 0.47216939009152925           
Epoch: #78   | Batch: #40   | Batch Loss: 0.4735843539237976            | Epoch Loss: 0.472204764187336             
Epoch: #78   | Batch: #41   | Batch Loss: 0.5607448220252991            | Epoch Loss: 0.47436427779313994           
Epoch: #78   | Batch: #42   | Batch Loss: 0.4769928753376007            | Epoch Loss: 0.47442686344896046           
Epoch: #78   | Batch: #43   | Batch Loss: 0.5226125121116638            | Epoch Loss: 0.47554745992948844           

Classifier Validation Epoch #78
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8272727272727273            
Batch: #2    | Top-1 Accuracy: 0.8431818181818183            
Batch: #3    | Top-1 Accuracy: 0.8378787878787879            
Batch: #4    | Top-1 Accuracy: 0.8443181818181819            
Batch: #5    | Top-1 Accuracy: 0.8472727272727273            
Batch: #6    | Top-1 Accuracy: 0.8507575757575757            
Batch: #7    | Top-1 Accuracy: 0.8538961038961039            
Batch: #8    | Top-1 Accuracy: 0.852840909090909             
Batch: #9    | Top-1 Accuracy: 0.8515151515151514            
Batch: #10   | Top-1 Accuracy: 0.849090909090909             
Batch: #11   | Top-1 Accuracy: 0.85                          
Batch: #12   | Top-1 Accuracy: 0.8507575757575757            
Batch: #13   | Top-1 Accuracy: 0.8500000000000001            
Batch: #14   | Top-1 Accuracy: 0.8483766233766235            
Batch: #15   | Top-1 Accuracy: 0.8496969696969697            
Batch: #16   | Top-1 Accuracy: 0.850284090909091             
Batch: #17   | Top-1 Accuracy: 0.8489304812834225            

Classifier Training Epoch #79
------------------------------------------
Epoch: #79   | Batch: #1    | Batch Loss: 0.4090121388435364            | Epoch Loss: 0.4090121388435364            
Epoch: #79   | Batch: #2    | Batch Loss: 0.5128256678581238            | Epoch Loss: 0.4609189033508301            
Epoch: #79   | Batch: #3    | Batch Loss: 0.4499659836292267            | Epoch Loss: 0.4572679301102956            
Epoch: #79   | Batch: #4    | Batch Loss: 0.4046732783317566            | Epoch Loss: 0.44411926716566086           
Epoch: #79   | Batch: #5    | Batch Loss: 0.6250102519989014            | Epoch Loss: 0.48029746413230895           
Epoch: #79   | Batch: #6    | Batch Loss: 0.5511136054992676            | Epoch Loss: 0.4921001543601354            
Epoch: #79   | Batch: #7    | Batch Loss: 0.5210496783256531            | Epoch Loss: 0.49623580064092365           
Epoch: #79   | Batch: #8    | Batch Loss: 0.4707409739494324            | Epoch Loss: 0.49304894730448723           
Epoch: #79   | Batch: #9    | Batch Loss: 0.4497084617614746            | Epoch Loss: 0.48823333779970807           
Epoch: #79   | Batch: #10   | Batch Loss: 0.4061793386936188            | Epoch Loss: 0.48002793788909914           
Epoch: #79   | Batch: #11   | Batch Loss: 0.5596542358398438            | Epoch Loss: 0.48726669224825775           
Epoch: #79   | Batch: #12   | Batch Loss: 0.4943426549434662            | Epoch Loss: 0.48785635580619174           
Epoch: #79   | Batch: #13   | Batch Loss: 0.6052971482276917            | Epoch Loss: 0.4968902629155379            
Epoch: #79   | Batch: #14   | Batch Loss: 0.45997941493988037           | Epoch Loss: 0.49425377377441954           
Epoch: #79   | Batch: #15   | Batch Loss: 0.4703631103038788            | Epoch Loss: 0.49266106287638345           
Epoch: #79   | Batch: #16   | Batch Loss: 0.36639082431793213           | Epoch Loss: 0.48476917296648026           
Epoch: #79   | Batch: #17   | Batch Loss: 0.4063386023044586            | Epoch Loss: 0.4801556099863613            
Epoch: #79   | Batch: #18   | Batch Loss: 0.4795761704444885            | Epoch Loss: 0.48012341890070176           
Epoch: #79   | Batch: #19   | Batch Loss: 0.5015996098518372            | Epoch Loss: 0.4812537447402352            
Epoch: #79   | Batch: #20   | Batch Loss: 0.46433180570602417           | Epoch Loss: 0.4804076477885246            
Epoch: #79   | Batch: #21   | Batch Loss: 0.45348885655403137           | Epoch Loss: 0.4791258005868821            
Epoch: #79   | Batch: #22   | Batch Loss: 0.45281100273132324           | Epoch Loss: 0.4779296734116294            
Epoch: #79   | Batch: #23   | Batch Loss: 0.4843515157699585            | Epoch Loss: 0.47820888394894806           
Epoch: #79   | Batch: #24   | Batch Loss: 0.47507983446121216           | Epoch Loss: 0.4780785068869591            
Epoch: #79   | Batch: #25   | Batch Loss: 0.44115450978279114           | Epoch Loss: 0.47660154700279234           
Epoch: #79   | Batch: #26   | Batch Loss: 0.5308906435966492            | Epoch Loss: 0.4786895891794792            
Epoch: #79   | Batch: #27   | Batch Loss: 0.5589879751205444            | Epoch Loss: 0.48166360347359266           
Epoch: #79   | Batch: #28   | Batch Loss: 0.5089079141616821            | Epoch Loss: 0.4826366145695959            
Epoch: #79   | Batch: #29   | Batch Loss: 0.493185430765152             | Epoch Loss: 0.48300036685220127           
Epoch: #79   | Batch: #30   | Batch Loss: 0.5479882955551147            | Epoch Loss: 0.48516663114229835           
Epoch: #79   | Batch: #31   | Batch Loss: 0.5427741408348083            | Epoch Loss: 0.4870249379065729            
Epoch: #79   | Batch: #32   | Batch Loss: 0.5426252484321594            | Epoch Loss: 0.4887624476104975            
Epoch: #79   | Batch: #33   | Batch Loss: 0.41982677578926086           | Epoch Loss: 0.4866734878583388            
Epoch: #79   | Batch: #34   | Batch Loss: 0.5376973152160645            | Epoch Loss: 0.4881741886629778            
Epoch: #79   | Batch: #35   | Batch Loss: 0.5248498320579529            | Epoch Loss: 0.4892220641885485            
Epoch: #79   | Batch: #36   | Batch Loss: 0.514735996723175             | Epoch Loss: 0.48993078453673256           
Epoch: #79   | Batch: #37   | Batch Loss: 0.41855964064598083           | Epoch Loss: 0.4880018347018474            
Epoch: #79   | Batch: #38   | Batch Loss: 0.49030861258506775           | Epoch Loss: 0.4880625393829848            
Epoch: #79   | Batch: #39   | Batch Loss: 0.42491239309310913           | Epoch Loss: 0.4864433048627315            
Epoch: #79   | Batch: #40   | Batch Loss: 0.48852553963661194           | Epoch Loss: 0.48649536073207855           
Epoch: #79   | Batch: #41   | Batch Loss: 0.4070301949977875            | Epoch Loss: 0.48455718595807146           
Epoch: #79   | Batch: #42   | Batch Loss: 0.5113021731376648            | Epoch Loss: 0.4851939713671094            
Epoch: #79   | Batch: #43   | Batch Loss: 0.4695591330528259            | Epoch Loss: 0.4848303704760795            

Classifier Validation Epoch #79
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8681818181818182            
Batch: #2    | Top-1 Accuracy: 0.8522727272727273            
Batch: #3    | Top-1 Accuracy: 0.8545454545454545            
Batch: #4    | Top-1 Accuracy: 0.8545454545454545            
Batch: #5    | Top-1 Accuracy: 0.8563636363636362            
Batch: #6    | Top-1 Accuracy: 0.8530303030303029            
Batch: #7    | Top-1 Accuracy: 0.8467532467532466            
Batch: #8    | Top-1 Accuracy: 0.8471590909090909            
Batch: #9    | Top-1 Accuracy: 0.848989898989899             
Batch: #10   | Top-1 Accuracy: 0.85                          
Batch: #11   | Top-1 Accuracy: 0.8512396694214875            
Batch: #12   | Top-1 Accuracy: 0.8492424242424242            
Batch: #13   | Top-1 Accuracy: 0.8486013986013987            
Batch: #14   | Top-1 Accuracy: 0.849025974025974             
Batch: #15   | Top-1 Accuracy: 0.8506060606060607            
Batch: #16   | Top-1 Accuracy: 0.8505681818181818            
Batch: #17   | Top-1 Accuracy: 0.8521390374331551            

Classifier Training Epoch #80
------------------------------------------
Epoch: #80   | Batch: #1    | Batch Loss: 0.39983394742012024           | Epoch Loss: 0.39983394742012024           
Epoch: #80   | Batch: #2    | Batch Loss: 0.5352609157562256            | Epoch Loss: 0.4675474315881729            
Epoch: #80   | Batch: #3    | Batch Loss: 0.5078790187835693            | Epoch Loss: 0.48099129398663837           
Epoch: #80   | Batch: #4    | Batch Loss: 0.3769868314266205            | Epoch Loss: 0.4549901783466339            
Epoch: #80   | Batch: #5    | Batch Loss: 0.46945807337760925           | Epoch Loss: 0.457883757352829             
Epoch: #80   | Batch: #6    | Batch Loss: 0.5736458897590637            | Epoch Loss: 0.4771774460872014            
Epoch: #80   | Batch: #7    | Batch Loss: 0.46847018599510193           | Epoch Loss: 0.4759335517883301            
Epoch: #80   | Batch: #8    | Batch Loss: 0.4894660711288452            | Epoch Loss: 0.47762511670589447           
Epoch: #80   | Batch: #9    | Batch Loss: 0.4730078876018524            | Epoch Loss: 0.4771120912498898            
Epoch: #80   | Batch: #10   | Batch Loss: 0.5101801156997681            | Epoch Loss: 0.4804188936948776            
Epoch: #80   | Batch: #11   | Batch Loss: 0.48597466945648193           | Epoch Loss: 0.48092396421865985           
Epoch: #80   | Batch: #12   | Batch Loss: 0.4698777496814728            | Epoch Loss: 0.4800034463405609            
Epoch: #80   | Batch: #13   | Batch Loss: 0.49767005443573              | Epoch Loss: 0.48136241619403547           
Epoch: #80   | Batch: #14   | Batch Loss: 0.45311617851257324           | Epoch Loss: 0.47934482778821674           
Epoch: #80   | Batch: #15   | Batch Loss: 0.3716888129711151            | Epoch Loss: 0.4721677601337433            
Epoch: #80   | Batch: #16   | Batch Loss: 0.5218994617462158            | Epoch Loss: 0.4752759914845228            
Epoch: #80   | Batch: #17   | Batch Loss: 0.5477738976478577            | Epoch Loss: 0.4795405742000131            
Epoch: #80   | Batch: #18   | Batch Loss: 0.4325380027294159            | Epoch Loss: 0.47692932022942436           
Epoch: #80   | Batch: #19   | Batch Loss: 0.44179368019104004           | Epoch Loss: 0.47508007601687785           
Epoch: #80   | Batch: #20   | Batch Loss: 0.43572133779525757           | Epoch Loss: 0.4731121391057968            
Epoch: #80   | Batch: #21   | Batch Loss: 0.5554510354995728            | Epoch Loss: 0.47703303893407184           
Epoch: #80   | Batch: #22   | Batch Loss: 0.4226418137550354            | Epoch Loss: 0.47456071051684295           
Epoch: #80   | Batch: #23   | Batch Loss: 0.3918548822402954            | Epoch Loss: 0.47096480493960174           
Epoch: #80   | Batch: #24   | Batch Loss: 0.432858407497406             | Epoch Loss: 0.4693770383795102            
Epoch: #80   | Batch: #25   | Batch Loss: 0.4727107882499695            | Epoch Loss: 0.4695103883743286            
Epoch: #80   | Batch: #26   | Batch Loss: 0.47860655188560486           | Epoch Loss: 0.46986024081707              
Epoch: #80   | Batch: #27   | Batch Loss: 0.4333207309246063            | Epoch Loss: 0.46850692563586765           
Epoch: #80   | Batch: #28   | Batch Loss: 0.4536830484867096            | Epoch Loss: 0.46797750145196915           
Epoch: #80   | Batch: #29   | Batch Loss: 0.47961482405662537           | Epoch Loss: 0.4683787884383366            
Epoch: #80   | Batch: #30   | Batch Loss: 0.4728299081325531            | Epoch Loss: 0.46852715909481046           
Epoch: #80   | Batch: #31   | Batch Loss: 0.4751257002353668            | Epoch Loss: 0.46874001526063486           
Epoch: #80   | Batch: #32   | Batch Loss: 0.4355902373790741            | Epoch Loss: 0.4677040847018361            
Epoch: #80   | Batch: #33   | Batch Loss: 0.39167091250419617           | Epoch Loss: 0.4654000491806955            
Epoch: #80   | Batch: #34   | Batch Loss: 0.6095944046974182            | Epoch Loss: 0.4696410596370697            
Epoch: #80   | Batch: #35   | Batch Loss: 0.5834388136863708            | Epoch Loss: 0.4728924240384783            
Epoch: #80   | Batch: #36   | Batch Loss: 0.37051257491111755           | Epoch Loss: 0.47004853934049606           
Epoch: #80   | Batch: #37   | Batch Loss: 0.5492166876792908            | Epoch Loss: 0.47218821902532837           
Epoch: #80   | Batch: #38   | Batch Loss: 0.6053692102432251            | Epoch Loss: 0.4756929819521151            
Epoch: #80   | Batch: #39   | Batch Loss: 0.5023056268692017            | Epoch Loss: 0.4763753574628096            
Epoch: #80   | Batch: #40   | Batch Loss: 0.4098256230354309            | Epoch Loss: 0.47471161410212515           
Epoch: #80   | Batch: #41   | Batch Loss: 0.5924357175827026            | Epoch Loss: 0.4775829336992124            
Epoch: #80   | Batch: #42   | Batch Loss: 0.536056637763977             | Epoch Loss: 0.4789751647483735            
Epoch: #80   | Batch: #43   | Batch Loss: 0.4849245548248291            | Epoch Loss: 0.47911352265712825           

Classifier Validation Epoch #80
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8590909090909091            
Batch: #2    | Top-1 Accuracy: 0.8386363636363636            
Batch: #3    | Top-1 Accuracy: 0.8393939393939394            
Batch: #4    | Top-1 Accuracy: 0.825                         
Batch: #5    | Top-1 Accuracy: 0.8272727272727272            
Batch: #6    | Top-1 Accuracy: 0.8318181818181817            
Batch: #7    | Top-1 Accuracy: 0.8272727272727272            
Batch: #8    | Top-1 Accuracy: 0.8340909090909091            
Batch: #9    | Top-1 Accuracy: 0.8343434343434343            
Batch: #10   | Top-1 Accuracy: 0.835                         
Batch: #11   | Top-1 Accuracy: 0.8347107438016529            
Batch: #12   | Top-1 Accuracy: 0.8397727272727273            
Batch: #13   | Top-1 Accuracy: 0.8419580419580419            
Batch: #14   | Top-1 Accuracy: 0.8422077922077922            
Batch: #15   | Top-1 Accuracy: 0.8433333333333334            
Batch: #16   | Top-1 Accuracy: 0.8428977272727273            
Batch: #17   | Top-1 Accuracy: 0.8409090909090908            

Classifier Training Epoch #81
------------------------------------------
Epoch: #81   | Batch: #1    | Batch Loss: 0.4482720196247101            | Epoch Loss: 0.4482720196247101            
Epoch: #81   | Batch: #2    | Batch Loss: 0.49247556924819946           | Epoch Loss: 0.4703737944364548            
Epoch: #81   | Batch: #3    | Batch Loss: 0.501367449760437             | Epoch Loss: 0.48070501287778217           
Epoch: #81   | Batch: #4    | Batch Loss: 0.43433016538619995           | Epoch Loss: 0.4691113010048866            
Epoch: #81   | Batch: #5    | Batch Loss: 0.5434390902519226            | Epoch Loss: 0.4839768588542938            
Epoch: #81   | Batch: #6    | Batch Loss: 0.38400062918663025           | Epoch Loss: 0.4673141539096832            
Epoch: #81   | Batch: #7    | Batch Loss: 0.4587112367153168            | Epoch Loss: 0.46608516573905945           
Epoch: #81   | Batch: #8    | Batch Loss: 0.47272568941116333           | Epoch Loss: 0.46691523119807243           
Epoch: #81   | Batch: #9    | Batch Loss: 0.4922141134738922            | Epoch Loss: 0.46972621811760795           
Epoch: #81   | Batch: #10   | Batch Loss: 0.454002320766449             | Epoch Loss: 0.4681538283824921            
Epoch: #81   | Batch: #11   | Batch Loss: 0.5633924007415771            | Epoch Loss: 0.47681188041513617           
Epoch: #81   | Batch: #12   | Batch Loss: 0.4276105761528015            | Epoch Loss: 0.4727117717266083            
Epoch: #81   | Batch: #13   | Batch Loss: 0.5163670778274536            | Epoch Loss: 0.4760698721959041            
Epoch: #81   | Batch: #14   | Batch Loss: 0.4947200417518616            | Epoch Loss: 0.4774020271641867            
Epoch: #81   | Batch: #15   | Batch Loss: 0.33306679129600525           | Epoch Loss: 0.467779678106308             
Epoch: #81   | Batch: #16   | Batch Loss: 0.5751926898956299            | Epoch Loss: 0.4744929913431406            
Epoch: #81   | Batch: #17   | Batch Loss: 0.673330545425415             | Epoch Loss: 0.4861893180538626            
Epoch: #81   | Batch: #18   | Batch Loss: 0.4339769184589386            | Epoch Loss: 0.48328862918747795           
Epoch: #81   | Batch: #19   | Batch Loss: 0.4218025803565979            | Epoch Loss: 0.48005252135427373           
Epoch: #81   | Batch: #20   | Batch Loss: 0.5778952240943909            | Epoch Loss: 0.4849446564912796            
Epoch: #81   | Batch: #21   | Batch Loss: 0.46438878774642944           | Epoch Loss: 0.4839658055986677            
Epoch: #81   | Batch: #22   | Batch Loss: 0.4898228645324707            | Epoch Loss: 0.48423203555020417           
Epoch: #81   | Batch: #23   | Batch Loss: 0.40726199746131897           | Epoch Loss: 0.4808855121550353            
Epoch: #81   | Batch: #24   | Batch Loss: 0.4741852581501007            | Epoch Loss: 0.4806063349048297            
Epoch: #81   | Batch: #25   | Batch Loss: 0.4512316882610321            | Epoch Loss: 0.47943134903907775           
Epoch: #81   | Batch: #26   | Batch Loss: 0.4542206823825836            | Epoch Loss: 0.478461708013828             
Epoch: #81   | Batch: #27   | Batch Loss: 0.5207192897796631            | Epoch Loss: 0.48002680363478484           
Epoch: #81   | Batch: #28   | Batch Loss: 0.4416506290435791            | Epoch Loss: 0.4786562259708132            
Epoch: #81   | Batch: #29   | Batch Loss: 0.4754892587661743            | Epoch Loss: 0.478547020205136             
Epoch: #81   | Batch: #30   | Batch Loss: 0.5286824107170105            | Epoch Loss: 0.48021819988886516           
Epoch: #81   | Batch: #31   | Batch Loss: 0.43413031101226807           | Epoch Loss: 0.4787314937960717            
Epoch: #81   | Batch: #32   | Batch Loss: 0.4479544460773468            | Epoch Loss: 0.47776971105486155           
Epoch: #81   | Batch: #33   | Batch Loss: 0.43321797251701355           | Epoch Loss: 0.4764196583718965            
Epoch: #81   | Batch: #34   | Batch Loss: 0.53194659948349              | Epoch Loss: 0.478052803698708             
Epoch: #81   | Batch: #35   | Batch Loss: 0.4283306896686554            | Epoch Loss: 0.476632171869278             
Epoch: #81   | Batch: #36   | Batch Loss: 0.4544762372970581            | Epoch Loss: 0.47601672924227184           
Epoch: #81   | Batch: #37   | Batch Loss: 0.5463622808456421            | Epoch Loss: 0.47791796036668727           
Epoch: #81   | Batch: #38   | Batch Loss: 0.4639114737510681            | Epoch Loss: 0.47754936861364466           
Epoch: #81   | Batch: #39   | Batch Loss: 0.41465985774993896           | Epoch Loss: 0.4759368170530368            
Epoch: #81   | Batch: #40   | Batch Loss: 0.4773564040660858            | Epoch Loss: 0.47597230672836305           
Epoch: #81   | Batch: #41   | Batch Loss: 0.4570155143737793            | Epoch Loss: 0.47550994593922685           
Epoch: #81   | Batch: #42   | Batch Loss: 0.46063360571861267           | Epoch Loss: 0.47515574736254557           
Epoch: #81   | Batch: #43   | Batch Loss: 0.41266342997550964           | Epoch Loss: 0.4737024376558703            

Classifier Validation Epoch #81
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8409090909090909            
Batch: #2    | Top-1 Accuracy: 0.8340909090909091            
Batch: #3    | Top-1 Accuracy: 0.8303030303030304            
Batch: #4    | Top-1 Accuracy: 0.8318181818181819            
Batch: #5    | Top-1 Accuracy: 0.8336363636363637            
Batch: #6    | Top-1 Accuracy: 0.837878787878788             
Batch: #7    | Top-1 Accuracy: 0.8324675324675326            
Batch: #8    | Top-1 Accuracy: 0.834659090909091             
Batch: #9    | Top-1 Accuracy: 0.8348484848484848            
Batch: #10   | Top-1 Accuracy: 0.8363636363636363            
Batch: #11   | Top-1 Accuracy: 0.8334710743801653            
Batch: #12   | Top-1 Accuracy: 0.834469696969697             
Batch: #13   | Top-1 Accuracy: 0.8367132867132867            
Batch: #14   | Top-1 Accuracy: 0.8370129870129871            
Batch: #15   | Top-1 Accuracy: 0.8348484848484848            
Batch: #16   | Top-1 Accuracy: 0.8335227272727272            
Batch: #17   | Top-1 Accuracy: 0.8347593582887701            

Classifier Training Epoch #82
------------------------------------------
Epoch: #82   | Batch: #1    | Batch Loss: 0.4676969051361084            | Epoch Loss: 0.4676969051361084            
Epoch: #82   | Batch: #2    | Batch Loss: 0.45501938462257385           | Epoch Loss: 0.4613581448793411            
Epoch: #82   | Batch: #3    | Batch Loss: 0.4689198136329651            | Epoch Loss: 0.46387870113054913           
Epoch: #82   | Batch: #4    | Batch Loss: 0.43520137667655945           | Epoch Loss: 0.4567093700170517            
Epoch: #82   | Batch: #5    | Batch Loss: 0.5124327540397644            | Epoch Loss: 0.46785404682159426           
Epoch: #82   | Batch: #6    | Batch Loss: 0.39154958724975586           | Epoch Loss: 0.4551366368929545            
Epoch: #82   | Batch: #7    | Batch Loss: 0.5156700015068054            | Epoch Loss: 0.46378426040921894           
Epoch: #82   | Batch: #8    | Batch Loss: 0.5720847249031067            | Epoch Loss: 0.4773218184709549            
Epoch: #82   | Batch: #9    | Batch Loss: 0.4430490732192993            | Epoch Loss: 0.4735137356652154            
Epoch: #82   | Batch: #10   | Batch Loss: 0.4886983335018158            | Epoch Loss: 0.4750321954488754            
Epoch: #82   | Batch: #11   | Batch Loss: 0.48236513137817383           | Epoch Loss: 0.47569882598790253           
Epoch: #82   | Batch: #12   | Batch Loss: 0.4782702922821045            | Epoch Loss: 0.4759131148457527            
Epoch: #82   | Batch: #13   | Batch Loss: 0.4044249355792999            | Epoch Loss: 0.47041402413294864           
Epoch: #82   | Batch: #14   | Batch Loss: 0.38477998971939087           | Epoch Loss: 0.4642973073891231            
Epoch: #82   | Batch: #15   | Batch Loss: 0.5619214177131653            | Epoch Loss: 0.4708055814107259            
Epoch: #82   | Batch: #16   | Batch Loss: 0.45042210817337036           | Epoch Loss: 0.4695316143333912            
Epoch: #82   | Batch: #17   | Batch Loss: 0.48765772581100464           | Epoch Loss: 0.4705978561850155            
Epoch: #82   | Batch: #18   | Batch Loss: 0.5492380857467651            | Epoch Loss: 0.47496675782733494           
Epoch: #82   | Batch: #19   | Batch Loss: 0.4733472764492035            | Epoch Loss: 0.474881521965328             
Epoch: #82   | Batch: #20   | Batch Loss: 0.39452439546585083           | Epoch Loss: 0.47086366564035415           
Epoch: #82   | Batch: #21   | Batch Loss: 0.5210953950881958            | Epoch Loss: 0.47325565275691805           
Epoch: #82   | Batch: #22   | Batch Loss: 0.4270307421684265            | Epoch Loss: 0.47115452045744116           
Epoch: #82   | Batch: #23   | Batch Loss: 0.5114666223526001            | Epoch Loss: 0.47290722053983936           
Epoch: #82   | Batch: #24   | Batch Loss: 0.5793050527572632            | Epoch Loss: 0.4773404635488987            
Epoch: #82   | Batch: #25   | Batch Loss: 0.5238991379737854            | Epoch Loss: 0.47920281052589414           
Epoch: #82   | Batch: #26   | Batch Loss: 0.46580445766448975           | Epoch Loss: 0.478687489261994             
Epoch: #82   | Batch: #27   | Batch Loss: 0.5085021257400513            | Epoch Loss: 0.4797917350574776            
Epoch: #82   | Batch: #28   | Batch Loss: 0.4910551607608795            | Epoch Loss: 0.48019400026117054           
Epoch: #82   | Batch: #29   | Batch Loss: 0.5086174011230469            | Epoch Loss: 0.4811741175322697            
Epoch: #82   | Batch: #30   | Batch Loss: 0.4603542983531952            | Epoch Loss: 0.4804801235596339            
Epoch: #82   | Batch: #31   | Batch Loss: 0.4245988428592682            | Epoch Loss: 0.4786775016015576            
Epoch: #82   | Batch: #32   | Batch Loss: 0.48654210567474365           | Epoch Loss: 0.47892327047884464           
Epoch: #82   | Batch: #33   | Batch Loss: 0.5389748215675354            | Epoch Loss: 0.4807430144512292            
Epoch: #82   | Batch: #34   | Batch Loss: 0.5074827075004578            | Epoch Loss: 0.48152947601150065           
Epoch: #82   | Batch: #35   | Batch Loss: 0.5147480964660645            | Epoch Loss: 0.4824785794530596            
Epoch: #82   | Batch: #36   | Batch Loss: 0.4418918192386627            | Epoch Loss: 0.48135116944710415           
Epoch: #82   | Batch: #37   | Batch Loss: 0.485331267118454             | Epoch Loss: 0.4814587396544379            
Epoch: #82   | Batch: #38   | Batch Loss: 0.4848349690437317            | Epoch Loss: 0.48154758779626144           
Epoch: #82   | Batch: #39   | Batch Loss: 0.3793354034423828            | Epoch Loss: 0.4789267625564184            
Epoch: #82   | Batch: #40   | Batch Loss: 0.49975502490997314           | Epoch Loss: 0.47944746911525726           
Epoch: #82   | Batch: #41   | Batch Loss: 0.45804619789123535           | Epoch Loss: 0.47892548689028114           
Epoch: #82   | Batch: #42   | Batch Loss: 0.5883206725120544            | Epoch Loss: 0.48153013416699003           
Epoch: #82   | Batch: #43   | Batch Loss: 0.5059669017791748            | Epoch Loss: 0.4820984310882036            

Classifier Validation Epoch #82
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.85                          
Batch: #2    | Top-1 Accuracy: 0.8545454545454545            
Batch: #3    | Top-1 Accuracy: 0.8515151515151516            
Batch: #4    | Top-1 Accuracy: 0.8488636363636364            
Batch: #5    | Top-1 Accuracy: 0.8454545454545455            
Batch: #6    | Top-1 Accuracy: 0.8446969696969697            
Batch: #7    | Top-1 Accuracy: 0.8409090909090909            
Batch: #8    | Top-1 Accuracy: 0.8414772727272728            
Batch: #9    | Top-1 Accuracy: 0.844949494949495             
Batch: #10   | Top-1 Accuracy: 0.8477272727272729            
Batch: #11   | Top-1 Accuracy: 0.8458677685950415            
Batch: #12   | Top-1 Accuracy: 0.8469696969696972            
Batch: #13   | Top-1 Accuracy: 0.8465034965034968            
Batch: #14   | Top-1 Accuracy: 0.8490259740259744            
Batch: #15   | Top-1 Accuracy: 0.84969696969697              
Batch: #16   | Top-1 Accuracy: 0.8497159090909091            
Batch: #17   | Top-1 Accuracy: 0.8489304812834224            

Classifier Training Epoch #83
------------------------------------------
Epoch: #83   | Batch: #1    | Batch Loss: 0.5191230177879333            | Epoch Loss: 0.5191230177879333            
Epoch: #83   | Batch: #2    | Batch Loss: 0.40963637828826904           | Epoch Loss: 0.4643796980381012            
Epoch: #83   | Batch: #3    | Batch Loss: 0.39756155014038086           | Epoch Loss: 0.4421069820721944            
Epoch: #83   | Batch: #4    | Batch Loss: 0.5893514752388               | Epoch Loss: 0.4789181053638458            
Epoch: #83   | Batch: #5    | Batch Loss: 0.46744516491889954           | Epoch Loss: 0.4766235172748566            
Epoch: #83   | Batch: #6    | Batch Loss: 0.4637294113636017            | Epoch Loss: 0.47447449962298077           
Epoch: #83   | Batch: #7    | Batch Loss: 0.37416329979896545           | Epoch Loss: 0.46014432821955              
Epoch: #83   | Batch: #8    | Batch Loss: 0.4684630036354065            | Epoch Loss: 0.46118416264653206           
Epoch: #83   | Batch: #9    | Batch Loss: 0.495321124792099             | Epoch Loss: 0.46497715844048393           
Epoch: #83   | Batch: #10   | Batch Loss: 0.481528103351593             | Epoch Loss: 0.4666322529315948            
Epoch: #83   | Batch: #11   | Batch Loss: 0.4848506450653076            | Epoch Loss: 0.468288470398296             
Epoch: #83   | Batch: #12   | Batch Loss: 0.48407191038131714           | Epoch Loss: 0.4696037570635478            
Epoch: #83   | Batch: #13   | Batch Loss: 0.4214043617248535            | Epoch Loss: 0.4658961112682636            
Epoch: #83   | Batch: #14   | Batch Loss: 0.445157527923584             | Epoch Loss: 0.46441478388650076           
Epoch: #83   | Batch: #15   | Batch Loss: 0.3350639343261719            | Epoch Loss: 0.4557913939158122            
Epoch: #83   | Batch: #16   | Batch Loss: 0.5246211886405945            | Epoch Loss: 0.46009325608611107           
Epoch: #83   | Batch: #17   | Batch Loss: 0.48590412735939026           | Epoch Loss: 0.4616115426315981            
Epoch: #83   | Batch: #18   | Batch Loss: 0.26658895611763              | Epoch Loss: 0.45077695449193317           
Epoch: #83   | Batch: #19   | Batch Loss: 0.5276238918304443            | Epoch Loss: 0.45482153014132853           
Epoch: #83   | Batch: #20   | Batch Loss: 0.5447583794593811            | Epoch Loss: 0.45931837260723113           
Epoch: #83   | Batch: #21   | Batch Loss: 0.43605536222457886           | Epoch Loss: 0.4582106102080572            
Epoch: #83   | Batch: #22   | Batch Loss: 0.40907523036003113           | Epoch Loss: 0.45597718385132874           
Epoch: #83   | Batch: #23   | Batch Loss: 0.4607357084751129            | Epoch Loss: 0.4561840762262759            
Epoch: #83   | Batch: #24   | Batch Loss: 0.4355688989162445            | Epoch Loss: 0.4553251105050246            
Epoch: #83   | Batch: #25   | Batch Loss: 0.5771740674972534            | Epoch Loss: 0.4601990687847137            
Epoch: #83   | Batch: #26   | Batch Loss: 0.5215398073196411            | Epoch Loss: 0.46255832795913404           
Epoch: #83   | Batch: #27   | Batch Loss: 0.49113374948501587           | Epoch Loss: 0.46361667690453706           
Epoch: #83   | Batch: #28   | Batch Loss: 0.48807233572006226           | Epoch Loss: 0.4644900932908058            
Epoch: #83   | Batch: #29   | Batch Loss: 0.3590419888496399            | Epoch Loss: 0.4608539517583518            
Epoch: #83   | Batch: #30   | Batch Loss: 0.4638349115848541            | Epoch Loss: 0.46095331708590187           
Epoch: #83   | Batch: #31   | Batch Loss: 0.4161859452724457            | Epoch Loss: 0.4595092083177259            
Epoch: #83   | Batch: #32   | Batch Loss: 0.46464383602142334           | Epoch Loss: 0.45966966543346643           
Epoch: #83   | Batch: #33   | Batch Loss: 0.5775231719017029            | Epoch Loss: 0.4632409838112918            
Epoch: #83   | Batch: #34   | Batch Loss: 0.5033602714538574            | Epoch Loss: 0.46442096285960255           
Epoch: #83   | Batch: #35   | Batch Loss: 0.41717374324798584           | Epoch Loss: 0.46307104229927065           
Epoch: #83   | Batch: #36   | Batch Loss: 0.4840237498283386            | Epoch Loss: 0.4636530619528558            
Epoch: #83   | Batch: #37   | Batch Loss: 0.5182468295097351            | Epoch Loss: 0.4651285691841229            
Epoch: #83   | Batch: #38   | Batch Loss: 0.3716309666633606            | Epoch Loss: 0.4626681059598923            
Epoch: #83   | Batch: #39   | Batch Loss: 0.42000478506088257           | Epoch Loss: 0.46157417465478945           
Epoch: #83   | Batch: #40   | Batch Loss: 0.4084354043006897            | Epoch Loss: 0.46024570539593695           
Epoch: #83   | Batch: #41   | Batch Loss: 0.5007171630859375            | Epoch Loss: 0.4612328141200833            
Epoch: #83   | Batch: #42   | Batch Loss: 0.44433799386024475           | Epoch Loss: 0.46083055649484905           
Epoch: #83   | Batch: #43   | Batch Loss: 0.38129472732543945           | Epoch Loss: 0.45898088604904885           

Classifier Validation Epoch #83
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8227272727272728            
Batch: #2    | Top-1 Accuracy: 0.8431818181818183            
Batch: #3    | Top-1 Accuracy: 0.8378787878787879            
Batch: #4    | Top-1 Accuracy: 0.8375                        
Batch: #5    | Top-1 Accuracy: 0.8390909090909091            
Batch: #6    | Top-1 Accuracy: 0.8287878787878787            
Batch: #7    | Top-1 Accuracy: 0.8344155844155844            
Batch: #8    | Top-1 Accuracy: 0.8380681818181819            
Batch: #9    | Top-1 Accuracy: 0.8333333333333334            
Batch: #10   | Top-1 Accuracy: 0.835                         
Batch: #11   | Top-1 Accuracy: 0.8330578512396695            
Batch: #12   | Top-1 Accuracy: 0.8337121212121213            
Batch: #13   | Top-1 Accuracy: 0.8353146853146853            
Batch: #14   | Top-1 Accuracy: 0.8370129870129871            
Batch: #15   | Top-1 Accuracy: 0.8366666666666667            
Batch: #16   | Top-1 Accuracy: 0.837215909090909             
Batch: #17   | Top-1 Accuracy: 0.83475935828877              

Classifier Training Epoch #84
------------------------------------------
Epoch: #84   | Batch: #1    | Batch Loss: 0.5040265321731567            | Epoch Loss: 0.5040265321731567            
Epoch: #84   | Batch: #2    | Batch Loss: 0.4310629069805145            | Epoch Loss: 0.46754471957683563           
Epoch: #84   | Batch: #3    | Batch Loss: 0.44927626848220825           | Epoch Loss: 0.4614552358786265            
Epoch: #84   | Batch: #4    | Batch Loss: 0.5413258671760559            | Epoch Loss: 0.48142289370298386           
Epoch: #84   | Batch: #5    | Batch Loss: 0.554056704044342             | Epoch Loss: 0.4959496557712555            
Epoch: #84   | Batch: #6    | Batch Loss: 0.5773983001708984            | Epoch Loss: 0.5095244298378626            
Epoch: #84   | Batch: #7    | Batch Loss: 0.5504725575447083            | Epoch Loss: 0.5153741623674121            
Epoch: #84   | Batch: #8    | Batch Loss: 0.44678905606269836           | Epoch Loss: 0.5068010240793228            
Epoch: #84   | Batch: #9    | Batch Loss: 0.5241653919219971            | Epoch Loss: 0.5087303982840644            
Epoch: #84   | Batch: #10   | Batch Loss: 0.41121813654899597           | Epoch Loss: 0.49897917211055753           
Epoch: #84   | Batch: #11   | Batch Loss: 0.6146546006202698            | Epoch Loss: 0.509495120156895             
Epoch: #84   | Batch: #12   | Batch Loss: 0.4450705349445343            | Epoch Loss: 0.5041264047225317            
Epoch: #84   | Batch: #13   | Batch Loss: 0.4609341621398926            | Epoch Loss: 0.5008039245238671            
Epoch: #84   | Batch: #14   | Batch Loss: 0.6851911544799805            | Epoch Loss: 0.5139744409493038            
Epoch: #84   | Batch: #15   | Batch Loss: 0.43929392099380493           | Epoch Loss: 0.5089957396189372            
Epoch: #84   | Batch: #16   | Batch Loss: 0.3404795527458191            | Epoch Loss: 0.4984634779393673            
Epoch: #84   | Batch: #17   | Batch Loss: 0.4751953184604645            | Epoch Loss: 0.4970947626759024            
Epoch: #84   | Batch: #18   | Batch Loss: 0.4366755187511444            | Epoch Loss: 0.4937381380134159            
Epoch: #84   | Batch: #19   | Batch Loss: 0.6188617944717407            | Epoch Loss: 0.5003235936164856            
Epoch: #84   | Batch: #20   | Batch Loss: 0.4450269639492035            | Epoch Loss: 0.4975587621331215            
Epoch: #84   | Batch: #21   | Batch Loss: 0.48709169030189514           | Epoch Loss: 0.49706033014115836           
Epoch: #84   | Batch: #22   | Batch Loss: 0.37967824935913086           | Epoch Loss: 0.4917247810147025            
Epoch: #84   | Batch: #23   | Batch Loss: 0.4407464563846588            | Epoch Loss: 0.48950833211774414           
Epoch: #84   | Batch: #24   | Batch Loss: 0.46668824553489685           | Epoch Loss: 0.48855749517679214           
Epoch: #84   | Batch: #25   | Batch Loss: 0.3686332404613495            | Epoch Loss: 0.48376052498817446           
Epoch: #84   | Batch: #26   | Batch Loss: 0.47034409642219543           | Epoch Loss: 0.48324450850486755           
Epoch: #84   | Batch: #27   | Batch Loss: 0.3617880344390869            | Epoch Loss: 0.4787461205765053            
Epoch: #84   | Batch: #28   | Batch Loss: 0.48678475618362427           | Epoch Loss: 0.479033214705331             
Epoch: #84   | Batch: #29   | Batch Loss: 0.46152353286743164           | Epoch Loss: 0.47842943257298964           
Epoch: #84   | Batch: #30   | Batch Loss: 0.3871212303638458            | Epoch Loss: 0.47538582583268485           
Epoch: #84   | Batch: #31   | Batch Loss: 0.3619205057621002            | Epoch Loss: 0.4717256542175047            
Epoch: #84   | Batch: #32   | Batch Loss: 0.5428922176361084            | Epoch Loss: 0.47394960932433605           
Epoch: #84   | Batch: #33   | Batch Loss: 0.37562474608421326           | Epoch Loss: 0.4709700680140293            
Epoch: #84   | Batch: #34   | Batch Loss: 0.391531765460968             | Epoch Loss: 0.46863364735070395           
Epoch: #84   | Batch: #35   | Batch Loss: 0.4155886173248291            | Epoch Loss: 0.4671180750642504            
Epoch: #84   | Batch: #36   | Batch Loss: 0.466116338968277             | Epoch Loss: 0.4670902490615845            
Epoch: #84   | Batch: #37   | Batch Loss: 0.4169260561466217            | Epoch Loss: 0.46573446006388275           
Epoch: #84   | Batch: #38   | Batch Loss: 0.455291748046875             | Epoch Loss: 0.4654596518529089            
Epoch: #84   | Batch: #39   | Batch Loss: 0.5607829689979553            | Epoch Loss: 0.46790383947201264           
Epoch: #84   | Batch: #40   | Batch Loss: 0.5066937208175659            | Epoch Loss: 0.4688735865056515            
Epoch: #84   | Batch: #41   | Batch Loss: 0.4651539623737335            | Epoch Loss: 0.4687828639658486            
Epoch: #84   | Batch: #42   | Batch Loss: 0.4837632179260254            | Epoch Loss: 0.4691395390601385            
Epoch: #84   | Batch: #43   | Batch Loss: 0.47283855080604553           | Epoch Loss: 0.4692255625891131            

Classifier Validation Epoch #84
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8863636363636364            
Batch: #2    | Top-1 Accuracy: 0.8522727272727273            
Batch: #3    | Top-1 Accuracy: 0.853030303030303             
Batch: #4    | Top-1 Accuracy: 0.8545454545454545            
Batch: #5    | Top-1 Accuracy: 0.85                          
Batch: #6    | Top-1 Accuracy: 0.8515151515151516            
Batch: #7    | Top-1 Accuracy: 0.8493506493506493            
Batch: #8    | Top-1 Accuracy: 0.8460227272727272            
Batch: #9    | Top-1 Accuracy: 0.8479797979797978            
Batch: #10   | Top-1 Accuracy: 0.8504545454545454            
Batch: #11   | Top-1 Accuracy: 0.8483471074380166            
Batch: #12   | Top-1 Accuracy: 0.8484848484848485            
Batch: #13   | Top-1 Accuracy: 0.8489510489510489            
Batch: #14   | Top-1 Accuracy: 0.8487012987012986            
Batch: #15   | Top-1 Accuracy: 0.8478787878787878            
Batch: #16   | Top-1 Accuracy: 0.846875                      
Batch: #17   | Top-1 Accuracy: 0.8465240641711231            

Classifier Training Epoch #85
------------------------------------------
Epoch: #85   | Batch: #1    | Batch Loss: 0.5865175127983093            | Epoch Loss: 0.5865175127983093            
Epoch: #85   | Batch: #2    | Batch Loss: 0.3706991374492645            | Epoch Loss: 0.4786083251237869            
Epoch: #85   | Batch: #3    | Batch Loss: 0.4143826961517334            | Epoch Loss: 0.4571997821331024            
Epoch: #85   | Batch: #4    | Batch Loss: 0.40629199147224426           | Epoch Loss: 0.4444728344678879            
Epoch: #85   | Batch: #5    | Batch Loss: 0.43290430307388306           | Epoch Loss: 0.4421591281890869            
Epoch: #85   | Batch: #6    | Batch Loss: 0.48947516083717346           | Epoch Loss: 0.4500451336304347            
Epoch: #85   | Batch: #7    | Batch Loss: 0.6192641258239746            | Epoch Loss: 0.47421927537236896           
Epoch: #85   | Batch: #8    | Batch Loss: 0.4464879035949707            | Epoch Loss: 0.47075285390019417           
Epoch: #85   | Batch: #9    | Batch Loss: 0.45471110939979553           | Epoch Loss: 0.4689704378445943            
Epoch: #85   | Batch: #10   | Batch Loss: 0.3141905665397644            | Epoch Loss: 0.4534924507141113            
Epoch: #85   | Batch: #11   | Batch Loss: 0.46670448780059814           | Epoch Loss: 0.45469354499470105           
Epoch: #85   | Batch: #12   | Batch Loss: 0.3414422869682312            | Epoch Loss: 0.44525594015916187           
Epoch: #85   | Batch: #13   | Batch Loss: 0.4345875382423401            | Epoch Loss: 0.4444352938578679            
Epoch: #85   | Batch: #14   | Batch Loss: 0.5555981397628784            | Epoch Loss: 0.4523754971367972            
Epoch: #85   | Batch: #15   | Batch Loss: 0.4693896174430847            | Epoch Loss: 0.4535097718238831            
Epoch: #85   | Batch: #16   | Batch Loss: 0.47918036580085754           | Epoch Loss: 0.45511418394744396           
Epoch: #85   | Batch: #17   | Batch Loss: 0.43364009261131287           | Epoch Loss: 0.45385100210414214           
Epoch: #85   | Batch: #18   | Batch Loss: 0.4834582209587097            | Epoch Loss: 0.45549584759606254           
Epoch: #85   | Batch: #19   | Batch Loss: 0.4624691903591156            | Epoch Loss: 0.45586286563622325           
Epoch: #85   | Batch: #20   | Batch Loss: 0.42166802287101746           | Epoch Loss: 0.45415312349796294           
Epoch: #85   | Batch: #21   | Batch Loss: 0.41031283140182495           | Epoch Loss: 0.452065490541004             
Epoch: #85   | Batch: #22   | Batch Loss: 0.3873905539512634            | Epoch Loss: 0.4491257206960158            
Epoch: #85   | Batch: #23   | Batch Loss: 0.4319833517074585            | Epoch Loss: 0.4483804003052089            
Epoch: #85   | Batch: #24   | Batch Loss: 0.47561362385749817           | Epoch Loss: 0.449515117953221             
Epoch: #85   | Batch: #25   | Batch Loss: 0.4635990858078003            | Epoch Loss: 0.4500784766674042            
Epoch: #85   | Batch: #26   | Batch Loss: 0.5141499638557434            | Epoch Loss: 0.4525427646361865            
Epoch: #85   | Batch: #27   | Batch Loss: 0.572388768196106             | Epoch Loss: 0.4569815055087761            
Epoch: #85   | Batch: #28   | Batch Loss: 0.4419797956943512            | Epoch Loss: 0.4564457301582609            
Epoch: #85   | Batch: #29   | Batch Loss: 0.4465225338935852            | Epoch Loss: 0.45610355097672034           
Epoch: #85   | Batch: #30   | Batch Loss: 0.5688156485557556            | Epoch Loss: 0.45986062089602153           
Epoch: #85   | Batch: #31   | Batch Loss: 0.5553033351898193            | Epoch Loss: 0.46293941813130535           
Epoch: #85   | Batch: #32   | Batch Loss: 0.5078777074813843            | Epoch Loss: 0.4643437396734953            
Epoch: #85   | Batch: #33   | Batch Loss: 0.44923895597457886           | Epoch Loss: 0.4638860189553463            
Epoch: #85   | Batch: #34   | Batch Loss: 0.5472843647003174            | Epoch Loss: 0.4663389114772572            
Epoch: #85   | Batch: #35   | Batch Loss: 0.5609133839607239            | Epoch Loss: 0.4690410392624991            
Epoch: #85   | Batch: #36   | Batch Loss: 0.3859144151210785            | Epoch Loss: 0.4667319663696819            
Epoch: #85   | Batch: #37   | Batch Loss: 0.46480223536491394           | Epoch Loss: 0.46667981147766113           
Epoch: #85   | Batch: #38   | Batch Loss: 0.45034217834472656           | Epoch Loss: 0.4662498737636365            
Epoch: #85   | Batch: #39   | Batch Loss: 0.41109487414360046           | Epoch Loss: 0.4648356430041484            
Epoch: #85   | Batch: #40   | Batch Loss: 0.4296310544013977            | Epoch Loss: 0.46395552828907966           
Epoch: #85   | Batch: #41   | Batch Loss: 0.5176730751991272            | Epoch Loss: 0.4652657123600564            
Epoch: #85   | Batch: #42   | Batch Loss: 0.3887212574481964            | Epoch Loss: 0.4634432253383455            
Epoch: #85   | Batch: #43   | Batch Loss: 0.39354899525642395           | Epoch Loss: 0.461817778127138             

Classifier Validation Epoch #85
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8681818181818182            
Batch: #2    | Top-1 Accuracy: 0.8613636363636363            
Batch: #3    | Top-1 Accuracy: 0.8636363636363636            
Batch: #4    | Top-1 Accuracy: 0.8625                        
Batch: #5    | Top-1 Accuracy: 0.86                          
Batch: #6    | Top-1 Accuracy: 0.8484848484848485            
Batch: #7    | Top-1 Accuracy: 0.8525974025974026            
Batch: #8    | Top-1 Accuracy: 0.8573863636363637            
Batch: #9    | Top-1 Accuracy: 0.8601010101010101            
Batch: #10   | Top-1 Accuracy: 0.8572727272727272            
Batch: #11   | Top-1 Accuracy: 0.8570247933884296            
Batch: #12   | Top-1 Accuracy: 0.856060606060606             
Batch: #13   | Top-1 Accuracy: 0.8552447552447552            
Batch: #14   | Top-1 Accuracy: 0.8535714285714285            
Batch: #15   | Top-1 Accuracy: 0.850909090909091             
Batch: #16   | Top-1 Accuracy: 0.8522727272727273            
Batch: #17   | Top-1 Accuracy: 0.8510695187165775            

Classifier Training Epoch #86
------------------------------------------
Epoch: #86   | Batch: #1    | Batch Loss: 0.4616764187812805            | Epoch Loss: 0.4616764187812805            
Epoch: #86   | Batch: #2    | Batch Loss: 0.38330650329589844           | Epoch Loss: 0.4224914610385895            
Epoch: #86   | Batch: #3    | Batch Loss: 0.4799957871437073            | Epoch Loss: 0.4416595697402954            
Epoch: #86   | Batch: #4    | Batch Loss: 0.4412470757961273            | Epoch Loss: 0.4415564462542534            
Epoch: #86   | Batch: #5    | Batch Loss: 0.48668551445007324           | Epoch Loss: 0.4505822598934174            
Epoch: #86   | Batch: #6    | Batch Loss: 0.48922353982925415           | Epoch Loss: 0.4570224732160568            
Epoch: #86   | Batch: #7    | Batch Loss: 0.36655399203300476           | Epoch Loss: 0.4440984044756208            
Epoch: #86   | Batch: #8    | Batch Loss: 0.3864734172821045            | Epoch Loss: 0.4368952810764313            
Epoch: #86   | Batch: #9    | Batch Loss: 0.40951308608055115           | Epoch Loss: 0.43385281496577793           
Epoch: #86   | Batch: #10   | Batch Loss: 0.3516984283924103            | Epoch Loss: 0.4256373763084412            
Epoch: #86   | Batch: #11   | Batch Loss: 0.5978686809539795            | Epoch Loss: 0.44129476763985376           
Epoch: #86   | Batch: #12   | Batch Loss: 0.42620420455932617           | Epoch Loss: 0.44003722071647644           
Epoch: #86   | Batch: #13   | Batch Loss: 0.45359480381011963           | Epoch Loss: 0.4410801117236798            
Epoch: #86   | Batch: #14   | Batch Loss: 0.3864558935165405            | Epoch Loss: 0.43717838185174124           
Epoch: #86   | Batch: #15   | Batch Loss: 0.5134533047676086            | Epoch Loss: 0.4422633767127991            
Epoch: #86   | Batch: #16   | Batch Loss: 0.4315710663795471            | Epoch Loss: 0.4415951073169708            
Epoch: #86   | Batch: #17   | Batch Loss: 0.4568960964679718            | Epoch Loss: 0.4424951655023238            
Epoch: #86   | Batch: #18   | Batch Loss: 0.5453725457191467            | Epoch Loss: 0.44821057551436955           
Epoch: #86   | Batch: #19   | Batch Loss: 0.6024168133735657            | Epoch Loss: 0.4563266932964325            
Epoch: #86   | Batch: #20   | Batch Loss: 0.5175538659095764            | Epoch Loss: 0.4593880519270897            
Epoch: #86   | Batch: #21   | Batch Loss: 0.5436943769454956            | Epoch Loss: 0.4634026388327281            
Epoch: #86   | Batch: #22   | Batch Loss: 0.4253282845020294            | Epoch Loss: 0.4616719863631509            
Epoch: #86   | Batch: #23   | Batch Loss: 0.37093499302864075           | Epoch Loss: 0.45772689969643304           
Epoch: #86   | Batch: #24   | Batch Loss: 0.354588121175766             | Epoch Loss: 0.4534294505914052            
Epoch: #86   | Batch: #25   | Batch Loss: 0.48952716588974              | Epoch Loss: 0.4548733592033386            
Epoch: #86   | Batch: #26   | Batch Loss: 0.3380281627178192            | Epoch Loss: 0.4503793131846648            
Epoch: #86   | Batch: #27   | Batch Loss: 0.4676555395126343            | Epoch Loss: 0.451019173419034             
Epoch: #86   | Batch: #28   | Batch Loss: 0.462163507938385             | Epoch Loss: 0.4514171853661537            
Epoch: #86   | Batch: #29   | Batch Loss: 0.4056469202041626            | Epoch Loss: 0.44983890036056784           
Epoch: #86   | Batch: #30   | Batch Loss: 0.45535513758659363           | Epoch Loss: 0.45002277493476867           
Epoch: #86   | Batch: #31   | Batch Loss: 0.4488881528377533            | Epoch Loss: 0.44998617422196174           
Epoch: #86   | Batch: #32   | Batch Loss: 0.34812191128730774           | Epoch Loss: 0.4468029160052538            
Epoch: #86   | Batch: #33   | Batch Loss: 0.496210515499115             | Epoch Loss: 0.44830011598991626           
Epoch: #86   | Batch: #34   | Batch Loss: 0.47909656167030334           | Epoch Loss: 0.4492058938040453            
Epoch: #86   | Batch: #35   | Batch Loss: 0.49404528737068176           | Epoch Loss: 0.4504870193345206            
Epoch: #86   | Batch: #36   | Batch Loss: 0.4094845652580261            | Epoch Loss: 0.4493480622768402            
Epoch: #86   | Batch: #37   | Batch Loss: 0.6177427172660828            | Epoch Loss: 0.45389926916844137           
Epoch: #86   | Batch: #38   | Batch Loss: 0.5348095297813416            | Epoch Loss: 0.4560284865529914            
Epoch: #86   | Batch: #39   | Batch Loss: 0.5809804201126099            | Epoch Loss: 0.4592323822852893            
Epoch: #86   | Batch: #40   | Batch Loss: 0.5497725009918213            | Epoch Loss: 0.4614958852529526            
Epoch: #86   | Batch: #41   | Batch Loss: 0.4195418059825897            | Epoch Loss: 0.46047261502684617           
Epoch: #86   | Batch: #42   | Batch Loss: 0.4326745569705963            | Epoch Loss: 0.45981075650169734           
Epoch: #86   | Batch: #43   | Batch Loss: 0.5084202289581299            | Epoch Loss: 0.4609412093495214            

Classifier Validation Epoch #86
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8136363636363636            
Batch: #2    | Top-1 Accuracy: 0.8295454545454546            
Batch: #3    | Top-1 Accuracy: 0.834848484848485             
Batch: #4    | Top-1 Accuracy: 0.8329545454545455            
Batch: #5    | Top-1 Accuracy: 0.8309090909090908            
Batch: #6    | Top-1 Accuracy: 0.8325757575757575            
Batch: #7    | Top-1 Accuracy: 0.8285714285714285            
Batch: #8    | Top-1 Accuracy: 0.8301136363636363            
Batch: #9    | Top-1 Accuracy: 0.8333333333333334            
Batch: #10   | Top-1 Accuracy: 0.8359090909090909            
Batch: #11   | Top-1 Accuracy: 0.8351239669421489            
Batch: #12   | Top-1 Accuracy: 0.8359848484848486            
Batch: #13   | Top-1 Accuracy: 0.8360139860139861            
Batch: #14   | Top-1 Accuracy: 0.8373376623376624            
Batch: #15   | Top-1 Accuracy: 0.8372727272727273            
Batch: #16   | Top-1 Accuracy: 0.8380681818181819            
Batch: #17   | Top-1 Accuracy: 0.8379679144385027            

Classifier Training Epoch #87
------------------------------------------
Epoch: #87   | Batch: #1    | Batch Loss: 0.4602743983268738            | Epoch Loss: 0.4602743983268738            
Epoch: #87   | Batch: #2    | Batch Loss: 0.5571478605270386            | Epoch Loss: 0.5087111294269562            
Epoch: #87   | Batch: #3    | Batch Loss: 0.4344804584980011            | Epoch Loss: 0.4839675724506378            
Epoch: #87   | Batch: #4    | Batch Loss: 0.5435330271720886            | Epoch Loss: 0.4988589361310005            
Epoch: #87   | Batch: #5    | Batch Loss: 0.4278709888458252            | Epoch Loss: 0.48466134667396543           
Epoch: #87   | Batch: #6    | Batch Loss: 0.415800541639328             | Epoch Loss: 0.4731845458348592            
Epoch: #87   | Batch: #7    | Batch Loss: 0.42625299096107483           | Epoch Loss: 0.46648003799574717           
Epoch: #87   | Batch: #8    | Batch Loss: 0.4558785557746887            | Epoch Loss: 0.46515485271811485           
Epoch: #87   | Batch: #9    | Batch Loss: 0.5009569525718689            | Epoch Loss: 0.4691328638129764            
Epoch: #87   | Batch: #10   | Batch Loss: 0.4252929985523224            | Epoch Loss: 0.464748877286911             
Epoch: #87   | Batch: #11   | Batch Loss: 0.4280620217323303            | Epoch Loss: 0.4614137086001309            
Epoch: #87   | Batch: #12   | Batch Loss: 0.4233042597770691            | Epoch Loss: 0.4582379211982091            
Epoch: #87   | Batch: #13   | Batch Loss: 0.5959637761116028            | Epoch Loss: 0.46883221773000866           
Epoch: #87   | Batch: #14   | Batch Loss: 0.3155624270439148            | Epoch Loss: 0.4578843755381448            
Epoch: #87   | Batch: #15   | Batch Loss: 0.48490625619888306           | Epoch Loss: 0.45968583424886067           
Epoch: #87   | Batch: #16   | Batch Loss: 0.40568381547927856           | Epoch Loss: 0.4563107080757618            
Epoch: #87   | Batch: #17   | Batch Loss: 0.4080263078212738            | Epoch Loss: 0.4534704492372625            
Epoch: #87   | Batch: #18   | Batch Loss: 0.4592861831188202            | Epoch Loss: 0.4537935455640157            
Epoch: #87   | Batch: #19   | Batch Loss: 0.5489296913146973            | Epoch Loss: 0.45880071112984105           
Epoch: #87   | Batch: #20   | Batch Loss: 0.4151655435562134            | Epoch Loss: 0.45661895275115966           
Epoch: #87   | Batch: #21   | Batch Loss: 0.48463842272758484           | Epoch Loss: 0.4579532132262275            
Epoch: #87   | Batch: #22   | Batch Loss: 0.4292265474796295            | Epoch Loss: 0.45664745569229126           
Epoch: #87   | Batch: #23   | Batch Loss: 0.4590545892715454            | Epoch Loss: 0.456752113673998             
Epoch: #87   | Batch: #24   | Batch Loss: 0.5212751030921936            | Epoch Loss: 0.45944057156642276           
Epoch: #87   | Batch: #25   | Batch Loss: 0.5111084580421448            | Epoch Loss: 0.4615072870254517            
Epoch: #87   | Batch: #26   | Batch Loss: 0.5009762644767761            | Epoch Loss: 0.4630253246197334            
Epoch: #87   | Batch: #27   | Batch Loss: 0.3190493881702423            | Epoch Loss: 0.45769288252901147           
Epoch: #87   | Batch: #28   | Batch Loss: 0.5428232550621033            | Epoch Loss: 0.4607332529766219            
Epoch: #87   | Batch: #29   | Batch Loss: 0.472939133644104             | Epoch Loss: 0.46115414541343164           
Epoch: #87   | Batch: #30   | Batch Loss: 0.496688574552536             | Epoch Loss: 0.4623386263847351            
Epoch: #87   | Batch: #31   | Batch Loss: 0.42663031816482544           | Epoch Loss: 0.46118674547441546           
Epoch: #87   | Batch: #32   | Batch Loss: 0.4772185683250427            | Epoch Loss: 0.46168773993849754           
Epoch: #87   | Batch: #33   | Batch Loss: 0.5647096037864685            | Epoch Loss: 0.46480961460055725           
Epoch: #87   | Batch: #34   | Batch Loss: 0.4845408499240875            | Epoch Loss: 0.4653899450512493            
Epoch: #87   | Batch: #35   | Batch Loss: 0.5614892244338989            | Epoch Loss: 0.4681356387478965            
Epoch: #87   | Batch: #36   | Batch Loss: 0.3983302414417267            | Epoch Loss: 0.4661965999338362            
Epoch: #87   | Batch: #37   | Batch Loss: 0.456058531999588             | Epoch Loss: 0.4659225980977754            
Epoch: #87   | Batch: #38   | Batch Loss: 0.45322948694229126           | Epoch Loss: 0.46558856885684163           
Epoch: #87   | Batch: #39   | Batch Loss: 0.48431631922721863           | Epoch Loss: 0.4660687675842872            
Epoch: #87   | Batch: #40   | Batch Loss: 0.4733145833015442            | Epoch Loss: 0.46624991297721863           
Epoch: #87   | Batch: #41   | Batch Loss: 0.39731356501579285           | Epoch Loss: 0.46456853863669606           
Epoch: #87   | Batch: #42   | Batch Loss: 0.43352219462394714           | Epoch Loss: 0.46382933996972586           
Epoch: #87   | Batch: #43   | Batch Loss: 0.5347381830215454            | Epoch Loss: 0.46547838283139603           

Classifier Validation Epoch #87
------------------------------------------
Batch: #1    | Top-1 Accuracy: 0.8727272727272727            
Batch: #2    | Top-1 Accuracy: 0.8431818181818181            
Batch: #3    | Top-1 Accuracy: 0.8515151515151516            
Batch: #4    | Top-1 Accuracy: 0.8397727272727273            
Batch: #5    | Top-1 Accuracy: 0.8354545454545456            
Batch: #6    | Top-1 Accuracy: 0.8378787878787879            
Batch: #7    | Top-1 Accuracy: 0.8324675324675325            
Batch: #8    | Top-1 Accuracy: 0.8340909090909091            
Batch: #9    | Top-1 Accuracy: 0.8363636363636364            
Batch: #10   | Top-1 Accuracy: 0.8372727272727273            
Batch: #11   | Top-1 Accuracy: 0.8351239669421489            
Batch: #12   | Top-1 Accuracy: 0.8378787878787879            
Batch: #13   | Top-1 Accuracy: 0.8356643356643356            
Batch: #14   | Top-1 Accuracy: 0.8327922077922078            
Batch: #15   | Top-1 Accuracy: 0.8342424242424241            
Batch: #16   | Top-1 Accuracy: 0.8352272727272727            
Batch: #17   | Top-1 Accuracy: 0.8355614973262032            
